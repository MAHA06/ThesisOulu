{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Thesis.ipynb",
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MAHA06/ThesisOulu/blob/master/Adversarials/Thesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "54pec7A464Ta"
   },
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount(‘/content/gdrive’)"
   ],
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4GzjWdjAfomN"
   },
   "source": [
    "import math\n",
    "from torch.autograd.gradcheck import zero_gradients\n",
    "\n",
    "import seaborn as sns\n",
    "from torch.autograd import Variable\n",
    "import torch as torch\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pickle\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms,datasets\n",
    "np.random.seed(42) \n",
    "torch.manual_seed(42)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import save_image\n"
   ],
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GsxBn-i6fmfw"
   },
   "source": [
    "online=False\n",
    "\n",
    "img_size=(28,28)\n",
    "num_classes=10\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.0,), (1.0,))])\n",
    "dataset = datasets.MNIST(root = './data', train=True, transform = transform, download=True)\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [50000, 10000])\n",
    "test_set = datasets.MNIST(root = './data', train=False, transform = transform, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_set,batch_size=1,shuffle=True) \n",
    "val_loader = torch.utils.data.DataLoader(val_set,batch_size=1,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set,batch_size=1,shuffle=True)"
   ],
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YDRayKODFRju"
   },
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "    self.relu1=nn.ReLU()\n",
    "    self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "    self.relu2=nn.ReLU()\n",
    "    self.max_p1=nn.MaxPool2d(2)\n",
    "    self.dropout1 = nn.Dropout2d(0.25)\n",
    "    self.fc1 = nn.Linear(9216, 128)\n",
    "    self.relu3=nn.ReLU()\n",
    "    self.dropout2 = nn.Dropout2d(0.5)\n",
    "\n",
    "    self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.relu1(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.relu2(x)\n",
    "    x = self.max_p1(x)\n",
    "    x = self.dropout1(x)\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = self.fc1(x)\n",
    "    x = self.relu3(x)\n",
    "    x = self.dropout2(x)\n",
    "    x = self.fc2(x)\n",
    "    output = F.log_softmax(x, dim=1)\n",
    "    return output\n",
    "\n",
    "\n"
   ],
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aJMQz9fdFRpn",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "05243ea9-5f4d-43b2-e128-2e3b02ff85b4"
   },
   "source": [
    "use_cuda=False\n",
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
    "print(\"Using the device:\",device)"
   ],
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the device: cpu\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Gl-WshaZFRsF"
   },
   "source": [
    "def fitModel(model,optimizer,scheduler,criterion,device,train_loader,val_loader,epochs):\n",
    "  data_loader={'train':train_loader,'val':val_loader}\n",
    "  print(\"Starting to fit the model\")\n",
    "  train_error,test_error=[],[]\n",
    "  best_points=np.zeros(img_size+(num_classes,),dtype=float)\n",
    "  best_scores=np.full(num_classes,-np.inf)\n",
    "  print(best_scores[1])\n",
    "  for epc in range(epochs):\n",
    "    correct=0\n",
    "    total=0\n",
    "    loss_epoch,val_loss_epoch=0,0\n",
    "    for phase in ('train','val'):\n",
    "\n",
    "      for i,data in enumerate(data_loader[phase]):\n",
    "        \n",
    "        input,label=data[0].to(device),data[1].to(device)\n",
    "        # print(input.shape)\n",
    "        out=model(input)\n",
    "        # print(out)\n",
    "        loss=criterion(out,label)\n",
    "        predict_label=torch.argmax(out,dim=1)\n",
    "        out=out.squeeze().detach().cpu().numpy()\n",
    "        if phase == 'train':\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          loss_epoch+=loss.item()\n",
    "          # print(predict_label.item(),label.item())\n",
    "          # print(out[predict_label.item()],best_scores[predict_label.item()])\n",
    "          # print(out)\n",
    "          if epc == epochs-1 and predict_label.item()==label.item() and out[predict_label.item()]>best_scores[predict_label.item()]:#if last epoch and the predicted label is correct\n",
    "            best_scores[predict_label.item()]=out[predict_label.item()]\n",
    "            best_points[:,:,predict_label]=input.squeeze().detach().cpu().numpy()\n",
    "            # print(label.item(),out[label.item()])\n",
    "            # print(\"The scores were updated:\",best_scores)\n",
    "        else:\n",
    "          total+=1\n",
    "          if predict_label==label:\n",
    "            correct+=1\n",
    "          val_loss_epoch+=loss.item()\n",
    "    scheduler.step(val_loss_epoch/len(val_loader))\n",
    "    print(\"Epoch : {} Accuracy : {}\".format(epc+1,correct/total))\n",
    "    print(\"Epoch : {} Train Loss : {} Eval Loss : {}\".format(epc+1,loss_epoch/len(train_loader),val_loss_epoch/len(val_loader)))\n",
    "    train_error.append(loss_epoch)\n",
    "    test_error.append(val_loss_epoch)\n",
    "  \n",
    "  return train_error,test_error,best_points,best_scores\n",
    "        \n",
    "          \n",
    "\n",
    "\n"
   ],
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "E2VlV-xmFRuk"
   },
   "source": [
    "model=Net().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.0001, betas=(0.9, 0.999))\n",
    "criterion = nn.NLLLoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "\n",
    "\n",
    "###############\n"
   ],
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "AWmhvKHDgpCZ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "bffe0928-be65-4a35-b895-f819cf942caf"
   },
   "source": [
    "## RUN THE TRAINING\n",
    "if(online):\n",
    "  loss,val_loss,best_points,best_scores=fitModel(model,optimizer,scheduler,criterion,device,train_loader,val_loader,10)\n"
   ],
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "HAUXpIF_79an",
    "outputId": "3a8ab6ce-df9b-4020-cbb7-9cc03a5ce5de"
   },
   "source": [
    "if online:\n",
    "  print(best_points.shape)\n",
    "  plt.imshow(best_points[:,:,3])\n",
    "  v=torch.tensor(best_points[:,:,3].reshape(1,1,28,28),dtype=torch.float32)\n",
    "  print(model(v.to(device)))\n",
    "  print(torch.argmax(model(v.to(device))))\n",
    "  print(best_scores)"
   ],
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R59id5I4pOEj"
   },
   "source": [
    "if online:\n",
    "  filename = './model_MNIST'\n",
    "  pickle.dump(model, open(filename, 'wb'))\n",
    "  filename = './model_MNIST_cpu'\n",
    "  pickle.dump(model.cpu(), open(filename, 'wb'))\n",
    "  filename = './scores.mf'\n",
    "  best_markers={\"b_p\":best_points,\"b_s\":best_scores}\n",
    "  pickle.dump(best_markers,open(filename,'wb'))"
   ],
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1M92hyHXqrVI"
   },
   "source": [
    "def deepfool(image, net, num_classes=10, overshoot=0.02, max_iter=50):\n",
    "\n",
    "    \"\"\"\n",
    "       :param image: Image of size HxWx3\n",
    "       :param net: network (input: images, output: values of activation **BEFORE** softmax).\n",
    "       :param num_classes: num_classes (limits the number of classes to test against, by default = 10)\n",
    "       :param overshoot: used as a termination criterion to prevent vanishing updates (default = 0.02).\n",
    "       :param max_iter: maximum number of iterations for deepfool (default = 50)\n",
    "       :return: minimal perturbation that fools the classifier, number of iterations that it required, new estimated_label and perturbed image\n",
    "    \"\"\"\n",
    "    is_cuda = torch.cuda.is_available()\n",
    "\n",
    "    if is_cuda:\n",
    "        print(\"Using GPU\")\n",
    "        image = image.cuda()\n",
    "        net = net.cuda()\n",
    "    else:\n",
    "        print(\"Using CPU\")\n",
    "\n",
    "\n",
    "    f_image = net.forward(image).data.cpu().numpy().flatten()\n",
    "    I = (np.array(f_image)).flatten().argsort()[::-1]\n",
    "\n",
    "    I = I[0:num_classes]\n",
    "    label = I[0]\n",
    "\n",
    "    input_shape = image.shape\n",
    "    pert_image = copy.deepcopy(image)\n",
    "    w = np.zeros(input_shape)\n",
    "    r_tot = np.zeros(input_shape)\n",
    "\n",
    "    loop_i = 0\n",
    "\n",
    "    x = pert_image\n",
    "    fs = net.forward(x)\n",
    "    fs_list = [fs[0,I[k]] for k in range(num_classes)]\n",
    "    k_i = label\n",
    "\n",
    "    while k_i == label and loop_i < max_iter:\n",
    "\n",
    "        pert = np.inf\n",
    "        fs[0, I[0]].backward(retain_graph=True)\n",
    "        grad_orig = x.grad.data.cpu().numpy().copy()\n",
    "\n",
    "        for k in range(1, num_classes):\n",
    "            zero_gradients(x)\n",
    "\n",
    "            fs[0, I[k]].backward(retain_graph=True)\n",
    "            cur_grad = x.grad.data.cpu().numpy().copy()\n",
    "\n",
    "            # set new w_k and new f_k\n",
    "            w_k = cur_grad - grad_orig\n",
    "            f_k = (fs[0, I[k]] - fs[0, I[0]]).data.cpu().numpy()\n",
    "\n",
    "            pert_k = abs(f_k)/np.linalg.norm(w_k.flatten())\n",
    "\n",
    "            # determine which w_k to use\n",
    "            if pert_k < pert:\n",
    "                pert = pert_k\n",
    "                w = w_k\n",
    "\n",
    "        # compute r_i and r_tot\n",
    "        # Added 1e-4 for numerical stability\n",
    "        r_i =  (pert+1e-4) * w / np.linalg.norm(w)\n",
    "        r_tot = np.float32(r_tot + r_i)\n",
    "\n",
    "        if is_cuda:\n",
    "            pert_image = image + (1+overshoot)*torch.from_numpy(r_tot).cuda()\n",
    "        else:\n",
    "            pert_image = image + (1+overshoot)*torch.from_numpy(r_tot)\n",
    "\n",
    "        x = Variable(pert_image, requires_grad=True)\n",
    "        fs = net.forward(x)\n",
    "        k_i = np.argmax(fs.data.cpu().numpy().flatten())\n",
    "\n",
    "        loop_i += 1\n",
    "\n",
    "    r_tot = (1+overshoot)*r_tot\n",
    "\n",
    "    return r_tot, loop_i, label, k_i, pert_image\n",
    "def fgsm_attack(input,epsilon,data_grad):\n",
    "  pert_out = input + epsilon*data_grad.sign()\n",
    "  pert_out = torch.clamp(pert_out, 0, 1)\n",
    "  return pert_out\n",
    "\n",
    "def ifgsm_attack(input,epsilon,data_grad):\n",
    "  iter = 10\n",
    "  alpha = epsilon/iter\n",
    "  pert_out = input\n",
    "  for i in range(iter-1):\n",
    "    pert_out = pert_out + alpha*data_grad.sign()\n",
    "    pert_out = torch.clamp(pert_out, 0, 1)\n",
    "    if torch.norm((pert_out-input),p=float('inf')) > epsilon:\n",
    "      break\n",
    "  return pert_out\n",
    "\n",
    "def mifgsm_attack(input,epsilon,data_grad):\n",
    "  iter=10\n",
    "  decay_factor=1.0\n",
    "  pert_out = input\n",
    "  alpha = epsilon/iter\n",
    "  g=0\n",
    "  for i in range(iter-1):\n",
    "    g = decay_factor*g + data_grad/torch.norm(data_grad,p=1)\n",
    "    pert_out = pert_out + alpha*torch.sign(g)\n",
    "    pert_out = torch.clamp(pert_out, 0, 1)\n",
    "    if torch.norm((pert_out-input),p=float('inf')) > epsilon:\n",
    "      break\n",
    "  return pert_out"
   ],
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j1wEKDumo2wC"
   },
   "source": [
    "def JacobianMatrix(model,input):\n",
    "  print(\"Input size \",input.shape)\n",
    "  number_of_piels=input.shape[0]*input.shape[1]\n",
    "  J = torch.zeros ((1, 784, 10))   # loop will fill in Jacobian\n",
    "  input.requires_grad = True\n",
    "  preds = model (input)\n",
    "  for  i in range (10):\n",
    "      grd = torch.zeros ((1, 10))   # same shape as preds\n",
    "      grd[0, i] = 1    # column of Jacobian to compute\n",
    "      preds.backward (gradient = grd, retain_graph = True)\n",
    "      J[:,:,i] = input.grad   # fill in one column of Jacobian\n",
    "      input.grad.zero_()   # .backward() accumulates gradients, so reset to zero\n",
    "def JSM_attack(model, input):\n",
    "  input.requires_grad=True"
   ],
   "execution_count": 39,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "ySq2vSmOJxS7"
   },
   "source": [
    "activation_in = {}\n",
    "activation_out={}\n",
    "def get_activation_input(name):\n",
    "    def hook(model, input, output):\n",
    "        activation_in[name] = input[0]\n",
    "    return hook\n",
    "def get_activation_output(name):\n",
    "    def hook(model, input, output):\n",
    "        activation_out[name] = output.clone().detach()\n",
    "    return hook\n",
    "def nth_derivative(f, wrt, n):\n",
    "\n",
    "    for i in range(n):\n",
    "\n",
    "        grads = torch.autograd.grad(f, wrt, create_graph=True)[0]\n",
    "        f = grads.sum()\n",
    "\n",
    "    return grads\n",
    "def get_higher_gradients(model,input,label, rank=3):\n",
    "  grads=[]\n",
    "  optimizer.zero_grad()\n",
    "  input.requires_grad=True\n",
    "  outs=model(input)\n",
    "  # pred=torch.zeros(num_classes)\n",
    "  # pred[label.item()]=1\n",
    "  l=criterion(outs,label)\n",
    "  aux=torch.autograd.grad(l,input,retain_graph=True,create_graph=True)[0]\n",
    "  grads.append(aux.reshape(1,-1))\n",
    "  for r in range(2,rank+1):\n",
    "    aux=torch.autograd.grad(aux.sum(),input,create_graph=True)[0]\n",
    "    grads.append(aux.reshape(1,-1))\n",
    "  return grads\n",
    "def check_taylor_prediction(model,input,marker_images,check_label,layer_name,rank):\n",
    "  # plt.imshow(input.squeeze().detach().cpu().numpy())\n",
    "  # grads=get_higher_gradients(model,input,true_label)\n",
    "\n",
    "  # grads=get_higher_gradients(model,input,perturbed_label)\n",
    "  # layer_grads=get_layer_gradients(model,input,check_label,\"conv2\")\n",
    "\n",
    "  # input=input.reshape(-1,1)\n",
    "  # # marker_image=marker_images[:,:,true_label.item()].reshape(-1,1)\n",
    "  # # marker_image=torch.tensor(marker_image,dtype=torch.float32)\n",
    "  # # total=1.+torch.mm(grads[0],(input-marker_image))+torch.mm(grads[1],torch.pow((input-marker_image),2))/2\n",
    "  # # total=1.+torch.mm(grads[0],(input-marker_image))+torch.mm(grads[1],torch.pow((input-marker_image),2))/2\n",
    "  # marker_image1=marker_images[:,:,perturbed_label.item()].reshape(-1,1)\n",
    "  # marker_image1=torch.tensor(marker_image1,dtype=torch.float32)\n",
    "  # total1=torch.mm(layer_grads[0],(input-marker_image1))+torch.mm(layer_grads[1],torch.pow((input-marker_image1),2))/2\n",
    "  marker_img=torch.tensor(marker_images[:,:,check_label.item()],dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "  # total_marker=compute_marker_image_prediction(model,input,marker_img,check_label,layer_name,rank)\n",
    "  total_unknown=compute_unknown_image_prediction(model,input,marker_img,check_label,layer_name,rank)\n",
    "\n",
    "  return total_unknown\n",
    "def compute_marker_image_prediction(model,unk_image,marker_img,marker_label,l_name,rank):\n",
    "  grads,input_marker,loss_marker=get_layer_gradients(model,marker_img,marker_label,l_name,rank)\n",
    "  optimizer.zero_grad()\n",
    "  input_marker=input_marker.view(-1,1)\n",
    "  # print(input.shape)\n",
    "  outs=model(unk_image)\n",
    "  input_layer=activation_in[l_name].view(-1,1)\n",
    "  total=loss_marker.item()\n",
    "  for i in range(0,rank):\n",
    "    total+=torch.mm(grads[i],torch.pow((input_layer-input_marker),i+1))/(math.factorial(i+1))\n",
    "  return total.item()\n",
    "def compute_unknown_image_prediction(model,unk_image,marker_image,marker_label,l_name,rank):\n",
    "\n",
    "  grads,unk_input,loss_unk=get_layer_gradients(model,unk_image,marker_label,l_name,rank)\n",
    "  optimizer.zero_grad()\n",
    "  layer_input=unk_input.view(-1,1)\n",
    "  # print(input.shape)\n",
    "  outs=model(marker_image)\n",
    "  input_layer=activation_in[l_name].view(-1,1)\n",
    "  total=loss_unk.item()\n",
    "  for i in range(0,rank):\n",
    "    total+=torch.mm(grads[i],torch.pow((input_layer-layer_input),i+1))/(math.factorial(i+1))\n",
    "  # total=loss_unk.item()+torch.mm(grads[0],(input_layer-layer_input))+torch.mm(grads[1],torch.pow((input_layer-layer_input),2))/2\n",
    "\n",
    "  return total.item()\n",
    "\n",
    "def get_layer_gradients(model,target_img,label,l_name, rank):\n",
    "  grads=[]\n",
    "  optimizer.zero_grad()\n",
    "  # target_img.requires_grad=True\n",
    "  outs=model(target_img)\n",
    "  # pred=torch.zeros((1,num_classes),dtype=np.int)\n",
    "  # pred[label.item()]=1\n",
    "  loss_value=criterion(outs,label)\n",
    "  layer_input_data=activation_in[l_name]\n",
    "  # layer_input_data.requires_grad=True\n",
    "  aux=torch.autograd.grad(loss_value,layer_input_data,retain_graph=True,create_graph=True)[0]\n",
    "  grads.append(aux.reshape(1,-1))\n",
    "  for r in range(1,rank):\n",
    "    aux=torch.autograd.grad(aux.sum(),layer_input_data,create_graph=True)[0]\n",
    "    grads.append(aux.reshape(1,-1))\n",
    "  return grads,layer_input_data,loss_value\n",
    "\n",
    "def get_marker_images(model,data_loader):\n",
    "  best_points=np.zeros(img_size+(num_classes,),dtype=float)\n",
    "  best_scores=np.full(num_classes,np.inf)\n",
    "  for i,data in enumerate(data_loader):\n",
    "\n",
    "      input,label=data[0].to(device),data[1].to(device)\n",
    "      # print(input.shape)\n",
    "      out=model(input)\n",
    "      # print(out)\n",
    "      loss=criterion(out,label)\n",
    "      predict_label=torch.argmax(out,dim=1)\n",
    "      out=out.squeeze().detach().cpu().numpy()\n",
    "\n",
    "      if   predict_label.item()==label.item() and loss<best_scores[predict_label.item()]:#if last epoch and the predicted label is correct\n",
    "        best_scores[predict_label.item()]=loss\n",
    "        best_points[:,:,predict_label.item()]=input.squeeze().detach().cpu().numpy()\n",
    "  best_markers={\"b_p\":best_points,\"b_s\":best_scores}\n",
    "  return best_markers\n",
    "\n",
    "filename = './model_MNIST_cpu'\n",
    "model= pickle.load(open(filename, 'rb'))\n",
    "filename = './scores.mf'\n",
    "# best_fits=get_marker_images(model,train_loader)\n",
    "# pickle.dump(best_fits, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "best_fits=pickle.load(open(filename,'rb'))\n",
    "\n",
    "def compute_statistics(confusion_matrix):\n",
    "  FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "  FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "  TP = np.diag(confusion_matrix)\n",
    "  TN = confusion_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "  # Sensitivity, hit rate, recall, or true positive rate\n",
    "  TPR = TP/(TP+FN)\n",
    "  print(\"Sensitivity\",TPR)\n",
    "  # Specificity or true negative rate\n",
    "  TNR = TN/(TN+FP)\n",
    "  print(\"Specificity\",TNR)\n",
    "  # Precision or positive predictive value\n",
    "  PPV = TP/(TP+FP)\n",
    "  print(\"Precision\",PPV)\n",
    "  # Negative predictive value\n",
    "  NPV = TN/(TN+FN)\n",
    "  # Fall out or false positive rate\n",
    "  FPR = FP/(FP+TN)\n",
    "  # False negative rate\n",
    "  FNR = FN/(TP+FN)\n",
    "  # False discovery rate\n",
    "  FDR = FP/(TP+FP)\n",
    "  # Overall accuracy\n",
    "  ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "  print(\"ACC:\",ACC)\n",
    "def testModel(model,criterion,device,test_loader,attack,rank,l_name=\"conv1\"):\n",
    "  print()\n",
    "  print(\"Started testing with A:{} on L:{} with Rank:{}\".format(attack,l_name,rank))\n",
    "  right=0\n",
    "  totals=0\n",
    "  fooled=0\n",
    "  binary=0\n",
    "  loss_e=0.02\n",
    "  loser_count=0\n",
    "  c_matrix_data=[]\n",
    "  for data,labels in test_loader:\n",
    "    data,labels=data.to(device),labels.to(device)\n",
    "    data.requires_grad=True\n",
    "    outs=model(data)\n",
    "    true_preds=torch.argmax(outs,dim=1)\n",
    "    if true_preds!=labels:\n",
    "      continue\n",
    "    epsilon=0.1\n",
    "    true_loss=criterion(outs,labels)\n",
    "    true_loss.backward()\n",
    "    data_grad=data.grad.data\n",
    "    if attack == \"fgsm\":\n",
    "      perturbed_data = fgsm_attack(data,epsilon,data_grad)\n",
    "    elif attack == \"ifgsm\":\n",
    "      perturbed_data = ifgsm_attack(data,epsilon,data_grad)\n",
    "    elif attack == \"mifgsm\":\n",
    "      perturbed_data = mifgsm_attack(data,epsilon,data_grad)\n",
    "    elif attack==\"deepfool\":\n",
    "      perturbed_data=deepfool(data,model)\n",
    "    elif attack == \"jacobian\":\n",
    "      print(\"Implement the jacobian\")\n",
    "    p_out=model(perturbed_data)\n",
    "    perturb_label=torch.argmax(p_out,dim=1)\n",
    "    # print(perturb_label.item())\n",
    "\n",
    "    if perturb_label!=labels:\n",
    "      totals+=1\n",
    "      # print(\"Trying to determine for the true class:\",labels.item(),\" which was labeled as:\",perturb_label.item())\n",
    "      taylors=range(0,num_classes)\n",
    "      taylors=[check_taylor_prediction(model,perturbed_data,best_fits['b_p'],torch.tensor(lab).reshape(1),layer_name=l_name,rank=rank) for lab in taylors ]\n",
    "      # taylors=[check_taylor_prediction(model,data,best_fits['b_p'],torch.tensor(lab).reshape(1),layer_name=l_name) for lab in taylors ]\n",
    "      taylor_np=np.array(taylors)\n",
    "      winning_labels=np.where((taylor_np>-5) )\n",
    "      # if(perturb_label.item() in winning_labels[0]):#check if the predicted label holds 0 loss\n",
    "      #   binary+=1\n",
    "      # print(\"Binary \",binary/totals)\n",
    "      # print(\"Right min :\",loser_count/totals)\n",
    "\n",
    "      abs_x=[abs(t) for t in taylors]\n",
    "      max_l=np.argmax(taylors)\n",
    "      min_l=np.argmin(abs_x)\n",
    "      c_matrix_data.append([ labels.item(),max_l])\n",
    "      if max_l == perturb_label.item():\n",
    "        fooled+=1\n",
    "      if max_l == labels.item():\n",
    "        right+=1\n",
    "        # save_image(data,\"./Outs/original.png\")\n",
    "        # save_image(perturbed_data,\"./Outs/perturbed_data.png\")\n",
    "        # save_image(perturbed_data-data,\"./Outs/gradient visualization.png\")\n",
    "      if min_l == labels.item():\n",
    "        loser_count+=1\n",
    "      # r=check_taylor_prediction(model,data,best_fits['b_p'],perturb_label)\n",
    "\n",
    "  print(\"Stats for attack type:\"+attack +\" on the layer:\"+l_name+\" with a gradient rank:\"+str(rank))\n",
    "  print(\"Right max :\",right/totals)\n",
    "  print(\"Fooled max:\",fooled/totals)\n",
    "  print(\"Right min :\",loser_count/totals)\n",
    "  print(\"Binary \",binary/totals)\n",
    "  confusion_array=np.array(c_matrix_data)\n",
    "  cm =   confusion_matrix(confusion_array[:,0],confusion_array[:,1])\n",
    "  compute_statistics(cm)\n",
    "  f = sns.heatmap(cm, annot=True)\n",
    "  plt.savefig('./Outs/heatMap-'+attack+'_'+l_name+'.png')\n",
    "  plt.clf()\n",
    "  return right/totals\n",
    "# conv2\n",
    "model.conv1.register_forward_hook(get_activation_input('conv1'))\n",
    "model.conv2.register_forward_hook(get_activation_input('conv2'))\n",
    "model.relu1.register_forward_hook(get_activation_input('relu1'))\n",
    "model.relu1.register_forward_hook(get_activation_input('relu2'))\n",
    "\n",
    "model.relu3.register_forward_hook(get_activation_input('relu3'))\n",
    "model.fc1.register_forward_hook(get_activation_input('fc1'))\n",
    "model.fc1.register_forward_hook(get_activation_input('fc2'))\n",
    "\n",
    "print(\"Starting\")\n",
    "ranks=[1,2,3,4]\n",
    "layers=[\"conv1\",\"relu1\",\"conv2\",\"relu2\",\"fc1\",\"relu3\",\"fc2\"]\n",
    "attack_types=[\"fgsm\",\"mifgsm\",\"ifgsm\"]\n",
    "\n",
    "for att in attack_types:\n",
    "\n",
    "  for r in ranks:\n",
    "    acc_mifgsm=[]\n",
    "    for l in layers:\n",
    "      t=testModel(model,criterion,'cpu',test_loader,att,r,l_name=l)\n",
    "      acc_mifgsm.append(t)\n",
    "    plt.plot(layers,acc_mifgsm)\n",
    "    plt.savefig('./Outs/{}_R{}.png'.format(att,r))\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ],
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n",
      "STarted testing with A:fgsm on L:conv1 with Rank:1\n",
      "\n",
      "Stats for attack type:fgsm on the layer:conv1 with a gradient rank:1\n",
      "Right max : 0.4373198847262248\n",
      "Fooled max: 0.3811239193083574\n",
      "Right min : 0.3170028818443804\n",
      "Binary  0.0\n",
      "Sensitivity [0.43478261 0.234375   0.52803738 0.22340426 0.44230769 0.65605096\n",
      " 0.38582677 0.44720497 0.44444444 0.3       ]\n",
      "Specificity [0.94768764 0.98338369 0.94378194 0.93353941 0.94805195 0.89033306\n",
      " 0.97145123 0.94784026 0.9052901  0.89825119]\n",
      "Precision [0.3030303  0.40540541 0.63128492 0.19626168 0.51879699 0.43277311\n",
      " 0.57647059 0.52941176 0.46376812 0.23353293]\n",
      "ACC: [0.9221902  0.94884726 0.879683   0.88544669 0.89121037 0.86383285\n",
      " 0.91786744 0.88976945 0.83357349 0.84221902]\n",
      "STarted testing with A:fgsm on L:relu1 with Rank:1\n",
      "\n",
      "Stats for attack type:fgsm on the layer:relu1 with a gradient rank:1\n",
      "Right max : 0.437727603787327\n",
      "Fooled max: 0.37509104151493083\n",
      "Right min : 0.30007283321194467\n",
      "Binary  0.0\n",
      "Sensitivity [0.38333333 0.14035088 0.53       0.1744186  0.45348837 0.61538462\n",
      " 0.43165468 0.48734177 0.46846847 0.23636364]\n",
      "Specificity [0.94973343 0.98860182 0.9369139  0.92463092 0.94421316 0.89368771\n",
      " 0.96596434 0.94650206 0.91398784 0.90577989]\n",
      "Precision [0.25842697 0.34782609 0.58888889 0.13392857 0.53793103 0.44827586\n",
      " 0.58823529 0.54225352 0.51231527 0.17931034]\n",
      "ACC: [0.92498179 0.95338674 0.8776402  0.8776402  0.88273853 0.8594319\n",
      " 0.91187181 0.89366351 0.84195193 0.85214858]\n",
      "STarted testing with A:fgsm on L:conv2 with Rank:1\n",
      "\n",
      "Stats for attack type:fgsm on the layer:conv2 with a gradient rank:1\n",
      "Right max : 0.5078066914498142\n",
      "Fooled max: 0.35018587360594794\n",
      "Right min : 0.3115241635687732\n",
      "Binary  0.0\n",
      "Sensitivity [0.57627119 0.3220339  0.57368421 0.24444444 0.4875     0.74050633\n",
      " 0.54918033 0.54705882 0.46948357 0.35483871]\n",
      "Specificity [0.96656299 0.98833593 0.93419913 0.95219124 0.95949367 0.88542544\n",
      " 0.97301717 0.94808511 0.92844523 0.91072891]\n",
      "Precision [0.44155844 0.55882353 0.58918919 0.26829268 0.61904762 0.46245059\n",
      " 0.67       0.6038961  0.55248619 0.2875817 ]\n",
      "ACC: [0.94944238 0.95910781 0.88327138 0.90483271 0.90334572 0.86840149\n",
      " 0.93457249 0.89739777 0.85576208 0.85947955]\n",
      "STarted testing with A:fgsm on L:relu2 with Rank:1\n",
      "\n",
      "Stats for attack type:fgsm on the layer:relu2 with a gradient rank:1\n",
      "Right max : 0.47338129496402875\n",
      "Fooled max: 0.3647482014388489\n",
      "Right min : 0.3043165467625899\n",
      "Binary  0.0\n",
      "Sensitivity [0.37681159 0.25       0.56650246 0.23469388 0.48780488 0.64705882\n",
      " 0.48905109 0.45833333 0.55707763 0.28571429]\n",
      "Specificity [0.95230886 0.98947368 0.9326032  0.95201238 0.95106036 0.90137429\n",
      " 0.98324022 0.94189853 0.91631085 0.88906373]\n",
      "Precision [0.29213483 0.51724138 0.58974359 0.27058824 0.57142857 0.4479638\n",
      " 0.76136364 0.52027027 0.55454545 0.19428571]\n",
      "ACC: [0.92374101 0.95755396 0.87913669 0.90143885 0.89640288 0.87338129\n",
      " 0.93453237 0.88345324 0.85971223 0.83741007]\n",
      "STarted testing with A:fgsm on L:fc1 with Rank:1\n",
      "\n",
      "Stats for attack type:fgsm on the layer:fc1 with a gradient rank:1\n",
      "Right max : 0.23473837209302326\n",
      "Fooled max: 0.2601744186046512\n",
      "Right min : 0.23982558139534885\n",
      "Binary  0.0\n",
      "Sensitivity [0.19607843 0.55223881 0.14883721 0.1372549  0.28776978 0.27922078\n",
      " 0.1953125  0.2122905  0.27027027 0.20168067]\n",
      "Specificity [0.9690566  0.78762414 0.95004307 0.9466248  0.93371059 0.88216039\n",
      " 0.96875    0.91562239 0.87001733 0.92680986]\n",
      "Precision [0.19607843 0.11746032 0.35555556 0.17073171 0.32786885 0.22994652\n",
      " 0.390625   0.27338129 0.28571429 0.20689655]\n",
      "ACC: [0.94040698 0.77616279 0.82485465 0.88662791 0.8684593  0.81468023\n",
      " 0.89680233 0.82412791 0.77325581 0.86409884]\n",
      "STarted testing with A:fgsm on L:relu3 with Rank:1\n",
      "\n",
      "Stats for attack type:fgsm on the layer:relu3 with a gradient rank:1\n",
      "Right max : 0.19371345029239767\n",
      "Fooled max: 0.2887426900584795\n",
      "Right min : 0.2046783625730994\n",
      "Binary  0.0\n",
      "Sensitivity [0.1147541  0.49230769 0.1127451  0.13095238 0.18954248 0.16384181\n",
      " 0.13636364 0.16352201 0.32467532 0.14705882]\n",
      "Specificity [0.97551645 0.80046048 0.95618557 0.9470405  0.90534979 0.89588581\n",
      " 0.96197411 0.90653433 0.84168865 0.9107425 ]\n",
      "Precision [0.17948718 0.10958904 0.31081081 0.13924051 0.20138889 0.18954248\n",
      " 0.27692308 0.18705036 0.29411765 0.1171875 ]\n",
      "ACC: [0.9371345  0.78581871 0.83040936 0.89692982 0.8252924  0.80116959\n",
      " 0.88230994 0.82017544 0.75438596 0.85380117]\n",
      "STarted testing with A:fgsm on L:fc2 with Rank:1\n",
      "\n",
      "Stats for attack type:fgsm on the layer:fc2 with a gradient rank:1\n",
      "Right max : 0.2182952182952183\n",
      "Fooled max: 0.24393624393624394\n",
      "Right min : 0.2286902286902287\n",
      "Binary  0.0\n",
      "Sensitivity [0.07142857 0.50666667 0.13063063 0.11926606 0.26035503 0.29447853\n",
      " 0.17829457 0.22352941 0.28384279 0.10743802]\n",
      "Specificity [0.97476568 0.77192982 0.94840295 0.94752624 0.92386185 0.909375\n",
      " 0.95738204 0.91123331 0.86326194 0.92435703]\n",
      "Precision [0.1025641  0.10857143 0.31521739 0.15662651 0.31205674 0.29268293\n",
      " 0.29113924 0.25165563 0.28138528 0.11504425]\n",
      "ACC: [0.93970894 0.75814276 0.82259182 0.88496188 0.84615385 0.83991684\n",
      " 0.88773389 0.83021483 0.77130977 0.85585586]\n",
      "STarted testing with A:fgsm on L:conv1 with Rank:2\n",
      "\n",
      "Stats for attack type:fgsm on the layer:conv1 with a gradient rank:2\n",
      "Right max : 0.3954843408594319\n",
      "Fooled max: 0.3590677348871085\n",
      "Right min : 0.2942461762563729\n",
      "Binary  0.0\n",
      "Sensitivity [0.42372881 0.12857143 0.48241206 0.22105263 0.41549296 0.58791209\n",
      " 0.34645669 0.43636364 0.3952381  0.21774194]\n",
      "Specificity [0.94368341 0.98772064 0.90289608 0.92644757 0.9471974  0.89168766\n",
      " 0.97191011 0.94370861 0.90369733 0.90152122]\n",
      "Precision [0.25252525 0.36       0.45714286 0.1826087  0.47580645 0.45338983\n",
      " 0.55696203 0.51428571 0.42564103 0.18      ]\n",
      "ACC: [0.92134013 0.94391843 0.84195193 0.8776402  0.89220685 0.85142025\n",
      " 0.91405681 0.88273853 0.82592862 0.83976693]\n",
      "STarted testing with A:fgsm on L:relu1 with Rank:2\n",
      "\n",
      "Stats for attack type:fgsm on the layer:relu1 with a gradient rank:2\n",
      "Right max : 0.3606082548877625\n",
      "Fooled max: 0.38015930485155686\n",
      "Right min : 0.33019551049963797\n",
      "Binary  0.0\n",
      "Sensitivity [0.26984127 0.17808219 0.5645933  0.13483146 0.27325581 0.44720497\n",
      " 0.37815126 0.31847134 0.47685185 0.17213115]\n",
      "Specificity [0.94840668 0.98700306 0.92576792 0.9125387  0.95037221 0.87786885\n",
      " 0.97464342 0.93137255 0.88669528 0.88800635]\n",
      "Precision [0.2        0.43333333 0.57560976 0.096      0.43925234 0.32579186\n",
      " 0.58441558 0.37313433 0.43829787 0.12962963]\n",
      "ACC: [0.91745112 0.9442433  0.87110789 0.86241854 0.8660391  0.82766112\n",
      " 0.92324403 0.86169442 0.82259232 0.82476466]\n",
      "STarted testing with A:fgsm on L:conv2 with Rank:2\n",
      "\n",
      "Stats for attack type:fgsm on the layer:conv2 with a gradient rank:2\n",
      "Right max : 0.2792857142857143\n",
      "Fooled max: 0.2785714285714286\n",
      "Right min : 0.21928571428571428\n",
      "Binary  0.0\n",
      "Sensitivity [0.15384615 0.10714286 0.41284404 0.26136364 0.14189189 0.47282609\n",
      " 0.20454545 0.13173653 0.35       0.2295082 ]\n",
      "Specificity [0.94681648 0.97916667 0.88071066 0.85594512 0.9528754  0.88569079\n",
      " 0.97003155 0.92295215 0.90762712 0.89045383]\n",
      "Precision [0.12345679 0.17647059 0.38961039 0.10849057 0.2625     0.38495575\n",
      " 0.41538462 0.18803419 0.41397849 0.16666667]\n",
      "ACC: [0.91       0.94428571 0.80785714 0.81857143 0.86714286 0.83142857\n",
      " 0.89785714 0.82857143 0.82       0.83285714]\n",
      "STarted testing with A:fgsm on L:relu2 with Rank:2\n",
      "\n",
      "Stats for attack type:fgsm on the layer:relu2 with a gradient rank:2\n",
      "Right max : 0.37609970674486803\n",
      "Fooled max: 0.3775659824046921\n",
      "Right min : 0.34750733137829914\n",
      "Binary  0.0\n",
      "Sensitivity [0.32142857 0.125      0.57692308 0.27173913 0.3        0.46308725\n",
      " 0.37398374 0.2202381  0.4679803  0.27067669]\n",
      "Specificity [0.9411315  0.9824159  0.90707965 0.91194969 0.95469522 0.87242798\n",
      " 0.97502015 0.9506689  0.89664083 0.90739236]\n",
      "Precision [0.18947368 0.23333333 0.5625     0.18248175 0.45       0.30803571\n",
      " 0.5974026  0.38541667 0.44186047 0.24      ]\n",
      "ACC: [0.91568915 0.94721408 0.85043988 0.86876833 0.88269795 0.82771261\n",
      " 0.92082111 0.86070381 0.83284457 0.84530792]\n",
      "STarted testing with A:fgsm on L:fc1 with Rank:2\n",
      "\n",
      "Stats for attack type:fgsm on the layer:fc1 with a gradient rank:2\n",
      "Right max : 0.22807017543859648\n",
      "Fooled max: 0.16812865497076024\n",
      "Right min : 0.062134502923976605\n",
      "Binary  0.0\n",
      "Sensitivity [0.31578947 0.01785714 0.29901961 0.12903226 0.29585799 0.19620253\n",
      " 0.21875    0.2375     0.25333333 0.13559322]\n",
      "Specificity [0.88176964 0.94283537 0.90206186 0.92941176 0.91409508 0.92644628\n",
      " 0.92741935 0.9147351  0.86701662 0.932     ]\n",
      "Precision [0.10404624 0.01315789 0.34857143 0.11764706 0.32679739 0.25833333\n",
      " 0.23728814 0.26950355 0.27272727 0.15841584]\n",
      "ACC: [0.85818713 0.90497076 0.8121345  0.875      0.8377193  0.84210526\n",
      " 0.86111111 0.83552632 0.76608187 0.86330409]\n",
      "STarted testing with A:fgsm on L:relu3 with Rank:2\n",
      "\n",
      "Stats for attack type:fgsm on the layer:relu3 with a gradient rank:2\n",
      "Right max : 0.23098591549295774\n",
      "Fooled max: 0.2676056338028169\n",
      "Right min : 0.16408450704225352\n",
      "Binary  0.0\n",
      "Sensitivity [0.35714286 0.11290323 0.30046948 0.17307692 0.20555556 0.19760479\n",
      " 0.18103448 0.19889503 0.30516432 0.2109375 ]\n",
      "Specificity [0.93181818 0.92857143 0.89312345 0.90729483 0.93225806 0.90422985\n",
      " 0.9547546  0.92816788 0.83844242 0.91950464]\n",
      "Precision [0.17699115 0.06730769 0.33160622 0.12857143 0.30578512 0.21568627\n",
      " 0.2625     0.288      0.25       0.20610687]\n",
      "ACC: [0.90915493 0.89295775 0.80422535 0.85352113 0.84014085 0.82112676\n",
      " 0.8915493  0.83521127 0.7584507  0.8556338 ]\n",
      "STarted testing with A:fgsm on L:fc2 with Rank:2\n",
      "\n",
      "Stats for attack type:fgsm on the layer:fc2 with a gradient rank:2\n",
      "Right max : 0.2060478199718706\n",
      "Fooled max: 0.18143459915611815\n",
      "Right min : 0.05414908579465542\n",
      "Binary  0.0\n",
      "Sensitivity [0.28333333 0.0952381  0.2489083  0.09433962 0.17575758 0.1875\n",
      " 0.21138211 0.25581395 0.24770642 0.15873016]\n",
      "Specificity [0.85756241 0.94849154 0.87845767 0.93693009 0.93556086 0.94690967\n",
      " 0.90762125 0.9216     0.87790698 0.90432099]\n",
      "Precision [0.08056872 0.07894737 0.28217822 0.10752688 0.26363636 0.30927835\n",
      " 0.17808219 0.30985915 0.26865672 0.13888889]\n",
      "ACC: [0.83333333 0.91068917 0.77707454 0.87412096 0.84739803 0.86146273\n",
      " 0.84739803 0.84106892 0.78129395 0.83825598]\n",
      "STarted testing with A:fgsm on L:conv1 with Rank:3\n",
      "\n",
      "Stats for attack type:fgsm on the layer:conv1 with a gradient rank:3\n",
      "Right max : 0.28469241773962806\n",
      "Fooled max: 0.25894134477825465\n",
      "Right min : 0.1759656652360515\n",
      "Binary  0.0\n",
      "Sensitivity [0.26785714 0.0483871  0.32701422 0.16842105 0.24324324 0.44186047\n",
      " 0.33333333 0.26857143 0.29707113 0.20175439]\n",
      "Specificity [0.92548435 0.95434132 0.88795282 0.91174213 0.9448     0.88988581\n",
      " 0.9504717  0.92641047 0.90250216 0.90576324]\n",
      "Precision [0.13043478 0.046875   0.34158416 0.1221374  0.34285714 0.36018957\n",
      " 0.4        0.34306569 0.38586957 0.15972222]\n",
      "ACC: [0.89914163 0.91416309 0.80329041 0.86123033 0.87052933 0.83476395\n",
      " 0.89484979 0.84406295 0.79899857 0.84835479]\n",
      "STarted testing with A:fgsm on L:relu1 with Rank:3\n",
      "\n",
      "Stats for attack type:fgsm on the layer:relu1 with a gradient rank:3\n",
      "Right max : 0.19769949676491733\n",
      "Fooled max: 0.24299065420560748\n",
      "Right min : 0.13299784327821712\n",
      "Binary  0.0\n",
      "Sensitivity [0.14285714 0.04761905 0.2283105  0.14141414 0.19620253 0.25316456\n",
      " 0.16666667 0.21176471 0.23152709 0.18253968]\n",
      "Specificity [0.91792169 0.95783133 0.8831058  0.89009288 0.9432279  0.87266829\n",
      " 0.95552025 0.90990991 0.88636364 0.88616601]\n",
      "Precision [0.07627119 0.05084746 0.26737968 0.08974359 0.30693069 0.20304569\n",
      " 0.28205128 0.24657534 0.25824176 0.13772455]\n",
      "ACC: [0.88281812 0.91660676 0.78001438 0.83680805 0.85837527 0.8023005\n",
      " 0.88066139 0.82458663 0.79079799 0.82242991]\n",
      "STarted testing with A:fgsm on L:conv2 with Rank:3\n",
      "\n",
      "Stats for attack type:fgsm on the layer:conv2 with a gradient rank:3\n",
      "Right max : 0.08439716312056737\n",
      "Fooled max: 0.10567375886524823\n",
      "Right min : 0.11205673758865248\n",
      "Binary  0.0\n",
      "Sensitivity [0.09677419 0.01785714 0.13661202 0.08247423 0.06111111 0.11229947\n",
      " 0.07913669 0.07303371 0.06829268 0.07317073]\n",
      "Specificity [0.90281899 0.93574594 0.82314588 0.86747906 0.93658537 0.89288635\n",
      " 0.91030685 0.89123377 0.91286307 0.90675991]\n",
      "Precision [0.04379562 0.01136364 0.10330579 0.04395604 0.12359551 0.13815789\n",
      " 0.088      0.08843537 0.11764706 0.06976744]\n",
      "ACC: [0.86737589 0.89929078 0.73404255 0.81347518 0.8248227  0.7893617\n",
      " 0.82836879 0.78794326 0.79007092 0.83404255]\n",
      "STarted testing with A:fgsm on L:relu2 with Rank:3\n",
      "\n",
      "Stats for attack type:fgsm on the layer:relu2 with a gradient rank:3\n",
      "Right max : 0.17753623188405798\n",
      "Fooled max: 0.2463768115942029\n",
      "Right min : 0.15869565217391304\n",
      "Binary  0.0\n",
      "Sensitivity [0.09230769 0.06060606 0.2010582  0.11458333 0.11038961 0.22929936\n",
      " 0.16551724 0.13450292 0.30188679 0.176     ]\n",
      "Specificity [0.921673   0.96194825 0.87825357 0.88707165 0.94290375 0.85118561\n",
      " 0.95303644 0.89991729 0.88441781 0.89960159]\n",
      "Precision [0.05504587 0.07407407 0.20765027 0.07051282 0.1954023  0.16513761\n",
      " 0.29268293 0.15972222 0.32160804 0.14864865]\n",
      "ACC: [0.8826087  0.91884058 0.78550725 0.83333333 0.85       0.78043478\n",
      " 0.87028986 0.80507246 0.79492754 0.83405797]\n",
      "STarted testing with A:fgsm on L:fc1 with Rank:3\n",
      "\n",
      "Stats for attack type:fgsm on the layer:fc1 with a gradient rank:3\n",
      "Right max : 0.07676056338028169\n",
      "Fooled max: 0.08873239436619719\n",
      "Right min : 0.06971830985915493\n",
      "Binary  0.0\n",
      "Sensitivity [0.06896552 0.         0.06103286 0.17142857 0.06179775 0.0754717\n",
      " 0.07692308 0.04624277 0.09606987 0.09243697]\n",
      "Specificity [0.90895742 0.93768328 0.86578293 0.85551331 0.90740741 0.84298176\n",
      " 0.92635659 0.89735365 0.9395466  0.89085319]\n",
      "Precision [0.03125    0.         0.07428571 0.08653846 0.08730159 0.05714286\n",
      " 0.0952381  0.05882353 0.23404255 0.07189542]\n",
      "ACC: [0.87464789 0.90070423 0.74507042 0.80492958 0.80140845 0.75704225\n",
      " 0.84859155 0.79366197 0.80352113 0.82394366]\n",
      "STarted testing with A:fgsm on L:relu3 with Rank:3\n",
      "\n",
      "Stats for attack type:fgsm on the layer:relu3 with a gradient rank:3\n",
      "Right max : 0.10549777117384844\n",
      "Fooled max: 0.10846953937592868\n",
      "Right min : 0.1300148588410104\n",
      "Binary  0.0\n",
      "Sensitivity [0.18367347 0.03703704 0.13333333 0.12631579 0.08536585 0.08387097\n",
      " 0.09774436 0.04268293 0.16299559 0.08181818]\n",
      "Specificity [0.91904395 0.91718266 0.87923545 0.87130296 0.9323181  0.81696054\n",
      " 0.93734542 0.9179357  0.88918677 0.92152104]\n",
      "Precision [0.07894737 0.01834862 0.15757576 0.06936416 0.14893617 0.05627706\n",
      " 0.14606742 0.06730769 0.22981366 0.08490566]\n",
      "ACC: [0.8922734  0.88187221 0.77117385 0.81872214 0.82912333 0.73254086\n",
      " 0.85438336 0.81129272 0.7667162  0.85289747]\n",
      "STarted testing with A:fgsm on L:fc2 with Rank:3\n",
      "\n",
      "Stats for attack type:fgsm on the layer:fc2 with a gradient rank:3\n",
      "Right max : 0.07060518731988473\n",
      "Fooled max: 0.08501440922190202\n",
      "Right min : 0.06051873198847262\n",
      "Binary  0.0\n",
      "Sensitivity [0.03389831 0.01818182 0.04568528 0.15384615 0.06586826 0.09146341\n",
      " 0.06993007 0.03468208 0.07619048 0.10852713]\n",
      "Specificity [0.92174567 0.93473368 0.86565911 0.86430224 0.91318591 0.82924837\n",
      " 0.92208835 0.88888889 0.93378608 0.8903892 ]\n",
      "Precision [0.01886792 0.01136364 0.05325444 0.07368421 0.09401709 0.06696429\n",
      " 0.09345794 0.04255319 0.17021277 0.09210526]\n",
      "ACC: [0.88400576 0.89841499 0.74927954 0.81772334 0.81123919 0.74207493\n",
      " 0.83429395 0.78242075 0.80403458 0.81772334]\n",
      "STarted testing with A:fgsm on L:conv1 with Rank:4\n",
      "\n",
      "Stats for attack type:fgsm on the layer:conv1 with a gradient rank:4\n",
      "Right max : 0.25615050651230103\n",
      "Fooled max: 0.21562952243125905\n",
      "Right min : 0.18234442836468887\n",
      "Binary  0.0\n",
      "Sensitivity [0.19672131 0.09836066 0.37837838 0.16455696 0.20833333 0.33333333\n",
      " 0.24264706 0.28654971 0.23786408 0.16806723]\n",
      "Specificity [0.92127176 0.94928085 0.88965517 0.91788181 0.92668863 0.9002453\n",
      " 0.94622793 0.92155244 0.8962585  0.89944576]\n",
      "Precision [0.10344828 0.08219178 0.39622642 0.10833333 0.28225806 0.30285714\n",
      " 0.33       0.34027778 0.28654971 0.13605442]\n",
      "ACC: [0.88929088 0.91172214 0.80752533 0.8748191  0.83936324 0.83502171\n",
      " 0.87698987 0.84298119 0.79811867 0.83646889]\n",
      "STarted testing with A:fgsm on L:relu1 with Rank:4\n",
      "\n",
      "Stats for attack type:fgsm on the layer:relu1 with a gradient rank:4\n",
      "Right max : 0.1701649175412294\n",
      "Fooled max: 0.18215892053973012\n",
      "Right min : 0.17391304347826086\n",
      "Binary  0.0\n",
      "Sensitivity [0.07462687 0.13793103 0.15508021 0.15909091 0.18589744 0.22727273\n",
      " 0.14615385 0.14857143 0.19512195 0.19298246]\n",
      "Specificity [0.925809   0.93495298 0.89189189 0.88041734 0.92105263 0.87457627\n",
      " 0.94767442 0.91285591 0.89459699 0.89098361]\n",
      "Precision [0.05050505 0.08791209 0.18954248 0.08588957 0.23770492 0.19125683\n",
      " 0.23170732 0.20472441 0.25157233 0.14193548]\n",
      "ACC: [0.88305847 0.90029985 0.7886057  0.83283358 0.83508246 0.79985007\n",
      " 0.86956522 0.8125937  0.78710645 0.83133433]\n",
      "STarted testing with A:fgsm on L:conv2 with Rank:4\n",
      "\n",
      "Stats for attack type:fgsm on the layer:conv2 with a gradient rank:4\n",
      "Right max : 0.10860366713681241\n",
      "Fooled max: 0.09520451339915374\n",
      "Right min : 0.11212976022566996\n",
      "Binary  0.0\n",
      "Sensitivity [0.03797468 0.05       0.14553991 0.14130435 0.07784431 0.19496855\n",
      " 0.03623188 0.11299435 0.12264151 0.07438017]\n",
      "Specificity [0.91262136 0.93078056 0.81244813 0.86802413 0.94484412 0.87450357\n",
      " 0.93046875 0.89846898 0.89966833 0.93138011]\n",
      "Precision [0.025      0.03092784 0.12062257 0.06914894 0.15853659 0.16402116\n",
      " 0.05319149 0.1369863  0.17687075 0.09183673]\n",
      "ACC: [0.86389281 0.89351199 0.7122708  0.82087447 0.84273625 0.79830748\n",
      " 0.84344147 0.80042313 0.78349788 0.85825106]\n",
      "STarted testing with A:fgsm on L:relu2 with Rank:4\n",
      "\n",
      "Stats for attack type:fgsm on the layer:relu2 with a gradient rank:4\n",
      "Right max : 0.16323529411764706\n",
      "Fooled max: 0.2088235294117647\n",
      "Right min : 0.1676470588235294\n",
      "Binary  0.0\n",
      "Sensitivity [0.10204082 0.0483871  0.17934783 0.12765957 0.15625    0.27380952\n",
      " 0.11627907 0.14705882 0.22119816 0.07874016]\n",
      "Specificity [0.92677346 0.93605547 0.87244898 0.8957346  0.9325     0.86073826\n",
      " 0.93582453 0.91428571 0.90201225 0.88888889]\n",
      "Precision [0.04950495 0.03488372 0.18032787 0.08333333 0.23584906 0.21698113\n",
      " 0.15957447 0.19685039 0.3        0.06802721]\n",
      "ACC: [0.89705882 0.89558824 0.77867647 0.84264706 0.84117647 0.78823529\n",
      " 0.85808824 0.81838235 0.79338235 0.81323529]\n",
      "STarted testing with A:fgsm on L:fc1 with Rank:4\n",
      "\n",
      "Stats for attack type:fgsm on the layer:fc1 with a gradient rank:4\n",
      "Right max : 0.15834522111269614\n",
      "Fooled max: 0.1718972895863053\n",
      "Right min : 0.07774607703281027\n",
      "Binary  0.0\n",
      "Sensitivity [0.203125   0.05405405 0.18367347 0.08333333 0.08641975 0.21511628\n",
      " 0.16666667 0.18285714 0.18981481 0.13043478]\n",
      "Specificity [0.90059791 0.9375     0.89800995 0.91730475 0.94112903 0.88455285\n",
      " 0.90944882 0.91768541 0.87183811 0.88344988]\n",
      "Precision [0.0890411  0.04597701 0.22641509 0.06896552 0.16091954 0.20670391\n",
      " 0.16058394 0.2406015  0.21243523 0.09090909]\n",
      "ACC: [0.86875892 0.89087019 0.79814551 0.86019971 0.84236805 0.80242511\n",
      " 0.83951498 0.82596291 0.76676177 0.82168331]\n",
      "STarted testing with A:fgsm on L:relu3 with Rank:4\n",
      "\n",
      "Stats for attack type:fgsm on the layer:relu3 with a gradient rank:4\n",
      "Right max : 0.08186309103740297\n",
      "Fooled max: 0.10797459421312633\n",
      "Right min : 0.13832039520112915\n",
      "Binary  0.0\n",
      "Sensitivity [0.21666667 0.         0.12380952 0.13402062 0.02380952 0.11258278\n",
      " 0.03378378 0.0441989  0.10232558 0.06349206]\n",
      "Specificity [0.81945468 0.9800885  0.83264292 0.86060606 0.94315452 0.84834123\n",
      " 0.9070134  0.94093851 0.91680532 0.93106119]\n",
      "Precision [0.0503876  0.         0.11403509 0.06598985 0.05333333 0.08133971\n",
      " 0.04065041 0.09876543 0.18032787 0.08247423]\n",
      "ACC: [0.79393084 0.93789697 0.72759351 0.81086803 0.83415667 0.76993649\n",
      " 0.81580805 0.82639379 0.79322512 0.85391673]\n",
      "STarted testing with A:fgsm on L:fc2 with Rank:4\n",
      "\n",
      "Stats for attack type:fgsm on the layer:fc2 with a gradient rank:4\n",
      "Right max : 0.13688760806916425\n",
      "Fooled max: 0.2010086455331412\n",
      "Right min : 0.07276657060518732\n",
      "Binary  0.0\n",
      "Sensitivity [0.15714286 0.03773585 0.12244898 0.12222222 0.09714286 0.20979021\n",
      " 0.07692308 0.17159763 0.15319149 0.15748031]\n",
      "Specificity [0.91122914 0.94831461 0.89597315 0.91371341 0.93981863 0.87710843\n",
      " 0.90222576 0.90812141 0.86556808 0.87311657]\n",
      "Precision [0.0859375  0.02816901 0.16216216 0.08943089 0.18888889 0.16393443\n",
      " 0.07518797 0.20567376 0.18848168 0.11111111]\n",
      "ACC: [0.87319885 0.91354467 0.78674352 0.86239193 0.83357349 0.80835735\n",
      " 0.82492795 0.8184438  0.74495677 0.80763689]\n",
      "STarted testing with A:mifgsm on L:conv1 with Rank:1\n",
      "\n",
      "Stats for attack type:mifgsm on the layer:conv1 with a gradient rank:1\n",
      "Right max : 0.48092280390417036\n",
      "Fooled max: 0.35403726708074534\n",
      "Right min : 0.32653061224489793\n",
      "Binary  0.0\n",
      "Sensitivity [0.43137255 0.22727273 0.53374233 0.30263158 0.54198473 0.66666667\n",
      " 0.43809524 0.56296296 0.46195652 0.28865979]\n",
      "Specificity [0.96096654 0.98984303 0.93983402 0.94100856 0.95381526 0.89350913\n",
      " 0.9667319  0.94758065 0.90243902 0.91941748]\n",
      "Precision [0.34375    0.47619048 0.6        0.27058824 0.60683761 0.47236181\n",
      " 0.575      0.59375    0.48022599 0.25225225]\n",
      "ACC: [0.93700089 0.96007098 0.88110027 0.89795918 0.90594499 0.86512866\n",
      " 0.91748004 0.90150843 0.83052351 0.86512866]\n",
      "STarted testing with A:mifgsm on L:relu1 with Rank:1\n",
      "\n",
      "Stats for attack type:mifgsm on the layer:relu1 with a gradient rank:1\n",
      "Right max : 0.4919786096256685\n",
      "Fooled max: 0.35650623885918004\n",
      "Right min : 0.3333333333333333\n",
      "Binary  0.0\n",
      "Sensitivity [0.41463415 0.23529412 0.59477124 0.20289855 0.54014599 0.61940299\n",
      " 0.48571429 0.55214724 0.52325581 0.30927835]\n",
      "Specificity [0.95282146 0.97945845 0.94530444 0.94966762 0.95532995 0.89979757\n",
      " 0.97246804 0.9468196  0.91368421 0.91512195]\n",
      "Precision [0.25       0.35294118 0.63194444 0.20895522 0.62711864 0.45604396\n",
      " 0.64556962 0.63829787 0.52325581 0.25641026]\n",
      "ACC: [0.93315508 0.9456328  0.89750446 0.90374332 0.90463458 0.86631016\n",
      " 0.92691622 0.88948307 0.85383244 0.8627451 ]\n",
      "STarted testing with A:mifgsm on L:conv2 with Rank:1\n",
      "\n",
      "Stats for attack type:mifgsm on the layer:conv2 with a gradient rank:1\n",
      "Right max : 0.5399495374264087\n",
      "Fooled max: 0.312026913372582\n",
      "Right min : 0.3843566021867115\n",
      "Binary  0.0\n",
      "Sensitivity [0.54166667 0.21153846 0.67231638 0.24705882 0.51428571 0.72592593\n",
      " 0.55102041 0.60689655 0.56923077 0.36842105]\n",
      "Specificity [0.95793164 0.98944591 0.94071146 0.94746377 0.96663489 0.89658444\n",
      " 0.97800183 0.94731801 0.93158954 0.92744186]\n",
      "Precision [0.35135135 0.47826087 0.66480447 0.26582278 0.6728972  0.47342995\n",
      " 0.69230769 0.61538462 0.62011173 0.35      ]\n",
      "ACC: [0.941127   0.95542473 0.90075694 0.89739277 0.91337258 0.87720774\n",
      " 0.94280908 0.9058032  0.87216148 0.87384357]\n",
      "STarted testing with A:mifgsm on L:relu2 with Rank:1\n",
      "\n",
      "Stats for attack type:mifgsm on the layer:relu2 with a gradient rank:1\n",
      "Right max : 0.4781849912739965\n",
      "Fooled max: 0.34642233856893545\n",
      "Right min : 0.34293193717277487\n",
      "Binary  0.0\n",
      "Sensitivity [0.47727273 0.17021277 0.51785714 0.24731183 0.57777778 0.75806452\n",
      " 0.51327434 0.46710526 0.49079755 0.26168224]\n",
      "Specificity [0.94283122 0.9899909  0.92331288 0.94586895 0.96439169 0.89432485\n",
      " 0.97773475 0.93158954 0.92980671 0.91530318]\n",
      "Precision [0.25       0.42105263 0.53703704 0.2875     0.68421053 0.46534653\n",
      " 0.71604938 0.51079137 0.53691275 0.24137931]\n",
      "ACC: [0.92495637 0.95636998 0.86387435 0.88917976 0.91884817 0.87958115\n",
      " 0.93193717 0.86998255 0.86736475 0.85427574]\n",
      "STarted testing with A:mifgsm on L:fc1 with Rank:1\n",
      "\n",
      "Stats for attack type:mifgsm on the layer:fc1 with a gradient rank:1\n",
      "Right max : 0.22164502164502164\n",
      "Fooled max: 0.25367965367965367\n",
      "Right min : 0.22683982683982684\n",
      "Binary  0.0\n",
      "Sensitivity [0.16666667 0.37735849 0.11111111 0.12820513 0.32167832 0.31343284\n",
      " 0.15841584 0.15602837 0.30810811 0.16326531]\n",
      "Specificity [0.98472597 0.77586207 0.9374359  0.93871866 0.92588933 0.91283056\n",
      " 0.9658444  0.91617357 0.84845361 0.92809839]\n",
      "Precision [0.29166667 0.07490637 0.24691358 0.13157895 0.38016529 0.32061069\n",
      " 0.30769231 0.20560748 0.27941176 0.17391304]\n",
      "ACC: [0.95497835 0.75757576 0.80865801 0.88398268 0.85108225 0.84329004\n",
      " 0.8952381  0.82337662 0.76190476 0.86320346]\n",
      "STarted testing with A:mifgsm on L:relu3 with Rank:1\n",
      "\n",
      "Stats for attack type:mifgsm on the layer:relu3 with a gradient rank:1\n",
      "Right max : 0.22810060711188204\n",
      "Fooled max: 0.26019080659150046\n",
      "Right min : 0.23503902862098872\n",
      "Binary  0.0\n",
      "Sensitivity [0.10638298 0.50980392 0.14035088 0.13253012 0.34814815 0.20714286\n",
      " 0.27619048 0.18656716 0.27868852 0.15384615]\n",
      "Specificity [0.97649186 0.78584392 0.95621181 0.95046729 0.92534381 0.90424482\n",
      " 0.95706107 0.90873405 0.84639175 0.9313632 ]\n",
      "Precision [0.16129032 0.09923664 0.35820896 0.171875   0.38211382 0.23015873\n",
      " 0.39189189 0.21186441 0.255      0.18181818]\n",
      "ACC: [0.94102342 0.773634   0.83521249 0.89158716 0.85776236 0.81960104\n",
      " 0.89505637 0.82480486 0.75628794 0.86123157]\n",
      "STarted testing with A:mifgsm on L:fc2 with Rank:1\n",
      "\n",
      "Stats for attack type:mifgsm on the layer:fc2 with a gradient rank:1\n",
      "Right max : 0.23575129533678757\n",
      "Fooled max: 0.24784110535405873\n",
      "Right min : 0.2452504317789292\n",
      "Binary  0.0\n",
      "Sensitivity [0.14893617 0.49019608 0.17816092 0.15853659 0.288      0.27083333\n",
      " 0.22222222 0.2147651  0.24848485 0.22115385]\n",
      "Specificity [0.9729973  0.82023487 0.94918699 0.94702602 0.90997096 0.90927022\n",
      " 0.94716619 0.92170466 0.8408862  0.9316888 ]\n",
      "Precision [0.18918919 0.11160714 0.38271605 0.18571429 0.27906977 0.29770992\n",
      " 0.32098765 0.28828829 0.20603015 0.24210526]\n",
      "ACC: [0.93955095 0.80569948 0.83333333 0.89119171 0.84283247 0.8298791\n",
      " 0.87392055 0.83074266 0.75647668 0.86787565]\n",
      "STarted testing with A:mifgsm on L:conv1 with Rank:2\n",
      "\n",
      "Stats for attack type:mifgsm on the layer:conv1 with a gradient rank:2\n",
      "Right max : 0.40580985915492956\n",
      "Fooled max: 0.3503521126760563\n",
      "Right min : 0.32922535211267606\n",
      "Binary  0.0\n",
      "Sensitivity [0.3902439  0.11764706 0.48295455 0.27631579 0.4765625  0.60606061\n",
      " 0.35185185 0.4496124  0.34375    0.29126214]\n",
      "Specificity [0.92876712 0.97788018 0.91875    0.93584906 0.94940476 0.89143426\n",
      " 0.97276265 0.93545184 0.91419492 0.90997096]\n",
      "Precision [0.17021277 0.2        0.52147239 0.23595506 0.54464286 0.42328042\n",
      " 0.57575758 0.47154472 0.44897959 0.24390244]\n",
      "ACC: [0.90933099 0.93926056 0.85123239 0.89172535 0.89612676 0.85827465\n",
      " 0.91373239 0.88028169 0.81778169 0.85387324]\n",
      "STarted testing with A:mifgsm on L:relu1 with Rank:2\n",
      "\n",
      "Stats for attack type:mifgsm on the layer:relu1 with a gradient rank:2\n",
      "Right max : 0.3799654576856649\n",
      "Fooled max: 0.3531951640759931\n",
      "Right min : 0.37305699481865284\n",
      "Binary  0.0\n",
      "Sensitivity [0.21052632 0.30508475 0.56024096 0.2247191  0.28461538 0.46341463\n",
      " 0.43859649 0.37908497 0.42934783 0.19607843]\n",
      "Specificity [0.95714286 0.98817106 0.90020161 0.91487371 0.95817121 0.86376812\n",
      " 0.97701149 0.94328358 0.89938398 0.90151515]\n",
      "Precision [0.14285714 0.58064516 0.484375   0.18018018 0.4625     0.28787879\n",
      " 0.67567568 0.50434783 0.44632768 0.16129032]\n",
      "ACC: [0.93264249 0.95336788 0.85146805 0.86183074 0.88255613 0.82124352\n",
      " 0.92400691 0.86873921 0.82469775 0.83937824]\n",
      "STarted testing with A:mifgsm on L:conv2 with Rank:2\n",
      "\n",
      "Stats for attack type:mifgsm on the layer:conv2 with a gradient rank:2\n",
      "Right max : 0.2844677137870855\n",
      "Fooled max: 0.2844677137870855\n",
      "Right min : 0.21727748691099477\n",
      "Binary  0.0\n",
      "Sensitivity [0.21052632 0.03174603 0.31914894 0.26865672 0.21192053 0.47154472\n",
      " 0.25       0.23846154 0.3715847  0.23076923]\n",
      "Specificity [0.93862816 0.98430286 0.86743215 0.86376274 0.94472362 0.87487781\n",
      " 0.95938104 0.94094488 0.9231568  0.90331754]\n",
      "Precision [0.10526316 0.10526316 0.32085561 0.10909091 0.36781609 0.31182796\n",
      " 0.4        0.34065934 0.47887324 0.17073171]\n",
      "ACC: [0.91448517 0.93193717 0.77748691 0.82897033 0.84816754 0.83158813\n",
      " 0.89005236 0.86125654 0.83507853 0.84991274]\n",
      "STarted testing with A:mifgsm on L:relu2 with Rank:2\n",
      "\n",
      "Stats for attack type:mifgsm on the layer:relu2 with a gradient rank:2\n",
      "Right max : 0.3810316139767055\n",
      "Fooled max: 0.38519134775374375\n",
      "Right min : 0.3652246256239601\n",
      "Binary  0.0\n",
      "Sensitivity [0.29166667 0.17241379 0.53977273 0.28571429 0.23357664 0.52066116\n",
      " 0.36697248 0.32026144 0.48979592 0.2920354 ]\n",
      "Specificity [0.94714038 0.98688811 0.90838207 0.92529253 0.94835681 0.87141536\n",
      " 0.97987191 0.93612965 0.90457256 0.89715335]\n",
      "Precision [0.18666667 0.4        0.5026455  0.23853211 0.36781609 0.31188119\n",
      " 0.64516129 0.42241379 0.5        0.22758621]\n",
      "ACC: [0.92096506 0.94758735 0.85440932 0.87687188 0.86688852 0.83610649\n",
      " 0.92429285 0.8577371  0.83693844 0.84026622]\n",
      "STarted testing with A:mifgsm on L:fc1 with Rank:2\n",
      "\n",
      "Stats for attack type:mifgsm on the layer:fc1 with a gradient rank:2\n",
      "Right max : 0.21164021164021163\n",
      "Fooled max: 0.19400352733686066\n",
      "Right min : 0.054673721340388004\n",
      "Binary  0.0\n",
      "Sensitivity [0.22222222 0.0754717  0.29113924 0.11842105 0.20863309 0.15966387\n",
      " 0.22689076 0.2826087  0.22777778 0.14285714]\n",
      "Specificity [0.87592593 0.93709528 0.90471311 0.93194707 0.92261307 0.93300493\n",
      " 0.89753695 0.92670683 0.88050314 0.91216216]\n",
      "Precision [0.08219178 0.05555556 0.33093525 0.11111111 0.27358491 0.2183908\n",
      " 0.20610687 0.34821429 0.26451613 0.13333333]\n",
      "ACC: [0.84479718 0.8968254  0.81922399 0.87742504 0.835097   0.85185185\n",
      " 0.82716049 0.84832451 0.77689594 0.84567901]\n",
      "STarted testing with A:mifgsm on L:relu3 with Rank:2\n",
      "\n",
      "Stats for attack type:mifgsm on the layer:relu3 with a gradient rank:2\n",
      "Right max : 0.2536924413553432\n",
      "Fooled max: 0.2562988705473501\n",
      "Right min : 0.19635099913119028\n",
      "Binary  0.0\n",
      "Sensitivity [0.24528302 0.07843137 0.25443787 0.24719101 0.248      0.26956522\n",
      " 0.24       0.17518248 0.41752577 0.16101695]\n",
      "Specificity [0.94808743 0.91727273 0.90529532 0.90583804 0.92397661 0.91119691\n",
      " 0.93815414 0.93984221 0.8338558  0.93901258]\n",
      "Precision [0.18571429 0.04210526 0.31617647 0.18032787 0.28440367 0.25203252\n",
      " 0.26966292 0.28235294 0.3375     0.23170732]\n",
      "ACC: [0.91572546 0.88010426 0.80973067 0.85490877 0.85056473 0.84708949\n",
      " 0.87749783 0.84882711 0.76368375 0.85925282]\n",
      "STarted testing with A:mifgsm on L:fc2 with Rank:2\n",
      "\n",
      "Stats for attack type:mifgsm on the layer:fc2 with a gradient rank:2\n",
      "Right max : 0.22307039864291772\n",
      "Fooled max: 0.1806615776081425\n",
      "Right min : 0.05258693808312129\n",
      "Binary  0.0\n",
      "Sensitivity [0.3255814  0.09677419 0.29714286 0.12195122 0.24242424 0.08965517\n",
      " 0.24576271 0.32142857 0.26486486 0.13402062]\n",
      "Specificity [0.88380282 0.94986571 0.88844622 0.93072015 0.91499522 0.94777563\n",
      " 0.90857681 0.92974013 0.87323944 0.90665434]\n",
      "Precision [0.09589041 0.09677419 0.31707317 0.11627907 0.26446281 0.19402985\n",
      " 0.23015873 0.38135593 0.28       0.11403509]\n",
      "ACC: [0.8634436  0.90500424 0.80067854 0.87446989 0.83969466 0.84223919\n",
      " 0.84223919 0.85750636 0.77777778 0.84308736]\n",
      "STarted testing with A:mifgsm on L:conv1 with Rank:3\n",
      "\n",
      "Stats for attack type:mifgsm on the layer:conv1 with a gradient rank:3\n",
      "Right max : 0.28608923884514437\n",
      "Fooled max: 0.2545931758530184\n",
      "Right min : 0.19510061242344706\n",
      "Binary  0.0\n",
      "Sensitivity [0.35416667 0.03773585 0.39880952 0.22368421 0.18939394 0.39393939\n",
      " 0.23404255 0.30821918 0.31843575 0.2       ]\n",
      "Specificity [0.91598174 0.95963303 0.91076923 0.91565136 0.9396637  0.88328388\n",
      " 0.94566254 0.93981946 0.90248963 0.89007782]\n",
      "Precision [0.1559633  0.04347826 0.43506494 0.1588785  0.29069767 0.30588235\n",
      " 0.27848101 0.42857143 0.37748344 0.16911765]\n",
      "ACC: [0.89238845 0.91688539 0.83552056 0.86964129 0.85301837 0.82677165\n",
      " 0.88713911 0.85914261 0.81102362 0.82064742]\n",
      "STarted testing with A:mifgsm on L:relu1 with Rank:3\n",
      "\n",
      "Stats for attack type:mifgsm on the layer:relu1 with a gradient rank:3\n",
      "Right max : 0.2341941228851291\n",
      "Fooled max: 0.21104185218165628\n",
      "Right min : 0.16918967052537845\n",
      "Binary  0.0\n",
      "Sensitivity [0.13043478 0.08510638 0.21341463 0.11842105 0.28030303 0.32520325\n",
      " 0.14414414 0.18791946 0.35       0.26315789]\n",
      "Specificity [0.91086351 0.95446097 0.88008342 0.89398281 0.92734612 0.899\n",
      " 0.9555336  0.91889117 0.89607635 0.90856031]\n",
      "Precision [0.05882353 0.0754717  0.23333333 0.075      0.33944954 0.28368794\n",
      " 0.26229508 0.26168224 0.39130435 0.21008403]\n",
      "ACC: [0.87889581 0.91807658 0.78272484 0.84149599 0.85129118 0.83615316\n",
      " 0.87533393 0.82190561 0.80854853 0.8539626 ]\n",
      "STarted testing with A:mifgsm on L:conv2 with Rank:3\n",
      "\n",
      "Stats for attack type:mifgsm on the layer:conv2 with a gradient rank:3\n",
      "Right max : 0.08880994671403197\n",
      "Fooled max: 0.11367673179396093\n",
      "Right min : 0.11367673179396093\n",
      "Binary  0.0\n",
      "Sensitivity [0.0754717  0.04166667 0.10526316 0.07142857 0.05405405 0.2109375\n",
      " 0.01052632 0.08029197 0.08426966 0.09708738]\n",
      "Specificity [0.90587139 0.93877551 0.8449692  0.87332054 0.9192229  0.88476954\n",
      " 0.91076625 0.9049545  0.89451477 0.90615836]\n",
      "Precision [0.03809524 0.02941176 0.09580838 0.04347826 0.09195402 0.19014085\n",
      " 0.01075269 0.1047619  0.13043478 0.09433962]\n",
      "ACC: [0.86678508 0.90053286 0.74511545 0.81349911 0.80550622 0.80817052\n",
      " 0.8348135  0.80461812 0.76642984 0.8321492 ]\n",
      "STarted testing with A:mifgsm on L:relu2 with Rank:3\n",
      "\n",
      "Stats for attack type:mifgsm on the layer:relu2 with a gradient rank:3\n",
      "Right max : 0.21623931623931625\n",
      "Fooled max: 0.2452991452991453\n",
      "Right min : 0.15982905982905982\n",
      "Binary  0.0\n",
      "Sensitivity [0.18965517 0.0483871  0.20454545 0.18309859 0.18045113 0.34558824\n",
      " 0.13913043 0.18320611 0.33333333 0.17142857]\n",
      "Specificity [0.91546763 0.96119134 0.87826962 0.90809827 0.93539055 0.86170213\n",
      " 0.94218009 0.91915303 0.89969605 0.90234742]\n",
      "Precision [0.1047619  0.06521739 0.22929936 0.11403509 0.26373626 0.24736842\n",
      " 0.20779221 0.22222222 0.38125    0.14754098]\n",
      "ACC: [0.87948718 0.91282051 0.77692308 0.86410256 0.84957265 0.8017094\n",
      " 0.86324786 0.83675214 0.81111111 0.83675214]\n",
      "STarted testing with A:mifgsm on L:fc1 with Rank:3\n",
      "\n",
      "Stats for attack type:mifgsm on the layer:fc1 with a gradient rank:3\n",
      "Right max : 0.09032846715328467\n",
      "Fooled max: 0.10218978102189781\n",
      "Right min : 0.072992700729927\n",
      "Binary  0.0\n",
      "Sensitivity [0.04444444 0.09836066 0.06040268 0.15068493 0.078125   0.07751938\n",
      " 0.08247423 0.04477612 0.09836066 0.19587629]\n",
      "Specificity [0.92388202 0.92850242 0.89017951 0.84164223 0.91322314 0.84281282\n",
      " 0.93193193 0.88461538 0.93866375 0.89389389]\n",
      "Precision [0.02439024 0.075      0.07964602 0.06358382 0.10638298 0.0617284\n",
      " 0.10526316 0.05128205 0.24324324 0.152     ]\n",
      "ACC: [0.88777372 0.88229927 0.77737226 0.79562044 0.81569343 0.75273723\n",
      " 0.85675182 0.78193431 0.79835766 0.83211679]\n",
      "STarted testing with A:mifgsm on L:relu3 with Rank:3\n",
      "\n",
      "Stats for attack type:mifgsm on the layer:relu3 with a gradient rank:3\n",
      "Right max : 0.08495575221238938\n",
      "Fooled max: 0.12123893805309735\n",
      "Right min : 0.11769911504424779\n",
      "Binary  0.0\n",
      "Sensitivity [0.10204082 0.05263158 0.12       0.06849315 0.04615385 0.08208955\n",
      " 0.08490566 0.07092199 0.12650602 0.05050505]\n",
      "Specificity [0.89824237 0.89748369 0.89005236 0.90066225 0.93       0.83634538\n",
      " 0.92773438 0.90394338 0.87448133 0.9214355 ]\n",
      "Precision [0.04347826 0.02654867 0.16666667 0.04545455 0.07894737 0.06321839\n",
      " 0.10843373 0.0952381  0.14788732 0.05813953]\n",
      "ACC: [0.86371681 0.85486726 0.77079646 0.84690265 0.82831858 0.74690265\n",
      " 0.84867257 0.8        0.76460177 0.84513274]\n",
      "STarted testing with A:mifgsm on L:fc2 with Rank:3\n",
      "\n",
      "Stats for attack type:mifgsm on the layer:fc2 with a gradient rank:3\n",
      "Right max : 0.07942238267148015\n",
      "Fooled max: 0.08935018050541517\n",
      "Right min : 0.07671480144404332\n",
      "Binary  0.0\n",
      "Sensitivity [0.06382979 0.025      0.05063291 0.15853659 0.07462687 0.10344828\n",
      " 0.04424779 0.02380952 0.09411765 0.15053763]\n",
      "Specificity [0.91705938 0.93539326 0.87578947 0.85672515 0.93326489 0.82866044\n",
      " 0.91557789 0.89511202 0.92537313 0.89064039]\n",
      "Precision [0.03296703 0.01428571 0.06349206 0.08125    0.13333333 0.08333333\n",
      " 0.05617978 0.02830189 0.18604651 0.112     ]\n",
      "ACC: [0.88086643 0.90252708 0.75812274 0.80505415 0.82942238 0.73375451\n",
      " 0.8267148  0.79602888 0.79783394 0.82851986]\n",
      "STarted testing with A:mifgsm on L:conv1 with Rank:4\n",
      "\n",
      "Stats for attack type:mifgsm on the layer:conv1 with a gradient rank:4\n",
      "Right max : 0.2783059636992221\n",
      "Fooled max: 0.21607605877268798\n",
      "Right min : 0.20743301642178047\n",
      "Binary  0.0\n",
      "Sensitivity [0.24074074 0.05263158 0.37016575 0.25       0.20408163 0.43181818\n",
      " 0.31683168 0.22222222 0.26553672 0.25      ]\n",
      "Specificity [0.93291024 0.94636364 0.86270492 0.90967742 0.93861386 0.91219512\n",
      " 0.95075758 0.92991115 0.89489796 0.91361502]\n",
      "Precision [0.14942529 0.0483871  0.33333333 0.15517241 0.32608696 0.3877551\n",
      " 0.38095238 0.31067961 0.31333333 0.2       ]\n",
      "ACC: [0.90060501 0.90233362 0.78565255 0.86862576 0.84528954 0.8573898\n",
      " 0.89541919 0.84183232 0.79861711 0.86084702]\n",
      "STarted testing with A:mifgsm on L:relu1 with Rank:4\n",
      "\n",
      "Stats for attack type:mifgsm on the layer:relu1 with a gradient rank:4\n",
      "Right max : 0.19360414866032843\n",
      "Fooled max: 0.1979256698357822\n",
      "Right min : 0.21175453759723423\n",
      "Binary  0.0\n",
      "Sensitivity [0.11904762 0.05660377 0.1954023  0.25675676 0.13970588 0.23571429\n",
      " 0.17241379 0.15441176 0.27272727 0.2       ]\n",
      "Specificity [0.90493274 0.94384058 0.89725331 0.89473684 0.93633692 0.8879056\n",
      " 0.94428434 0.90597453 0.88685015 0.89875836]\n",
      "Precision [0.04504505 0.04615385 0.25185185 0.14285714 0.22619048 0.2244898\n",
      " 0.25641026 0.17948718 0.30188679 0.171875  ]\n",
      "ACC: [0.87640449 0.90319793 0.79170268 0.85393258 0.84269663 0.80898876\n",
      " 0.86689715 0.81763181 0.79343129 0.83232498]\n",
      "STarted testing with A:mifgsm on L:conv2 with Rank:4\n",
      "\n",
      "Stats for attack type:mifgsm on the layer:conv2 with a gradient rank:4\n",
      "Right max : 0.10720562390158173\n",
      "Fooled max: 0.10369068541300527\n",
      "Right min : 0.10984182776801406\n",
      "Binary  0.0\n",
      "Sensitivity [0.05882353 0.07407407 0.17391304 0.11904762 0.04065041 0.12878788\n",
      " 0.10344828 0.104      0.11560694 0.0625    ]\n",
      "Specificity [0.89236431 0.94557196 0.81551363 0.87381404 0.93004926 0.90457256\n",
      " 0.92563601 0.89733465 0.90880829 0.90786948]\n",
      "Precision [0.025      0.06349206 0.15384615 0.06993007 0.06578947 0.15044248\n",
      " 0.13636364 0.11111111 0.18518519 0.05882353]\n",
      "ACC: [0.85500879 0.90421793 0.71177504 0.81810193 0.83391916 0.81458699\n",
      " 0.84182777 0.81019332 0.78822496 0.83655536]\n",
      "STarted testing with A:mifgsm on L:relu2 with Rank:4\n",
      "\n",
      "Stats for attack type:mifgsm on the layer:relu2 with a gradient rank:4\n",
      "Right max : 0.19121447028423771\n",
      "Fooled max: 0.20241171403962102\n",
      "Right min : 0.18776916451335057\n",
      "Binary  0.0\n",
      "Sensitivity [0.15555556 0.05357143 0.17791411 0.20430108 0.20149254 0.23846154\n",
      " 0.13513514 0.21374046 0.26203209 0.12612613]\n",
      "Specificity [0.89874552 0.94570136 0.88577154 0.91011236 0.9308666  0.87584869\n",
      " 0.93714286 0.91553398 0.90657084 0.89238095]\n",
      "Precision [0.05833333 0.04761905 0.2027972  0.16521739 0.2755102  0.19496855\n",
      " 0.18518519 0.24347826 0.35       0.11023622]\n",
      "ACC: [0.86993971 0.90267011 0.78639104 0.8535745  0.84668389 0.8044789\n",
      " 0.86046512 0.83634798 0.80275624 0.81912145]\n",
      "STarted testing with A:mifgsm on L:fc1 with Rank:4\n",
      "\n",
      "Stats for attack type:mifgsm on the layer:fc1 with a gradient rank:4\n",
      "Right max : 0.13763066202090593\n",
      "Fooled max: 0.17857142857142858\n",
      "Right min : 0.08885017421602788\n",
      "Binary  0.0\n",
      "Sensitivity [0.10526316 0.09259259 0.13661202 0.09876543 0.08730159 0.18548387\n",
      " 0.08943089 0.11538462 0.20903955 0.16964286]\n",
      "Specificity [0.8972973  0.95063985 0.90984456 0.91471415 0.94031311 0.86816406\n",
      " 0.90536585 0.90766208 0.86199794 0.88127413]\n",
      "Precision [0.03389831 0.08474576 0.22321429 0.08080808 0.15277778 0.14556962\n",
      " 0.10185185 0.13761468 0.21637427 0.13380282]\n",
      "ACC: [0.87108014 0.91027875 0.78658537 0.85714286 0.8466899  0.79442509\n",
      " 0.81794425 0.81794425 0.76132404 0.81184669]\n",
      "STarted testing with A:mifgsm on L:relu3 with Rank:4\n",
      "\n",
      "Stats for attack type:mifgsm on the layer:relu3 with a gradient rank:4\n",
      "Right max : 0.09699570815450644\n",
      "Fooled max: 0.11244635193133047\n",
      "Right min : 0.14163090128755365\n",
      "Binary  0.0\n",
      "Sensitivity [0.2745098  0.01639344 0.18934911 0.1625     0.04166667 0.10948905\n",
      " 0.08849558 0.02325581 0.06593407 0.07070707]\n",
      "Specificity [0.81777379 0.97463768 0.84939759 0.84423963 0.95494613 0.86381323\n",
      " 0.92585551 0.93146718 0.91658189 0.92026266]\n",
      "Precision [0.06451613 0.03448276 0.17582418 0.07142857 0.11538462 0.09677419\n",
      " 0.11363636 0.04054054 0.12765957 0.07608696]\n",
      "ACC: [0.79399142 0.92446352 0.75364807 0.79742489 0.84206009 0.7751073\n",
      " 0.84463519 0.83090129 0.78369099 0.84806867]\n",
      "STarted testing with A:mifgsm on L:fc2 with Rank:4\n",
      "\n",
      "Stats for attack type:mifgsm on the layer:fc2 with a gradient rank:4\n",
      "Right max : 0.13992869875222816\n",
      "Fooled max: 0.16755793226381463\n",
      "Right min : 0.09893048128342247\n",
      "Binary  0.0\n",
      "Sensitivity [0.2173913  0.12       0.10126582 0.11494253 0.2        0.15267176\n",
      " 0.12149533 0.10416667 0.16666667 0.12745098]\n",
      "Specificity [0.90055762 0.94029851 0.92219917 0.92463768 0.90780142 0.86881937\n",
      " 0.91231527 0.91717791 0.86354167 0.88333333]\n",
      "Precision [0.08547009 0.08571429 0.17582418 0.11363636 0.22881356 0.13333333\n",
      " 0.12745098 0.15625    0.17088608 0.09848485]\n",
      "ACC: [0.87254902 0.90374332 0.80659537 0.86185383 0.82263815 0.78520499\n",
      " 0.8368984  0.81283422 0.76292335 0.81461676]\n",
      "STarted testing with A:ifgsm on L:conv1 with Rank:1\n",
      "\n",
      "Stats for attack type:ifgsm on the layer:conv1 with a gradient rank:1\n",
      "Right max : 0.4873583260680035\n",
      "Fooled max: 0.34786399302528337\n",
      "Right min : 0.34437663469921537\n",
      "Binary  0.0\n",
      "Sensitivity [0.5        0.24       0.58709677 0.2195122  0.47916667 0.74100719\n",
      " 0.51923077 0.5177305  0.51704545 0.22727273]\n",
      "Specificity [0.94913715 0.98632634 0.93649194 0.94929577 0.95812562 0.90079365\n",
      " 0.96931927 0.95526839 0.91761071 0.90260366]\n",
      "Precision [0.29113924 0.44444444 0.59090909 0.25       0.62162162 0.50738916\n",
      " 0.62790698 0.61864407 0.53216374 0.1984127 ]\n",
      "ACC: [0.93112467 0.9537925  0.88927637 0.89712293 0.89799477 0.88142982\n",
      " 0.92850915 0.90148213 0.85614647 0.83783784]\n",
      "STarted testing with A:ifgsm on L:relu1 with Rank:1\n",
      "\n",
      "Stats for attack type:ifgsm on the layer:relu1 with a gradient rank:1\n",
      "Right max : 0.45606326889279436\n",
      "Fooled max: 0.3585237258347979\n",
      "Right min : 0.3321616871704745\n",
      "Binary  0.0\n",
      "Sensitivity [0.58823529 0.20634921 0.50574713 0.14864865 0.49206349 0.68531469\n",
      " 0.40869565 0.47407407 0.47682119 0.32075472]\n",
      "Specificity [0.93836247 0.98046512 0.93153527 0.94642857 0.94466403 0.88844221\n",
      " 0.97360704 0.94815553 0.9189463  0.92054264]\n",
      "Precision [0.30927835 0.38235294 0.57142857 0.16176471 0.52542373 0.46889952\n",
      " 0.63513514 0.55172414 0.47368421 0.29310345]\n",
      "ACC: [0.92267135 0.93760984 0.86643234 0.89455185 0.89455185 0.8629174\n",
      " 0.91652021 0.89191564 0.8602812  0.86467487]\n",
      "STarted testing with A:ifgsm on L:conv2 with Rank:1\n",
      "\n",
      "Stats for attack type:ifgsm on the layer:conv2 with a gradient rank:1\n",
      "Right max : 0.5118317265556529\n",
      "Fooled max: 0.3418054338299737\n",
      "Right min : 0.3654688869412796\n",
      "Binary  0.0\n",
      "Sensitivity [0.41176471 0.19565217 0.63473054 0.2804878  0.58474576 0.74045802\n",
      " 0.5203252  0.56153846 0.45086705 0.36666667]\n",
      "Specificity [0.96238532 0.99178082 0.9301848  0.95184136 0.93939394 0.89108911\n",
      " 0.98133595 0.9446093  0.94008264 0.91870715]\n",
      "Precision [0.33870968 0.5        0.6091954  0.31081081 0.52671756 0.46859903\n",
      " 0.77108434 0.56589147 0.57352941 0.34645669]\n",
      "ACC: [0.93777388 0.95968449 0.88694128 0.90359334 0.90271691 0.87379492\n",
      " 0.93163891 0.90096407 0.8659071  0.86064855]\n",
      "STarted testing with A:ifgsm on L:relu2 with Rank:1\n",
      "\n",
      "Stats for attack type:ifgsm on the layer:relu2 with a gradient rank:1\n",
      "Right max : 0.47405452946350046\n",
      "Fooled max: 0.3562005277044855\n",
      "Right min : 0.3368513632365875\n",
      "Binary  0.0\n",
      "Sensitivity [0.51219512 0.10909091 0.48965517 0.21686747 0.58015267 0.66923077\n",
      " 0.48780488 0.52941176 0.54748603 0.26315789]\n",
      "Specificity [0.95255474 0.98798521 0.93447581 0.9487666  0.94930417 0.90367428\n",
      " 0.98323471 0.93506494 0.90501044 0.90909091]\n",
      "Precision [0.28767123 0.31578947 0.52205882 0.25       0.5984252  0.47282609\n",
      " 0.77922078 0.52554745 0.51851852 0.24390244]\n",
      "ACC: [0.93667546 0.94547054 0.87774846 0.89533861 0.90677221 0.87686895\n",
      " 0.9296394  0.88654354 0.84872471 0.84432718]\n",
      "STarted testing with A:ifgsm on L:fc1 with Rank:1\n",
      "\n",
      "Stats for attack type:ifgsm on the layer:fc1 with a gradient rank:1\n",
      "Right max : 0.23893805309734514\n",
      "Fooled max: 0.26991150442477874\n",
      "Right min : 0.24955752212389382\n",
      "Binary  0.0\n",
      "Sensitivity [0.15217391 0.46153846 0.19318182 0.17105263 0.22413793 0.27777778\n",
      " 0.25233645 0.29285714 0.25       0.1443299 ]\n",
      "Specificity [0.97693727 0.80890538 0.95283019 0.9402277  0.92307692 0.90669371\n",
      " 0.95601173 0.91414141 0.85220126 0.92255566]\n",
      "Precision [0.21875    0.10434783 0.43037975 0.17105263 0.25       0.3030303\n",
      " 0.375      0.32539683 0.23783784 0.14893617]\n",
      "ACC: [0.94336283 0.79292035 0.83451327 0.88849558 0.85132743 0.82654867\n",
      " 0.88938053 0.83716814 0.75840708 0.85575221]\n",
      "STarted testing with A:ifgsm on L:relu3 with Rank:1\n",
      "\n",
      "Stats for attack type:ifgsm on the layer:relu3 with a gradient rank:1\n",
      "Right max : 0.231239092495637\n",
      "Fooled max: 0.25741710296684117\n",
      "Right min : 0.2425828970331588\n",
      "Binary  0.0\n",
      "Sensitivity [0.14285714 0.43103448 0.13888889 0.0952381  0.27480916 0.24590164\n",
      " 0.22       0.24342105 0.3021978  0.22105263]\n",
      "Specificity [0.97554348 0.79227941 0.94616977 0.94161959 0.91330049 0.92089844\n",
      " 0.94646272 0.9195171  0.85788382 0.93149382]\n",
      "Precision [0.18181818 0.09960159 0.32467532 0.11428571 0.29032258 0.27027027\n",
      " 0.28205128 0.31623932 0.28645833 0.22580645]\n",
      "ACC: [0.94502618 0.77399651 0.81937173 0.87958115 0.84031414 0.84904014\n",
      " 0.88307155 0.82984293 0.76963351 0.87260035]\n",
      "STarted testing with A:ifgsm on L:fc2 with Rank:1\n",
      "\n",
      "Stats for attack type:ifgsm on the layer:fc2 with a gradient rank:1\n",
      "Right max : 0.21355060034305318\n",
      "Fooled max: 0.2701543739279588\n",
      "Right min : 0.2169811320754717\n",
      "Binary  0.0\n",
      "Sensitivity [0.16326531 0.375      0.1299435  0.11764706 0.21705426 0.26153846\n",
      " 0.15833333 0.25517241 0.28042328 0.20212766]\n",
      "Specificity [0.96687556 0.78264758 0.95348837 0.94449584 0.91128255 0.90926641\n",
      " 0.95315488 0.93046033 0.85363357 0.92350746]\n",
      "Precision [0.17777778 0.06896552 0.33333333 0.14285714 0.23333333 0.265625\n",
      " 0.27941176 0.34259259 0.27040816 0.18811881]\n",
      "ACC: [0.93310463 0.76586621 0.82847341 0.88421955 0.83447684 0.83704974\n",
      " 0.87135506 0.8464837  0.76072041 0.86535163]\n",
      "STarted testing with A:ifgsm on L:conv1 with Rank:2\n",
      "\n",
      "Stats for attack type:ifgsm on the layer:conv1 with a gradient rank:2\n",
      "Right max : 0.4090909090909091\n",
      "Fooled max: 0.3493761140819964\n",
      "Right min : 0.30392156862745096\n",
      "Binary  0.0\n",
      "Sensitivity [0.36842105 0.10714286 0.46774194 0.225      0.44202899 0.57024793\n",
      " 0.38461538 0.45454545 0.48901099 0.17647059]\n",
      "Specificity [0.9400369  0.98311445 0.90384615 0.9328215  0.93495935 0.9010989\n",
      " 0.97642436 0.94040404 0.91914894 0.90549662]\n",
      "Precision [0.17721519 0.25       0.49152542 0.20454545 0.488      0.41071429\n",
      " 0.625      0.50420168 0.53939394 0.13274336]\n",
      "ACC: [0.92067736 0.93939394 0.8315508  0.88235294 0.87433155 0.86541889\n",
      " 0.92156863 0.88324421 0.84937611 0.85026738]\n",
      "STarted testing with A:ifgsm on L:relu1 with Rank:2\n",
      "\n",
      "Stats for attack type:ifgsm on the layer:relu1 with a gradient rank:2\n",
      "Right max : 0.39705882352941174\n",
      "Fooled max: 0.35726643598615915\n",
      "Right min : 0.3754325259515571\n",
      "Binary  0.0\n",
      "Sensitivity [0.31481481 0.29090909 0.60115607 0.20930233 0.34328358 0.4765625\n",
      " 0.36607143 0.30882353 0.48387097 0.26086957]\n",
      "Specificity [0.9446461  0.98365123 0.91149542 0.93271028 0.95499022 0.86478599\n",
      " 0.97222222 0.95490196 0.89896907 0.90507519]\n",
      "Precision [0.21794872 0.47058824 0.54450262 0.2        0.5        0.305\n",
      " 0.58571429 0.47727273 0.4787234  0.192     ]\n",
      "ACC: [0.91522491 0.95069204 0.8650519  0.87889273 0.88408304 0.82179931\n",
      " 0.91349481 0.87889273 0.83217993 0.85380623]\n",
      "STarted testing with A:ifgsm on L:conv2 with Rank:2\n",
      "\n",
      "Stats for attack type:ifgsm on the layer:conv2 with a gradient rank:2\n",
      "Right max : 0.2864816472694718\n",
      "Fooled max: 0.2757385854968666\n",
      "Right min : 0.20590868397493287\n",
      "Binary  0.0\n",
      "Sensitivity [0.20930233 0.03703704 0.31927711 0.27710843 0.26190476 0.46511628\n",
      " 0.23148148 0.2519685  0.32967033 0.23232323]\n",
      "Specificity [0.95344507 0.98682973 0.85909569 0.87137331 0.93340061 0.88967611\n",
      " 0.96035679 0.92828283 0.91016043 0.90569745]\n",
      "Precision [0.15254237 0.125      0.28342246 0.1474359  0.33333333 0.35502959\n",
      " 0.38461538 0.31067961 0.41666667 0.19327731]\n",
      "ACC: [0.92479857 0.94091316 0.77887198 0.82721576 0.85765443 0.84064458\n",
      " 0.88988362 0.85138765 0.81557744 0.84601611]\n",
      "STarted testing with A:ifgsm on L:relu2 with Rank:2\n",
      "\n",
      "Stats for attack type:ifgsm on the layer:relu2 with a gradient rank:2\n",
      "Right max : 0.36695652173913046\n",
      "Fooled max: 0.39652173913043476\n",
      "Right min : 0.37043478260869567\n",
      "Binary  0.0\n",
      "Sensitivity [0.36734694 0.1372549  0.49431818 0.24050633 0.32167832 0.4488189\n",
      " 0.35869565 0.32116788 0.43548387 0.27272727]\n",
      "Specificity [0.94732062 0.98817106 0.92094456 0.91503268 0.95134062 0.87194526\n",
      " 0.97258979 0.93780849 0.89522822 0.88846154]\n",
      "Precision [0.23684211 0.35       0.5304878  0.17272727 0.48421053 0.30319149\n",
      " 0.53225806 0.41121495 0.44505495 0.20547945]\n",
      "ACC: [0.9226087  0.95043478 0.85565217 0.86869565 0.87304348 0.82521739\n",
      " 0.92347826 0.86434783 0.82086957 0.82956522]\n",
      "STarted testing with A:ifgsm on L:fc1 with Rank:2\n",
      "\n",
      "Stats for attack type:ifgsm on the layer:fc1 with a gradient rank:2\n",
      "Right max : 0.22231985940246046\n",
      "Fooled max: 0.19244288224956063\n",
      "Right min : 0.06502636203866433\n",
      "Binary  0.0\n",
      "Sensitivity [0.21276596 0.02325581 0.31891892 0.19047619 0.27906977 0.16176471\n",
      " 0.2038835  0.28148148 0.2247191  0.10204082]\n",
      "Specificity [0.86709441 0.94611872 0.89296957 0.91840607 0.93161546 0.92015968\n",
      " 0.91690821 0.92622134 0.89166667 0.92307692]\n",
      "Precision [0.06451613 0.01666667 0.36645963 0.15686275 0.34285714 0.21568627\n",
      " 0.19626168 0.33928571 0.27777778 0.11111111]\n",
      "ACC: [0.8400703  0.9112478  0.79964851 0.86467487 0.85764499 0.82952548\n",
      " 0.85237258 0.84973638 0.78734622 0.85237258]\n",
      "STarted testing with A:ifgsm on L:relu3 with Rank:2\n",
      "\n",
      "Stats for attack type:ifgsm on the layer:relu3 with a gradient rank:2\n",
      "Right max : 0.2580364900086881\n",
      "Fooled max: 0.2675933970460469\n",
      "Right min : 0.18766290182450043\n",
      "Binary  0.0\n",
      "Sensitivity [0.33333333 0.1372549  0.3006135  0.1875     0.344      0.28378378\n",
      " 0.25225225 0.16176471 0.32       0.16666667]\n",
      "Specificity [0.93200363 0.93454545 0.90991903 0.92810458 0.93567251 0.88334995\n",
      " 0.93846154 0.93004926 0.84016393 0.93539055]\n",
      "Precision [0.17582418 0.08860759 0.35507246 0.16304348 0.39449541 0.26415094\n",
      " 0.30434783 0.23655914 0.26415094 0.22093023]\n",
      "ACC: [0.90703736 0.89921807 0.82363162 0.87662902 0.87141616 0.80625543\n",
      " 0.87228497 0.8392702  0.76107732 0.85925282]\n",
      "STarted testing with A:ifgsm on L:fc2 with Rank:2\n",
      "\n",
      "Stats for attack type:ifgsm on the layer:fc2 with a gradient rank:2\n",
      "Right max : 0.2252410166520596\n",
      "Fooled max: 0.18404907975460122\n",
      "Right min : 0.06485539000876424\n",
      "Binary  0.0\n",
      "Sensitivity [0.28301887 0.02040816 0.375      0.13157895 0.3046875  0.17687075\n",
      " 0.16666667 0.27007299 0.19393939 0.12745098]\n",
      "Specificity [0.87591912 0.95604396 0.89533679 0.9258216  0.92694965 0.92957746\n",
      " 0.91577928 0.90836653 0.88831967 0.91434071]\n",
      "Precision [0.1        0.02040816 0.39520958 0.11235955 0.34513274 0.27083333\n",
      " 0.17142857 0.28682171 0.22695035 0.12745098]\n",
      "ACC: [0.84837862 0.91586328 0.8150745  0.87291849 0.85714286 0.83260298\n",
      " 0.84487292 0.83172656 0.78790535 0.84399649]\n",
      "STarted testing with A:ifgsm on L:conv1 with Rank:3\n",
      "\n",
      "Stats for attack type:ifgsm on the layer:conv1 with a gradient rank:3\n",
      "Right max : 0.3161953727506427\n",
      "Fooled max: 0.23221936589545844\n",
      "Right min : 0.20651242502142245\n",
      "Binary  0.0\n",
      "Sensitivity [0.22222222 0.08510638 0.43975904 0.2        0.24822695 0.52777778\n",
      " 0.225      0.29285714 0.35195531 0.22916667]\n",
      "Specificity [0.93261456 0.94464286 0.9000999  0.91628335 0.93664717 0.8797654\n",
      " 0.95893028 0.94741967 0.89777328 0.92156863]\n",
      "Precision [0.13793103 0.06060606 0.42196532 0.14953271 0.35       0.38190955\n",
      " 0.38571429 0.43157895 0.38414634 0.20754717]\n",
      "ACC: [0.89974293 0.91002571 0.83461868 0.86718081 0.85347044 0.83633248\n",
      " 0.88346187 0.8688946  0.81405313 0.86461011]\n",
      "STarted testing with A:ifgsm on L:relu1 with Rank:3\n",
      "\n",
      "Stats for attack type:ifgsm on the layer:relu1 with a gradient rank:3\n",
      "Right max : 0.19318181818181818\n",
      "Fooled max: 0.2534965034965035\n",
      "Right min : 0.15646853146853146\n",
      "Binary  0.0\n",
      "Sensitivity [0.15686275 0.07017544 0.21787709 0.06024096 0.26190476 0.32258065\n",
      " 0.17       0.14583333 0.23255814 0.12962963]\n",
      "Specificity [0.91491308 0.97424103 0.87772021 0.91517436 0.93909627 0.85196078\n",
      " 0.93965517 0.907      0.89197531 0.88416988]\n",
      "Precision [0.07920792 0.125      0.24840764 0.05263158 0.34736842 0.20942408\n",
      " 0.2125     0.18421053 0.27586207 0.10447761]\n",
      "ACC: [0.88111888 0.9291958  0.77447552 0.85314685 0.86451049 0.79458042\n",
      " 0.87237762 0.81118881 0.79283217 0.81293706]\n",
      "STarted testing with A:ifgsm on L:conv2 with Rank:3\n",
      "\n",
      "Stats for attack type:ifgsm on the layer:conv2 with a gradient rank:3\n",
      "Right max : 0.10751748251748251\n",
      "Fooled max: 0.12674825174825174\n",
      "Right min : 0.0944055944055944\n",
      "Binary  0.0\n",
      "Sensitivity [0.05882353 0.05263158 0.18238994 0.05128205 0.07874016 0.15503876\n",
      " 0.08510638 0.10738255 0.10714286 0.08653846]\n",
      "Specificity [0.89752973 0.92364305 0.85076142 0.87054409 0.92822026 0.88669951\n",
      " 0.91904762 0.89849246 0.91561181 0.91634615]\n",
      "Precision [0.02608696 0.03488372 0.16477273 0.02816901 0.12048193 0.14814815\n",
      " 0.08602151 0.13675214 0.20792079 0.09375   ]\n",
      "ACC: [0.86013986 0.88024476 0.75786713 0.81468531 0.83391608 0.8041958\n",
      " 0.85052448 0.79545455 0.7770979  0.84090909]\n",
      "STarted testing with A:ifgsm on L:relu2 with Rank:3\n",
      "\n",
      "Stats for attack type:ifgsm on the layer:relu2 with a gradient rank:3\n",
      "Right max : 0.18776916451335057\n",
      "Fooled max: 0.2454780361757106\n",
      "Right min : 0.16106804478897502\n",
      "Binary  0.0\n",
      "Sensitivity [0.16666667 0.09433962 0.16766467 0.12048193 0.18120805 0.24460432\n",
      " 0.10309278 0.18656716 0.22826087 0.27722772]\n",
      "Specificity [0.92411924 0.9467509  0.88430584 0.90909091 0.93577075 0.87084149\n",
      " 0.94360902 0.90749757 0.89048106 0.87924528]\n",
      "Precision [0.09677419 0.078125   0.1958042  0.09259259 0.29347826 0.20481928\n",
      " 0.14285714 0.20833333 0.28187919 0.17948718]\n",
      "ACC: [0.88888889 0.90783807 0.78122308 0.85271318 0.83893196 0.79586563\n",
      " 0.87338501 0.82428941 0.78552972 0.82687339]\n",
      "STarted testing with A:ifgsm on L:fc1 with Rank:3\n",
      "\n",
      "Stats for attack type:ifgsm on the layer:fc1 with a gradient rank:3\n",
      "Right max : 0.07388316151202749\n",
      "Fooled max: 0.08676975945017182\n",
      "Right min : 0.058419243986254296\n",
      "Binary  0.0\n",
      "Sensitivity [0.06666667 0.0754717  0.04848485 0.12359551 0.06382979 0.12903226\n",
      " 0.09482759 0.02919708 0.05612245 0.09183673]\n",
      "Specificity [0.92493298 0.92529253 0.86686687 0.85674419 0.93059629 0.81826923\n",
      " 0.92652672 0.89873418 0.92561983 0.89587242]\n",
      "Precision [0.03448276 0.04597701 0.05673759 0.06666667 0.1125     0.07804878\n",
      " 0.125      0.03703704 0.13253012 0.075     ]\n",
      "ACC: [0.89175258 0.88659794 0.75085911 0.80068729 0.82560137 0.74484536\n",
      " 0.84364261 0.79639175 0.77920962 0.82817869]\n",
      "STarted testing with A:ifgsm on L:relu3 with Rank:3\n",
      "\n",
      "Stats for attack type:ifgsm on the layer:relu3 with a gradient rank:3\n",
      "Right max : 0.09771689497716896\n",
      "Fooled max: 0.12054794520547946\n",
      "Right min : 0.11963470319634703\n",
      "Binary  0.0\n",
      "Sensitivity [0.18181818 0.01818182 0.03896104 0.15189873 0.09929078 0.13605442\n",
      " 0.09375    0.03174603 0.14556962 0.10526316]\n",
      "Specificity [0.90865842 0.89230769 0.87566419 0.88188976 0.93186583 0.83438819\n",
      " 0.93293293 0.93292054 0.8719317  0.931     ]\n",
      "Precision [0.07692308 0.00884956 0.04878049 0.09090909 0.17721519 0.11299435\n",
      " 0.11842105 0.05797101 0.16083916 0.12658228]\n",
      "ACC: [0.87945205 0.84840183 0.75799087 0.82922374 0.82465753 0.74063927\n",
      " 0.85936073 0.82922374 0.76712329 0.85936073]\n",
      "STarted testing with A:ifgsm on L:fc2 with Rank:3\n",
      "\n",
      "Stats for attack type:ifgsm on the layer:fc2 with a gradient rank:3\n",
      "Right max : 0.08347676419965576\n",
      "Fooled max: 0.09208261617900172\n",
      "Right min : 0.07228915662650602\n",
      "Binary  0.0\n",
      "Sensitivity [0.05660377 0.07407407 0.07368421 0.1375     0.07575758 0.08333333\n",
      " 0.06363636 0.03125    0.10982659 0.13265306]\n",
      "Specificity [0.91433724 0.93501805 0.88888889 0.85674677 0.89029126 0.83398821\n",
      " 0.92775665 0.89941973 0.93832154 0.89473684]\n",
      "Precision [0.03061224 0.05263158 0.1147541  0.06626506 0.08130081 0.06629834\n",
      " 0.08433735 0.03703704 0.2375     0.104     ]\n",
      "ACC: [0.87521515 0.89500861 0.7555938  0.80722892 0.79776248 0.74096386\n",
      " 0.84595525 0.80378657 0.81497418 0.83046472]\n",
      "STarted testing with A:ifgsm on L:conv1 with Rank:4\n",
      "\n",
      "Stats for attack type:ifgsm on the layer:conv1 with a gradient rank:4\n",
      "Right max : 0.2713178294573643\n",
      "Fooled max: 0.2049956933677864\n",
      "Right min : 0.22652885443583118\n",
      "Binary  0.0\n",
      "Sensitivity [0.24444444 0.08888889 0.37362637 0.24705882 0.18320611 0.38095238\n",
      " 0.25242718 0.2244898  0.27225131 0.26415094]\n",
      "Specificity [0.93458781 0.94713262 0.87334014 0.89591078 0.93592233 0.91594203\n",
      " 0.94517958 0.92899408 0.89381443 0.91279621]\n",
      "Precision [0.13095238 0.06349206 0.35416667 0.15789474 0.26666667 0.35555556\n",
      " 0.30952381 0.31428571 0.33548387 0.23333333]\n",
      "ACC: [0.90783807 0.91386736 0.79500431 0.84840655 0.85099053 0.85788114\n",
      " 0.88372093 0.83979328 0.791559   0.8535745 ]\n",
      "STarted testing with A:ifgsm on L:relu1 with Rank:4\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "6_U32AoPJxTC"
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  }
 ]
}