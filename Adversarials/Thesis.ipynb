{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Thesis.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MAHA06/ThesisOulu/blob/master/Adversarials/Thesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54pec7A464Ta"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount(‘/content/gdrive’)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GzjWdjAfomN"
      },
      "source": [
        "import math\n",
        "# from torch.autograd.gradcheck import zero_gradients\n",
        "from cw2 import carlini_wagner_l2\n",
        "import seaborn as sns\n",
        "from torch.autograd import Variable\n",
        "import torch as torch\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import pickle\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms,datasets\n",
        "from os.path import exists\n",
        "np.random.seed(42) \n",
        "torch.manual_seed(42)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import save_image\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsxBn-i6fmfw"
      },
      "source": [
        "online=True\n",
        "\n",
        "img_size=(28,28)\n",
        "num_classes=10\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.0,), (1.0,))])\n",
        "dataset = datasets.MNIST(root = './data', train=True, transform = transform, download=True)\n",
        "train_set, val_set = torch.utils.data.random_split(dataset, [50000, 10000])\n",
        "test_set = datasets.MNIST(root = './data', train=False, transform = transform, download=True)\n",
        "train_loader = torch.utils.data.DataLoader(train_set,batch_size=1,shuffle=True) \n",
        "val_loader = torch.utils.data.DataLoader(val_set,batch_size=1,shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_set,batch_size=1,shuffle=True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDRayKODFRju"
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "    self.relu1=nn.ReLU()\n",
        "    self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "    self.relu2=nn.ReLU()\n",
        "    self.max_p1=nn.MaxPool2d(2)\n",
        "    self.dropout1 = nn.Dropout2d(0.25)\n",
        "    self.fc1 = nn.Linear(9216, 128)\n",
        "    self.relu3=nn.ReLU()\n",
        "    self.dropout2 = nn.Dropout2d(0.5)\n",
        "\n",
        "    self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.relu1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.relu2(x)\n",
        "    x = self.max_p1(x)\n",
        "    x = self.dropout1(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu3(x)\n",
        "    x = self.dropout2(x)\n",
        "    x = self.fc2(x)\n",
        "    output = F.log_softmax(x, dim=1)\n",
        "    return output\n",
        "\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJMQz9fdFRpn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aca51ae-6aac-42d6-dc4f-d1ec8750d863"
      },
      "source": [
        "use_cuda=True\n",
        "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
        "print(\"Using the device:\",device)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gl-WshaZFRsF"
      },
      "source": [
        "def fitModel(model,optimizer,scheduler,criterion,device,train_loader,val_loader,epochs):\n",
        "  data_loader={'train':train_loader,'val':val_loader}\n",
        "  print(\"Starting to fit the model\")\n",
        "  train_error,test_error=[],[]\n",
        "  best_points=np.zeros(img_size+(num_classes,),dtype=float)\n",
        "  best_scores=np.full(num_classes,-np.inf)\n",
        "  print(best_scores[1])\n",
        "  for epc in range(epochs):\n",
        "    correct=0\n",
        "    total=0\n",
        "    loss_epoch,val_loss_epoch=0,0\n",
        "    for phase in ('train','val'):\n",
        "\n",
        "      for i,data in enumerate(data_loader[phase]):\n",
        "        \n",
        "        input,label=data[0].to(device),data[1].to(device)\n",
        "        # print(input.shape)\n",
        "        out=model(input)\n",
        "        # print(out)\n",
        "        loss=criterion(out,label)\n",
        "        predict_label=torch.argmax(out,dim=1)\n",
        "        out=out.squeeze().detach().cpu().numpy()\n",
        "        if phase == 'train':\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          loss_epoch+=loss.item()\n",
        "          # print(predict_label.item(),label.item())\n",
        "          # print(out[predict_label.item()],best_scores[predict_label.item()])\n",
        "          # print(out)\n",
        "          if epc == epochs-1 and predict_label.item()==label.item() and out[predict_label.item()]>best_scores[predict_label.item()]:#if last epoch and the predicted label is correct\n",
        "            best_scores[predict_label.item()]=out[predict_label.item()]\n",
        "            best_points[:,:,predict_label]=input.squeeze().detach().cpu().numpy()\n",
        "            # print(label.item(),out[label.item()])\n",
        "            # print(\"The scores were updated:\",best_scores)\n",
        "        else:\n",
        "          total+=1\n",
        "          if predict_label==label:\n",
        "            correct+=1\n",
        "          val_loss_epoch+=loss.item()\n",
        "    scheduler.step(val_loss_epoch/len(val_loader))\n",
        "    print(\"Epoch : {} Accuracy : {}\".format(epc+1,correct/total))\n",
        "    print(\"Epoch : {} Train Loss : {} Eval Loss : {}\".format(epc+1,loss_epoch/len(train_loader),val_loss_epoch/len(val_loader)))\n",
        "    train_error.append(loss_epoch)\n",
        "    test_error.append(val_loss_epoch)\n",
        "  \n",
        "  return train_error,test_error,best_points,best_scores\n",
        "        \n",
        "          \n",
        "\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2VlV-xmFRuk"
      },
      "source": [
        "model=Net().to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),lr=0.0001, betas=(0.9, 0.999))\n",
        "criterion = nn.NLLLoss()\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
        "\n",
        "\n",
        "###############\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "AWmhvKHDgpCZ",
        "outputId": "7176e1b4-fc81-494e-9709-498e39079e29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## RUN THE TRAINING\n",
        "if(online):\n",
        "  loss,val_loss,best_points,best_scores=fitModel(model,optimizer,scheduler,criterion,device,train_loader,val_loader,10)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting to fit the model\n",
            "-inf\n",
            "Epoch : 1 Accuracy : 0.964\n",
            "Epoch : 1 Train Loss : 0.2809719892321084 Eval Loss : 0.128770722931983\n",
            "Epoch : 2 Accuracy : 0.9701\n",
            "Epoch : 2 Train Loss : 0.10423331830941181 Eval Loss : 0.10985168561769815\n",
            "Epoch : 3 Accuracy : 0.9737\n",
            "Epoch : 3 Train Loss : 0.08361956042722628 Eval Loss : 0.0972270224800066\n",
            "Epoch : 4 Accuracy : 0.9738\n",
            "Epoch : 4 Train Loss : 0.07509999243824193 Eval Loss : 0.10857437730478077\n",
            "Epoch : 5 Accuracy : 0.9755\n",
            "Epoch : 5 Train Loss : 0.07197592142436217 Eval Loss : 0.09111496984005918\n",
            "Epoch : 6 Accuracy : 0.9787\n",
            "Epoch : 6 Train Loss : 0.06904799065136984 Eval Loss : 0.0804546673782918\n",
            "Epoch : 7 Accuracy : 0.978\n",
            "Epoch : 7 Train Loss : 0.06508825486573737 Eval Loss : 0.0860824036057809\n",
            "Epoch : 8 Accuracy : 0.9786\n",
            "Epoch : 8 Train Loss : 0.06186806568235564 Eval Loss : 0.08805693026180589\n",
            "Epoch : 9 Accuracy : 0.9794\n",
            "Epoch : 9 Train Loss : 0.05893609165296004 Eval Loss : 0.08759447784395318\n",
            "Epoch : 10 Accuracy : 0.9779\n",
            "Epoch : 10 Train Loss : 0.05612957865402234 Eval Loss : 0.08857629569088568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAUXpIF_79an",
        "outputId": "bb1d99ce-58c3-458f-c79f-46c0c6f7ede8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "if online:\n",
        "  print(best_points.shape)\n",
        "  plt.imshow(best_points[:,:,3])\n",
        "  v=torch.tensor(best_points[:,:,3].reshape(1,1,28,28),dtype=torch.float32)\n",
        "  print(model(v.to(device)))\n",
        "  print(torch.argmax(model(v.to(device))))\n",
        "  print(best_scores)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28, 28, 10)\n",
            "tensor([[-41.0323, -39.5720, -29.8049,   0.0000, -35.3909, -18.7130, -50.9600,\n",
            "         -25.5387, -22.6081, -16.8371]], device='cuda:0',\n",
            "       grad_fn=<LogSoftmaxBackward0>)\n",
            "tensor(3, device='cuda:0')\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAOpElEQVR4nO3de4xc5X3G8efxHWyc2ibeWLaDCUFtnV4M2trhlrqxQI7zh0FENE4bORLNJlKgQaC0iFYKqlqEUC6ioqI1wYkTJURUkNoNSOCuqBA0cbyAwcZOwSV2sfEFahKbm+3d/fWPPY42sOed3bmdWb/fj7Sa2fObM/PzkZ89c+Y9c15HhACc/iZU3QCA9iDsQCYIO5AJwg5kgrADmZjUzheb4qkxTdPb+ZJAVt7RmzoRxz1SraGw214p6U5JEyV9KyJuTz1+mqZrmVc08pIAErZEb2mt7rfxtidK+idJn5C0WNIa24vrfT4ArdXIMftSSbsj4qWIOCHph5JWN6ctAM3WSNjnS3p52O/7imW/wXaP7T7bfSd1vIGXA9CIln8aHxHrIqI7Irona2qrXw5AiUbCvl/SwmG/LyiWAehAjYR9q6TzbZ9re4qkT0va1Jy2ADRb3UNvEdFv+zpJj2ho6G19RDzftM4ANFVD4+wR8bCkh5vUC4AW4nRZIBOEHcgEYQcyQdiBTBB2IBOEHcgEYQcyQdiBTBB2IBOEHcgEYQcyQdiBTBB2IBOEHcgEYQcyQdiBTBB2IBOEHcgEYQcyQdiBTBB2IBOEHcgEYQcyQdiBTBB2IBOEHcgEYQcyQdiBTBB2IBMNzeKKIXHJkmT94FeOJ+t/uzg9Ee7V018fc0+nvD74drL+9PHfStavOPNksj4Qg2Pu6ZR/fWNOsr7ptfR23fHqB5L1k9tmldbO2fir5LrxzOk3+3hDYbe9R9IxSQOS+iOiuxlNAWi+ZuzZ/yQiXmvC8wBoIY7ZgUw0GvaQ9Kjtp2z3jPQA2z22+2z3nVT62BVA6zT6Nv7SiNhve66kzbZ/HhGPD39ARKyTtE6SZnp2NPh6AOrU0J49IvYXt4cl/UjS0mY0BaD56g677em2zzp1X9IVknY0qzEAzeWI+t5Z2/6Qhvbm0tDhwA8i4h9S68z07FjmFXW9Xid76Y6LkvWdf3ZXsj5BTtYHVd3Rz+na26Y3y8fgJWn9xy9L1vv37U/Wq7IlenU0joy4Yeo+Zo+IlyT9Yd1dAWgrht6ATBB2IBOEHcgEYQcyQdiBTPAV1yaY8sv0ENC3jy5M1icq/TXRIwPTk/V/fvTy8uc+ke7t/U+lX3vmi8eS9Sq9sjz99dw//vOtpbW1c55Mrjswb3b6xTt06C2FPTuQCcIOZIKwA5kg7EAmCDuQCcIOZIKwA5lgnL0JFtz2X8n6A7fNbej5Jy2Yn6x/eN9PG3r+lPovFN1603/no8n6L0+eUVr7gykTk+u+fPlZyfqC8iH8jsWeHcgEYQcyQdiBTBB2IBOEHcgEYQcyQdiBTDDOPg506mWLW+3Nq5cl679/47PJ+l3znyit/aL/neS6H3woPU12J59/UIY9O5AJwg5kgrADmSDsQCYIO5AJwg5kgrADmWCcHUmTFi5I1o9clq4fXnW8tPapjzyTXPe2uXcn6xOd3lcNRPk18/9027XJdec+uytZH49q7tltr7d92PaOYctm295s+8XiNj3ZNYDKjeZt/HckrXzXspsl9UbE+ZJ6i98BdLCaYY+IxyUdedfi1ZI2FPc3SLqyyX0BaLJ6j9m7IuJAcf+gpK6yB9rukdQjSdN0Zp0vB6BRDX8aHxEhKRL1dRHRHRHdkzW10ZcDUKd6w37I9jxJKm4PN68lAK1Qb9g3SVpb3F8raWNz2gHQKjWP2W3fJ2m5pLNt75P0VUm3S7rf9rWS9kq6ppVNIm3wsgtKa/1n1rg++tr+ZP2RS+5K1j84qfza7I2q9Z3x7x1NX49//c3lnxt3/fjp5Lqlx6XjWM2wR8SaktKKJvcCoIU4XRbIBGEHMkHYgUwQdiAThB3IBF9xHQcO/eXFyfpjX/laaW3GhMbOWpxQ4xTnwQYGqQ4NvJ2sf+b6G5P1GU/sTtbP+L+fldZOx6G1WtizA5kg7EAmCDuQCcIOZIKwA5kg7EAmCDuQCcbZx4E5O8ovxyxJLw+U/83+3Qb/nP9v/1vJ+pHBKcl618QTpbWzJqS/frt/ebr5D29896URkcKeHcgEYQcyQdiBTBB2IBOEHcgEYQcyQdiBTDDOPg5Me+Fgsr76ketLax/4z/RYdi2zth5K1gd2/yJdX35hae3v19+TXPfMRUeTdYwNe3YgE4QdyARhBzJB2IFMEHYgE4QdyARhBzLhiPZdQXumZ8cyM/krhrxwzx8l67tX/UuyvqLnC8n61Ie2jrmn8W5L9OpoHPFItZp7dtvrbR+2vWPYsltt77e9rfhZ1cyGATTfaN7Gf0fSyhGWfzMilhQ/Dze3LQDNVjPsEfG4JK7/A4xzjXxAd53t54q3+bPKHmS7x3af7b6TSl9LDUDr1Bv2uyWdJ2mJpAOSvl72wIhYFxHdEdE9WY1NMgigfnWFPSIORcRARAxKukfS0ua2BaDZ6gq77XnDfr1K0o6yxwLoDDW/z277PknLJZ1te5+kr0pabnuJhqa53iMpPeAJjGDmzsnJ+uCq9Dkgvzo3vf7cMXd0eqsZ9ohYM8Lie1vQC4AW4nRZIBOEHcgEYQcyQdiBTBB2IBNcShqV6dryZkPrT1r5WvoBdzX09Kcd9uxAJgg7kAnCDmSCsAOZIOxAJgg7kAnCDmSCcXZUZk/5TNOjcvTZOcn67Mae/rTDnh3IBGEHMkHYgUwQdiAThB3IBGEHMkHYgUwwzo7KTNo5Pf2Aj6XL5128N1kfGGM/pzv27EAmCDuQCcIOZIKwA5kg7EAmCDuQCcIOZIJx9sLErvQEv29ctKi0dsa//azJ3Zw+Dt5wcWlt+xdrXdjdyerun5yTrJ+rV2o8f15q7tltL7T9mO2dtp+3/eVi+Wzbm22/WNzOan27AOo1mrfx/ZJuiojFkj4q6Uu2F0u6WVJvRJwvqbf4HUCHqhn2iDgQEU8X949J2iVpvqTVkjYUD9sg6cpWNQmgcWM6Zre9SNIFkrZI6oqIA0XpoKSuknV6JPVI0jSdWW+fABo06k/jbc+Q9ICkGyLi6PBaRISkGGm9iFgXEd0R0T1ZUxtqFkD9RhV225M1FPTvR8SDxeJDtucV9XmSDremRQDNUPNtvG1LulfSroj4xrDSJklrJd1e3G5sSYdtsuvv0sM42z/5j6W1L/7VyuS6R66akqwPHKru7+SkBfOT9bc+Mi9ZP/gXx5P1zcvuKK0N6ozkurUseujthtbPzWiO2S+R9FlJ221vK5bdoqGQ32/7Wkl7JV3TmhYBNEPNsEfEEyo/u2FFc9sB0CqcLgtkgrADmSDsQCYIO5AJwg5kgq+4Fj554XPJ+lRPLq19+5ze5Lq9T6ZPEz7Y/75kfSBa9zf5vCnpf/dl0/qT9cGRT5wcpnws/dBAepz8M9ffmH7mJ/lq8ViwZwcyQdiBTBB2IBOEHcgEYQcyQdiBTBB2IBOMsxde+tyiZP0n/76ltHbR1PTkwCvOeCtZn6D0eHPtsexWSl/O+fkT6XH4O14p/67/3jt/O7nujI0/TdYxNuzZgUwQdiAThB3IBGEHMkHYgUwQdiAThB3IhIcmc2mPmZ4dyzw+L0g7Ycni0toLN6Vnuvn5x7+VrD/wxtnJ+tUzXkvWU14ffCdZX9H3+WR9xv0zk/X3PfhMsh7H09eVR3NtiV4djSMjnhzBnh3IBGEHMkHYgUwQdiAThB3IBGEHMkHYgUzUHGe3vVDSdyV1SQpJ6yLiTtu3Svq8pFeLh94SEQ+nnms8j7MD40FqnH00F6/ol3RTRDxt+yxJT9neXNS+GRFfa1ajAFpnNPOzH5B0oLh/zPYuSfNb3RiA5hrTMbvtRZIukHTqGk3X2X7O9nrbs0rW6bHdZ7vvpDh1EqjKqMNue4akByTdEBFHJd0t6TxJSzS05//6SOtFxLqI6I6I7slKn0MOoHVGFXbbkzUU9O9HxIOSFBGHImIgIgYl3SNpaevaBNCommG3bUn3StoVEd8YtnzesIddJWlH89sD0Cyj+TT+EkmflbTd9rZi2S2S1theoqHhuD2SvtCSDgE0xWg+jX9CI188PDmmDqCzcAYdkAnCDmSCsAOZIOxAJgg7kAnCDmSCsAOZIOxAJgg7kAnCDmSCsAOZIOxAJgg7kAnCDmSirVM2235V0t5hi86WVP98xK3Vqb11al8SvdWrmb2dExHvH6nQ1rC/58XtvojorqyBhE7trVP7kuitXu3qjbfxQCYIO5CJqsO+ruLXT+nU3jq1L4ne6tWW3io9ZgfQPlXv2QG0CWEHMlFJ2G2vtP3ftnfbvrmKHsrY3mN7u+1ttvsq7mW97cO2dwxbNtv2ZtsvFrcjzrFXUW+32t5fbLtttldV1NtC24/Z3mn7edtfLpZXuu0SfbVlu7X9mN32REkvSLpc0j5JWyWtiYidbW2khO09krojovITMGx/TNIbkr4bEb9XLLtD0pGIuL34QzkrIv66Q3q7VdIbVU/jXcxWNG/4NOOSrpT0OVW47RJ9XaM2bLcq9uxLJe2OiJci4oSkH0paXUEfHS8iHpd05F2LV0vaUNzfoKH/LG1X0ltHiIgDEfF0cf+YpFPTjFe67RJ9tUUVYZ8v6eVhv+9TZ833HpIetf2U7Z6qmxlBV0QcKO4flNRVZTMjqDmNdzu9a5rxjtl29Ux/3ig+oHuvSyPiQkmfkPSl4u1qR4qhY7BOGjsd1TTe7TLCNOO/VuW2q3f680ZVEfb9khYO+31BsawjRMT+4vawpB+p86aiPnRqBt3i9nDF/fxaJ03jPdI04+qAbVfl9OdVhH2rpPNtn2t7iqRPS9pUQR/vYXt68cGJbE+XdIU6byrqTZLWFvfXStpYYS+/oVOm8S6bZlwVb7vKpz+PiLb/SFqloU/k/0fS31TRQ0lfH5L0bPHzfNW9SbpPQ2/rTmros41rJc2R1CvpRUn/IWl2B/X2PUnbJT2noWDNq6i3SzX0Fv05SduKn1VVb7tEX23ZbpwuC2SCD+iATBB2IBOEHcgEYQcyQdiBTBB2IBOEHcjE/wPIJF81y7HOoAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R59id5I4pOEj"
      },
      "source": [
        "if online:\n",
        "  filename = './model_MNIST'\n",
        "  pickle.dump(model, open(filename, 'wb'))\n",
        "  filename = './model_MNIST_cpu'\n",
        "  pickle.dump(model.cpu(), open(filename, 'wb'))\n",
        "  filename = './scores.mf'\n",
        "  best_markers={\"b_p\":best_points,\"b_s\":best_scores}\n",
        "  pickle.dump(best_markers,open(filename,'wb'))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1M92hyHXqrVI"
      },
      "source": [
        "def deepfool(image, net, num_classes=10, overshoot=0.02, max_iter=50):\n",
        "\n",
        "    \"\"\"\n",
        "       :param image: Image of size HxWx3\n",
        "       :param net: network (input: images, output: values of activation **BEFORE** softmax).\n",
        "       :param num_classes: num_classes (limits the number of classes to test against, by default = 10)\n",
        "       :param overshoot: used as a termination criterion to prevent vanishing updates (default = 0.02).\n",
        "       :param max_iter: maximum number of iterations for deepfool (default = 50)\n",
        "       :return: minimal perturbation that fools the classifier, number of iterations that it required, new estimated_label and perturbed image\n",
        "    \"\"\"\n",
        "    is_cuda = torch.cuda.is_available()\n",
        "\n",
        "    if is_cuda:\n",
        "        print(\"Using GPU\")\n",
        "        image = image.cuda()\n",
        "        net = net.cuda()\n",
        "\n",
        "\n",
        "\n",
        "    f_image = net.forward(image).data.cpu().numpy().flatten()\n",
        "    I = (np.array(f_image)).flatten().argsort()[::-1]\n",
        "\n",
        "    I = I[0:num_classes]\n",
        "    label = I[0]\n",
        "\n",
        "    input_shape = image.shape\n",
        "    pert_image = copy.deepcopy(image)\n",
        "    w = np.zeros(input_shape)\n",
        "    r_tot = np.zeros(input_shape)\n",
        "\n",
        "    loop_i = 0\n",
        "\n",
        "    x = pert_image\n",
        "    fs = net.forward(x)\n",
        "    fs_list = [fs[0,I[k]] for k in range(num_classes)]\n",
        "    k_i = label\n",
        "\n",
        "    while k_i == label and loop_i < max_iter:\n",
        "\n",
        "        pert = np.inf\n",
        "        fs[0, I[0]].backward(retain_graph=True)\n",
        "        grad_orig = x.grad.data.cpu().numpy().copy()\n",
        "\n",
        "        for k in range(1, num_classes):\n",
        "            zero_gradients(x)\n",
        "\n",
        "            fs[0, I[k]].backward(retain_graph=True)\n",
        "            cur_grad = x.grad.data.cpu().numpy().copy()\n",
        "\n",
        "            # set new w_k and new f_k\n",
        "            w_k = cur_grad - grad_orig\n",
        "            f_k = (fs[0, I[k]] - fs[0, I[0]]).data.cpu().numpy()\n",
        "\n",
        "            pert_k = abs(f_k)/np.linalg.norm(w_k.flatten())\n",
        "\n",
        "            # determine which w_k to use\n",
        "            if pert_k < pert:\n",
        "                pert = pert_k\n",
        "                w = w_k\n",
        "\n",
        "        # compute r_i and r_tot\n",
        "        # Added 1e-4 for numerical stability\n",
        "        r_i =  (pert+1e-4) * w / np.linalg.norm(w)\n",
        "        r_tot = np.float32(r_tot + r_i)\n",
        "\n",
        "        if is_cuda:\n",
        "            pert_image = image + (1+overshoot)*torch.from_numpy(r_tot).cuda()\n",
        "        else:\n",
        "            pert_image = image + (1+overshoot)*torch.from_numpy(r_tot)\n",
        "\n",
        "        x = Variable(pert_image, requires_grad=True)\n",
        "        fs = net.forward(x)\n",
        "        k_i = np.argmax(fs.data.cpu().numpy().flatten())\n",
        "\n",
        "        loop_i += 1\n",
        "\n",
        "    r_tot = (1+overshoot)*r_tot\n",
        "\n",
        "    return r_tot, loop_i, label, k_i, pert_image\n",
        "def fgsm_attack(input,epsilon,data_grad):\n",
        "  pert_out = input + epsilon*data_grad.sign()\n",
        "  pert_out = torch.clamp(pert_out, 0, 1)\n",
        "  return pert_out\n",
        "\n",
        "def ifgsm_attack(input,epsilon,data_grad):\n",
        "  iter = 10\n",
        "  alpha = epsilon/iter\n",
        "  pert_out = input\n",
        "  for i in range(iter-1):\n",
        "    pert_out = pert_out + alpha*data_grad.sign()\n",
        "    pert_out = torch.clamp(pert_out, 0, 1)\n",
        "    if torch.norm((pert_out-input),p=float('inf')) > epsilon:\n",
        "      break\n",
        "  return pert_out\n",
        "\n",
        "def mifgsm_attack(input,epsilon,data_grad):\n",
        "  iter=10\n",
        "  decay_factor=1.0\n",
        "  pert_out = input\n",
        "  alpha = epsilon/iter\n",
        "  g=0\n",
        "  for i in range(iter-1):\n",
        "    g = decay_factor*g + data_grad/torch.norm(data_grad,p=1)\n",
        "    pert_out = pert_out + alpha*torch.sign(g)\n",
        "    pert_out = torch.clamp(pert_out, 0, 1)\n",
        "    if torch.norm((pert_out-input),p=float('inf')) > epsilon:\n",
        "      break\n",
        "  return pert_out"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1wEKDumo2wC"
      },
      "source": [
        "def JacobianMatrix(model,input):\n",
        "  print(\"Input size \",input.shape)\n",
        "  number_of_piels=input.shape[0]*input.shape[1]\n",
        "  J = torch.zeros ((1, 784, 10))   # loop will fill in Jacobian\n",
        "  input.requires_grad = True\n",
        "  preds = model (input)\n",
        "  for  i in range (10):\n",
        "      grd = torch.zeros ((1, 10))   # same shape as preds\n",
        "      grd[0, i] = 1    # column of Jacobian to compute\n",
        "      preds.backward (gradient = grd, retain_graph = True)\n",
        "      J[:,:,i] = input.grad   # fill in one column of Jacobian\n",
        "      input.grad.zero_()   # .backward() accumulates gradients, so reset to zero\n",
        "def JSM_attack(model, input):\n",
        "  input.requires_grad=True"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "ySq2vSmOJxS7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "026d6f96-f90a-4286-cdef-21a63874d49a"
      },
      "source": [
        "activation_in = {}\n",
        "activation_out={}\n",
        "def get_activation_input(name):\n",
        "    def hook(model, input, output):\n",
        "        activation_in[name] = input[0]\n",
        "    return hook\n",
        "def get_activation_output(name):\n",
        "    def hook(model, input, output):\n",
        "        activation_out[name] = output.clone().detach()\n",
        "    return hook\n",
        "def nth_derivative(f, wrt, n):\n",
        "\n",
        "    for i in range(n):\n",
        "\n",
        "        grads = torch.autograd.grad(f, wrt, create_graph=True)[0]\n",
        "        f = grads.sum()\n",
        "\n",
        "    return grads\n",
        "def get_higher_gradients(model,input,label, rank=3):\n",
        "  grads=[]\n",
        "  optimizer.zero_grad()\n",
        "  input.requires_grad=True\n",
        "  outs=model(input)\n",
        "  # pred=torch.zeros(num_classes)\n",
        "  # pred[label.item()]=1\n",
        "  l=criterion(outs,label)\n",
        "  aux=torch.autograd.grad(l,input,retain_graph=True,create_graph=True)[0]\n",
        "  grads.append(aux.reshape(1,-1))\n",
        "  for r in range(2,rank+1):\n",
        "    aux=torch.autograd.grad(aux.sum(),input,create_graph=True)[0]\n",
        "    grads.append(aux.reshape(1,-1))\n",
        "  return grads\n",
        "def check_taylor_prediction(model,input,marker_images,check_label,centered,layer_name,rank):\n",
        "  # plt.imshow(input.squeeze().detach().cpu().numpy())\n",
        "  # grads=get_higher_gradients(model,input,true_label)\n",
        "\n",
        "  # grads=get_higher_gradients(model,input,perturbed_label)\n",
        "  # layer_grads=get_layer_gradients(model,input,check_label,\"conv2\")\n",
        "\n",
        "  # input=input.reshape(-1,1)\n",
        "  # # marker_image=marker_images[:,:,true_label.item()].reshape(-1,1)\n",
        "  # # marker_image=torch.tensor(marker_image,dtype=torch.float32)\n",
        "  # # total=1.+torch.mm(grads[0],(input-marker_image))+torch.mm(grads[1],torch.pow((input-marker_image),2))/2\n",
        "  # # total=1.+torch.mm(grads[0],(input-markerimage))+torch.mm(grads[1],torch.pow((input-marker_image),2))/2\n",
        "  # marker_image1=marker_images[:,:,perturbed_label.item()].reshape(-1,1)\n",
        "  # marker_image1=torch.tensor(marker_image1,dtype=torch.float32)\n",
        "  # total1=torch.mm(layer_grads[0],(input-marker_image1))+torch.mm(layer_grads[1],torch.pow((input-marker_image1),2))/2\n",
        "  marker_img=torch.tensor(marker_images[:,:,check_label.item()],device=device,dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
        "  check_label_tns=torch.tensor(check_label,device=device)\n",
        "  total=0\n",
        "  if centered==\"marker\":\n",
        "    total=compute_marker_image_prediction(model,input,marker_img,check_label_tns,layer_name,rank)\n",
        "  else:\n",
        "    total=compute_unknown_image_prediction(model,input,marker_img,check_label_tns,layer_name,rank)\n",
        "\n",
        "  return total\n",
        "  # return total_marker\n",
        "def compute_marker_image_prediction(model,unk_image,marker_img,marker_label,l_name,rank):\n",
        "  marker_img.requires_grad=True\n",
        "  grads,input_marker,loss_marker=get_layer_gradients(model,marker_img,marker_label,l_name,rank)\n",
        "  optimizer.zero_grad()\n",
        "  input_marker=input_marker.view(-1,1)\n",
        "  # print(input.shape)\n",
        "  outs=model(unk_image)\n",
        "  input_layer=activation_in[l_name].view(-1,1)\n",
        "  total=loss_marker.item()\n",
        "  for i in range(0,rank):\n",
        "    total+=torch.mm(grads[i],torch.pow((input_layer-input_marker),i+1))/(math.factorial(i+1))\n",
        "  return total.item()\n",
        "def compute_unknown_image_prediction(model,unk_image,marker_image,marker_label,l_name,rank):\n",
        "\n",
        "  grads,unk_input,loss_unk=get_layer_gradients(model,unk_image,marker_label,l_name,rank)\n",
        "  optimizer.zero_grad()\n",
        "  layer_input=unk_input.view(-1,1)\n",
        "  # print(input.shape)\n",
        "  outs=model(marker_image)\n",
        "  input_layer=activation_in[l_name].view(-1,1)\n",
        "  # total=loss_unk.item()\n",
        "  total=0\n",
        "  for i in range(0,rank):\n",
        "    total+=torch.mm(grads[i],torch.pow((input_layer-layer_input),i+1))/(math.factorial(i+1))\n",
        "  # total=loss_unk.item()+torch.mm(grads[0],(input_layer-layer_input))+torch.mm(grads[1],torch.pow((input_layer-layer_input),2))/2\n",
        "\n",
        "  return total.item()\n",
        "\n",
        "def get_layer_gradients(model,target_img,label,l_name, rank):\n",
        "  grads=[]\n",
        "  optimizer.zero_grad()\n",
        "  # target_img.requires_grad=True\n",
        "  outs=model(target_img)\n",
        "  # pred=torch.zeros((1,num_classes),dtype=np.int)\n",
        "  # pred[label.item()]=1\n",
        "  loss_value=criterion(outs,label)\n",
        "  layer_input_data=activation_in[l_name]\n",
        "  # layer_input_data.requires_grad=True\n",
        "  # loss_value.backward()\n",
        "  aux=torch.autograd.grad(loss_value,layer_input_data,retain_graph=True,create_graph=True)[0]\n",
        "  # aux=layer_input_data.grad\n",
        "  grads.append(aux.reshape(1,-1))\n",
        "  for r in range(1,rank):\n",
        "    aux=torch.autograd.grad(aux.sum(),layer_input_data,create_graph=True)[0]\n",
        "    grads.append(aux.reshape(1,-1))\n",
        "  return grads,layer_input_data,loss_value\n",
        "\n",
        "def get_marker_images(model,data_loader):\n",
        "  best_points=np.zeros(img_size+(num_classes,),dtype=float)\n",
        "  best_scores=np.full(num_classes,np.inf)\n",
        "  for i,data in enumerate(data_loader):\n",
        "\n",
        "      input,label=data[0].to(device),data[1].to(device)\n",
        "      # print(input.shape)\n",
        "      out=model(input)\n",
        "      # print(out)\n",
        "      loss=criterion(out,label)\n",
        "      predict_label=torch.argmax(out,dim=1)\n",
        "      out=out.squeeze().detach().cpu().numpy()\n",
        "\n",
        "      if   predict_label.item()==label.item() and loss<best_scores[predict_label.item()]:#if last epoch and the predicted label is correct\n",
        "        best_scores[predict_label.item()]=loss\n",
        "        best_points[:,:,predict_label.item()]=input.squeeze().detach().cpu().numpy()\n",
        "  best_markers={\"b_p\":best_points,\"b_s\":best_scores}\n",
        "  return best_markers\n",
        "if(device.type==\"cpu\"):\n",
        "  filename = './model_MNIST_cpu'\n",
        "else:\n",
        "  filename=\"./model_MNIST\"\n",
        "model= pickle.load(open(filename, 'rb'))\n",
        "filename = './scores.mf'\n",
        "# best_fits=get_marker_images(model,train_loader)\n",
        "# pickle.dump(best_fits, open(filename, 'wb'))\n",
        "best_fits=0\n",
        "if (exists(filename)) :\n",
        "  best_fits=pickle.load(open(filename,'rb'))\n",
        "else :\n",
        "  best_fits=get_marker_images(model,train_loader)\n",
        "  pickle.dump(best_fits, open(filename, 'wb'))\n",
        "\n",
        "def compute_statistics(confusion_matrix):\n",
        "  FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
        "  FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
        "  TP = np.diag(confusion_matrix)\n",
        "  TN = confusion_matrix.sum() - (FP + FN + TP)\n",
        "\n",
        "  # Sensitivity, hit rate, recall, or true positive rate\n",
        "  TPR = TP/(TP+FN)\n",
        "  print(\"Sensitivity\",TPR)\n",
        "  # Specificity or true negative rate\n",
        "  TNR = TN/(TN+FP)\n",
        "  print(\"Specificity\",TNR)\n",
        "  # Precision or positive predictive value\n",
        "  PPV = TP/(TP+FP)\n",
        "  print(\"Precision\",PPV)\n",
        "  # Negative predictive value\n",
        "  NPV = TN/(TN+FN)\n",
        "  # Fall out or false positive rate\n",
        "  FPR = FP/(FP+TN)\n",
        "  # False negative rate\n",
        "  FNR = FN/(TP+FN)\n",
        "  # False discovery rate\n",
        "  FDR = FP/(TP+FP)\n",
        "  # Overall accuracy\n",
        "  ACC = (TP+TN)/(TP+FP+FN+TN)\n",
        "  print(\"ACC:\",ACC)\n",
        "def testModel(model,criterion,device,test_loader,attack,rank,centered,l_name):\n",
        "  print(\"Running on \",device);\n",
        "  print(\"Started testing with A:{} on L:{} with Rank:{} with the center:{}\".format(attack,l_name,rank,centered))\n",
        "  right=0\n",
        "  totals=0\n",
        "  fooled=0\n",
        "  more_labels=0\n",
        "  true_small=0\n",
        "  binary=0\n",
        "  true_min=0\n",
        "  loss_e=0.02\n",
        "  loser_count=0\n",
        "  c_matrix_data=[]\n",
        "  correct_samples=0\n",
        "  number_samples=0\n",
        "  normal_samples_taylor=0\n",
        "  for data,labels in test_loader:\n",
        "    number_samples+=1\n",
        "   \n",
        "    data,labels=data.to(device),labels.to(device)\n",
        "    data.requires_grad=True\n",
        "    ################\n",
        "    taylors=range(0,num_classes)\n",
        "    taylors=[check_taylor_prediction(model,data,best_fits['b_p'],torch.tensor(lab).reshape(1),centered,layer_name=l_name,rank=rank) for lab in taylors ]\n",
        "    max_l=np.argmax(taylors)\n",
        "    if(max_l==labels):\n",
        "      normal_samples_taylor+=1\n",
        "      ##################\n",
        "\n",
        "\n",
        "\n",
        "    outs=model(data)\n",
        "    \n",
        "    true_preds=torch.argmax(outs,dim=1)\n",
        "    if true_preds!=labels:\n",
        "      continue\n",
        "    correct_samples+=1\n",
        "    epsilon=0.1\n",
        "    true_loss=criterion(outs,labels)\n",
        "    true_loss.backward()\n",
        "    data_grad=data.grad.data\n",
        "    if attack == \"fgsm\":\n",
        "      perturbed_data = fgsm_attack(data,epsilon,data_grad)\n",
        "    elif attack == \"ifgsm\":\n",
        "      perturbed_data = ifgsm_attack(data,epsilon,data_grad)\n",
        "    elif attack == \"mifgsm\":\n",
        "      perturbed_data = mifgsm_attack(data,epsilon,data_grad)\n",
        "    elif attack==\"deepfool\":\n",
        "      perturbed_data=deepfool(data,model)[4]\n",
        "    elif attack == \"jacobian\":\n",
        "      print(\"Implement the jacobian\")\n",
        "    elif attack==\"cw2\":\n",
        "      perturbed_data=carlini_wagner_l2(model,data,labels,max_iterations=80)\n",
        "    p_out=model(perturbed_data)\n",
        "    perturb_label=torch.argmax(p_out,dim=1)\n",
        "\n",
        "    if perturb_label!=labels:\n",
        "      totals+=1\n",
        "      taylors=range(0,num_classes)\n",
        "      taylors=[check_taylor_prediction(model,perturbed_data,best_fits['b_p'],torch.tensor(lab).reshape(1),centered,layer_name=l_name,rank=rank) for lab in taylors ]\n",
        "      taylor_np=np.array(taylors)\n",
        "      winning_labels=np.where((taylor_np>-5) )\n",
        "     \n",
        "      pert_loss=criterion(p_out, labels)\n",
        "      abs_x=[abs(t) for t in taylors]\n",
        "      max_l=np.argmax(taylors)\n",
        "      min_l=np.argmin(abs_x)\n",
        "      taylor_np[taylor_np<0]=np.inf\n",
        "      true_min=np.argmin(taylor_np)\n",
        "      smallest_indexs=np.array(abs_x).argsort()[:2]\n",
        "      c_matrix_data.append([ labels.item(),max_l])\n",
        "      if len([x if x>0. for x in taylor_np]):\n",
        "        more_labels+=1\n",
        "      if true_min==labels.item():\n",
        "        true_small+=1\n",
        "      if max_l == perturb_label.item():\n",
        "        fooled_max+=1\n",
        "      if max_l == labels.item():\n",
        "        right+=1\n",
        "     \n",
        "      if min_l == labels.item():\n",
        "        loser_count+=1\n",
        "  \n",
        "      print(\"Stats for attack type:\"+attack +\" on the layer:\"+l_name+\" with a gradient rank:\"+str(rank))\n",
        "      print(\"Normal taylor predictions\",normal_samples_taylor/number_samples)\n",
        "      print(\"More labels detected{}\".format(more_labels/totals))\n",
        "      print(\"Overall test accuracy:{}\".format(correct_samples/number_samples))\n",
        "      print(\"Right max :\",right/totals)\n",
        "      print(\"Fooled max:\",fooled_max/totals)\n",
        "      print(\"Right min :\",loser_count/totals)\n",
        "      print(\"True min:\",true_small/totals)\n",
        "      print(\"Binary \",binary/totals)\n",
        "  confusion_array=np.array(c_matrix_data)\n",
        "  cm =   confusion_matrix(confusion_array[:,0],confusion_array[:,1])\n",
        "  compute_statistics(cm)\n",
        "  f = sns.heatmap(cm, annot=True,fmt=\"d\")\n",
        "  plt.savefig('./Outs/heatMap-'+attack+'_'+l_name+'.png')\n",
        "  plt.clf()\n",
        "  return right/totals\n",
        "# conv2\n",
        "model.conv1.register_forward_hook(get_activation_input('conv1'))\n",
        "model.conv2.register_forward_hook(get_activation_input('conv2'))\n",
        "model.relu1.register_forward_hook(get_activation_input('relu1'))\n",
        "model.relu1.register_forward_hook(get_activation_input('relu2'))\n",
        "model.relu3.register_forward_hook(get_activation_input('relu3'))\n",
        "model.fc1.register_forward_hook(get_activation_input('fc1'))\n",
        "model.fc1.register_forward_hook(get_activation_input('fc2'))\n",
        "model.relu3.register_forward_hook(get_activation_input('dropout1'))\n",
        "model.relu3.register_forward_hook(get_activation_input('dropout2'))\n",
        "\n",
        "print(\"Starting\")\n",
        "ranks=[3]\n",
        "layers=[\"conv1\",\"relu1\",\"conv2\",\"relu2\",\"dropout1\",\"fc1\",\"relu3\",\"dropout2\",\"fc2\"]\n",
        "attack_types=[\"cw2\",\"fgsm\",\"deepfool\",\"ifgsm\"]\n",
        "centers=[\"sample\"]\n",
        "all_results_filename=\"all_results.md\"\n",
        "all_results=np.zeros((len(ranks),len(layers)))\n",
        "for c in centers:\n",
        "  for att_i,att in enumerate(attack_types):\n",
        "    for r_i,r in enumerate(ranks):\n",
        "      acc_mifgsm=[]\n",
        "      for l_i,l in enumerate(layers):\n",
        "        t=testModel(model,criterion,device,test_loader,att,r,c,l_name=l)\n",
        "        acc_mifgsm.append(t)\n",
        "        all_results[r_i,l_i]=t\n",
        "      plt.plot(layers,acc_mifgsm)\n",
        "      plt.savefig('./Outs/{}_R{}.png'.format(att,r))\n",
        "      plt.clf()\n",
        "    attack_filename=\"./Outs/{}.md\".format(att)\n",
        "    pickle.dump(all_results, open(attack_filename, 'wb+'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting\n",
            "Running on  cuda\n",
            "Started testing with A:cw2 on L:conv1 with Rank:3 with the center:sample\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9516129032258065\n",
            "Special stuff 0.0\n",
            "Overall test accuracy:1.0\n",
            "Right max : 1.0\n",
            "Fooled max: 0.0\n",
            "Right min : 1.0\n",
            "True min: 0.0\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9266666666666666\n",
            "Special stuff 0.0\n",
            "Overall test accuracy:1.0\n",
            "Right max : 0.5\n",
            "Fooled max: 0.0\n",
            "Right min : 0.5\n",
            "True min: 0.5\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9140893470790378\n",
            "Special stuff 0.0\n",
            "Overall test accuracy:0.9896907216494846\n",
            "Right max : 0.6666666666666666\n",
            "Fooled max: 0.0\n",
            "Right min : 0.3333333333333333\n",
            "True min: 0.6666666666666666\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9157608695652174\n",
            "Special stuff 0.0\n",
            "Overall test accuracy:0.9864130434782609\n",
            "Right max : 0.75\n",
            "Fooled max: 0.0\n",
            "Right min : 0.5\n",
            "True min: 0.75\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9139072847682119\n",
            "Special stuff 0.0\n",
            "Overall test accuracy:0.9801324503311258\n",
            "Right max : 0.6\n",
            "Fooled max: 0.2\n",
            "Right min : 0.6\n",
            "True min: 0.8\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9143426294820717\n",
            "Special stuff 0.0\n",
            "Overall test accuracy:0.9800796812749004\n",
            "Right max : 0.5\n",
            "Fooled max: 0.3333333333333333\n",
            "Right min : 0.5\n",
            "True min: 0.6666666666666666\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9061371841155235\n",
            "Special stuff 0.14285714285714285\n",
            "Overall test accuracy:0.98014440433213\n",
            "Right max : 0.5714285714285714\n",
            "Fooled max: 0.2857142857142857\n",
            "Right min : 0.5714285714285714\n",
            "True min: 0.7142857142857143\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9071803852889667\n",
            "Special stuff 0.125\n",
            "Overall test accuracy:0.978984238178634\n",
            "Right max : 0.5\n",
            "Fooled max: 0.25\n",
            "Right min : 0.5\n",
            "True min: 0.625\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9024707412223667\n",
            "Special stuff 0.1111111111111111\n",
            "Overall test accuracy:0.9791937581274383\n",
            "Right max : 0.4444444444444444\n",
            "Fooled max: 0.2222222222222222\n",
            "Right min : 0.4444444444444444\n",
            "True min: 0.6666666666666666\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9034981905910736\n",
            "Special stuff 0.1\n",
            "Overall test accuracy:0.9794933655006032\n",
            "Right max : 0.4\n",
            "Fooled max: 0.3\n",
            "Right min : 0.4\n",
            "True min: 0.6\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.8952668680765358\n",
            "Special stuff 0.09090909090909091\n",
            "Overall test accuracy:0.9748237663645518\n",
            "Right max : 0.36363636363636365\n",
            "Fooled max: 0.2727272727272727\n",
            "Right min : 0.36363636363636365\n",
            "True min: 0.5454545454545454\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.8982084690553745\n",
            "Special stuff 0.16666666666666666\n",
            "Overall test accuracy:0.9739413680781759\n",
            "Right max : 0.4166666666666667\n",
            "Fooled max: 0.25\n",
            "Right min : 0.4166666666666667\n",
            "True min: 0.5833333333333334\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.8987854251012146\n",
            "Special stuff 0.23076923076923078\n",
            "Overall test accuracy:0.974089068825911\n",
            "Right max : 0.46153846153846156\n",
            "Fooled max: 0.23076923076923078\n",
            "Right min : 0.38461538461538464\n",
            "True min: 0.6153846153846154\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.899119295436349\n",
            "Special stuff 0.21428571428571427\n",
            "Overall test accuracy:0.9743795036028823\n",
            "Right max : 0.42857142857142855\n",
            "Fooled max: 0.21428571428571427\n",
            "Right min : 0.42857142857142855\n",
            "True min: 0.6428571428571429\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9035916824196597\n",
            "Special stuff 0.26666666666666666\n",
            "Overall test accuracy:0.9785759294265911\n",
            "Right max : 0.4666666666666667\n",
            "Fooled max: 0.2\n",
            "Right min : 0.4666666666666667\n",
            "True min: 0.6666666666666666\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9047350620067643\n",
            "Special stuff 0.25\n",
            "Overall test accuracy:0.9791431792559189\n",
            "Right max : 0.4375\n",
            "Fooled max: 0.1875\n",
            "Right min : 0.4375\n",
            "True min: 0.625\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9034059184812954\n",
            "Special stuff 0.29411764705882354\n",
            "Overall test accuracy:0.9793411501954216\n",
            "Right max : 0.4117647058823529\n",
            "Fooled max: 0.17647058823529413\n",
            "Right min : 0.4117647058823529\n",
            "True min: 0.5882352941176471\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9017713365539453\n",
            "Special stuff 0.3333333333333333\n",
            "Overall test accuracy:0.9796027911969941\n",
            "Right max : 0.4444444444444444\n",
            "Fooled max: 0.16666666666666666\n",
            "Right min : 0.4444444444444444\n",
            "True min: 0.6111111111111112\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9042503503035965\n",
            "Special stuff 0.3157894736842105\n",
            "Overall test accuracy:0.9799159271368519\n",
            "Right max : 0.47368421052631576\n",
            "Fooled max: 0.15789473684210525\n",
            "Right min : 0.47368421052631576\n",
            "True min: 0.631578947368421\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9022869022869023\n",
            "Special stuff 0.35\n",
            "Overall test accuracy:0.9792099792099792\n",
            "Right max : 0.5\n",
            "Fooled max: 0.15\n",
            "Right min : 0.45\n",
            "True min: 0.65\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9024390243902439\n",
            "Special stuff 0.3333333333333333\n",
            "Overall test accuracy:0.9793303017775941\n",
            "Right max : 0.5238095238095238\n",
            "Fooled max: 0.14285714285714285\n",
            "Right min : 0.42857142857142855\n",
            "True min: 0.6190476190476191\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9022680412371133\n",
            "Special stuff 0.3181818181818182\n",
            "Overall test accuracy:0.979381443298969\n",
            "Right max : 0.5\n",
            "Fooled max: 0.13636363636363635\n",
            "Right min : 0.4090909090909091\n",
            "True min: 0.5909090909090909\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9021941242097434\n",
            "Special stuff 0.30434782608695654\n",
            "Overall test accuracy:0.9806619561175158\n",
            "Right max : 0.4782608695652174\n",
            "Fooled max: 0.13043478260869565\n",
            "Right min : 0.391304347826087\n",
            "True min: 0.5652173913043478\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9032035825008612\n",
            "Special stuff 0.2916666666666667\n",
            "Overall test accuracy:0.9803651395108508\n",
            "Right max : 0.5\n",
            "Fooled max: 0.125\n",
            "Right min : 0.375\n",
            "True min: 0.5416666666666666\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9043743641912513\n",
            "Special stuff 0.32\n",
            "Overall test accuracy:0.9806714140386572\n",
            "Right max : 0.52\n",
            "Fooled max: 0.12\n",
            "Right min : 0.4\n",
            "True min: 0.56\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9041554959785523\n",
            "Special stuff 0.3076923076923077\n",
            "Overall test accuracy:0.9798927613941019\n",
            "Right max : 0.5384615384615384\n",
            "Fooled max: 0.11538461538461539\n",
            "Right min : 0.38461538461538464\n",
            "True min: 0.5769230769230769\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9033112582781457\n",
            "Special stuff 0.2962962962962963\n",
            "Overall test accuracy:0.9798013245033113\n",
            "Right max : 0.5185185185185185\n",
            "Fooled max: 0.1111111111111111\n",
            "Right min : 0.4074074074074074\n",
            "True min: 0.5925925925925926\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9030443414956982\n",
            "Special stuff 0.2857142857142857\n",
            "Overall test accuracy:0.9798146922567836\n",
            "Right max : 0.5357142857142857\n",
            "Fooled max: 0.10714285714285714\n",
            "Right min : 0.42857142857142855\n",
            "True min: 0.6071428571428571\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.90234375\n",
            "Special stuff 0.27586206896551724\n",
            "Overall test accuracy:0.9798177083333334\n",
            "Right max : 0.5517241379310345\n",
            "Fooled max: 0.10344827586206896\n",
            "Right min : 0.41379310344827586\n",
            "True min: 0.5862068965517241\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.902304446608244\n",
            "Special stuff 0.26666666666666666\n",
            "Overall test accuracy:0.9795520934761441\n",
            "Right max : 0.5333333333333333\n",
            "Fooled max: 0.1\n",
            "Right min : 0.4\n",
            "True min: 0.5666666666666667\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9016083254493851\n",
            "Special stuff 0.2903225806451613\n",
            "Overall test accuracy:0.97918637653737\n",
            "Right max : 0.5483870967741935\n",
            "Fooled max: 0.0967741935483871\n",
            "Right min : 0.41935483870967744\n",
            "True min: 0.5806451612903226\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9012650416538106\n",
            "Special stuff 0.28125\n",
            "Overall test accuracy:0.978401727861771\n",
            "Right max : 0.53125\n",
            "Fooled max: 0.09375\n",
            "Right min : 0.40625\n",
            "True min: 0.5625\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9019666269368296\n",
            "Special stuff 0.2727272727272727\n",
            "Overall test accuracy:0.9788438617401669\n",
            "Right max : 0.5454545454545454\n",
            "Fooled max: 0.09090909090909091\n",
            "Right min : 0.42424242424242425\n",
            "True min: 0.5757575757575758\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9031700288184438\n",
            "Special stuff 0.29411764705882354\n",
            "Overall test accuracy:0.9795389048991354\n",
            "Right max : 0.5588235294117647\n",
            "Fooled max: 0.08823529411764706\n",
            "Right min : 0.4117647058823529\n",
            "True min: 0.5588235294117647\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9028848900314196\n",
            "Special stuff 0.3142857142857143\n",
            "Overall test accuracy:0.9797200799771494\n",
            "Right max : 0.5714285714285714\n",
            "Fooled max: 0.08571428571428572\n",
            "Right min : 0.42857142857142855\n",
            "True min: 0.5428571428571428\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9029126213592233\n",
            "Special stuff 0.3055555555555556\n",
            "Overall test accuracy:0.9797258709308966\n",
            "Right max : 0.5833333333333334\n",
            "Fooled max: 0.08333333333333333\n",
            "Right min : 0.4166666666666667\n",
            "True min: 0.5555555555555556\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9040351784790481\n",
            "Special stuff 0.2972972972972973\n",
            "Overall test accuracy:0.9787894464562855\n",
            "Right max : 0.5675675675675675\n",
            "Fooled max: 0.08108108108108109\n",
            "Right min : 0.40540540540540543\n",
            "True min: 0.5675675675675675\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9034552845528455\n",
            "Special stuff 0.2894736842105263\n",
            "Overall test accuracy:0.9784044715447154\n",
            "Right max : 0.5526315789473685\n",
            "Fooled max: 0.07894736842105263\n",
            "Right min : 0.39473684210526316\n",
            "True min: 0.5526315789473685\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9028023226457965\n",
            "Special stuff 0.28205128205128205\n",
            "Overall test accuracy:0.9782883110325675\n",
            "Right max : 0.5384615384615384\n",
            "Fooled max: 0.10256410256410256\n",
            "Right min : 0.38461538461538464\n",
            "True min: 0.5384615384615384\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9002587626440838\n",
            "Special stuff 0.275\n",
            "Overall test accuracy:0.9778875558692073\n",
            "Right max : 0.525\n",
            "Fooled max: 0.1\n",
            "Right min : 0.4\n",
            "True min: 0.55\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.900626678603402\n",
            "Special stuff 0.2682926829268293\n",
            "Overall test accuracy:0.9773948075201433\n",
            "Right max : 0.5365853658536586\n",
            "Fooled max: 0.0975609756097561\n",
            "Right min : 0.3902439024390244\n",
            "True min: 0.5365853658536586\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9003566651805618\n",
            "Special stuff 0.2619047619047619\n",
            "Overall test accuracy:0.9772625947391886\n",
            "Right max : 0.5476190476190477\n",
            "Fooled max: 0.09523809523809523\n",
            "Right min : 0.38095238095238093\n",
            "True min: 0.5238095238095238\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9007969798657718\n",
            "Special stuff 0.2558139534883721\n",
            "Overall test accuracy:0.9769295302013423\n",
            "Right max : 0.5348837209302325\n",
            "Fooled max: 0.09302325581395349\n",
            "Right min : 0.37209302325581395\n",
            "True min: 0.5116279069767442\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9006705783738475\n",
            "Special stuff 0.25\n",
            "Overall test accuracy:0.9769488683989941\n",
            "Right max : 0.5454545454545454\n",
            "Fooled max: 0.09090909090909091\n",
            "Right min : 0.36363636363636365\n",
            "True min: 0.5\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.8999176276771005\n",
            "Special stuff 0.24444444444444444\n",
            "Overall test accuracy:0.9771416803953872\n",
            "Right max : 0.5555555555555556\n",
            "Fooled max: 0.08888888888888889\n",
            "Right min : 0.37777777777777777\n",
            "True min: 0.5111111111111111\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.8992777669334374\n",
            "Special stuff 0.2391304347826087\n",
            "Overall test accuracy:0.9771618192465352\n",
            "Right max : 0.5434782608695652\n",
            "Fooled max: 0.08695652173913043\n",
            "Right min : 0.3695652173913043\n",
            "True min: 0.5\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.8994780591532959\n",
            "Special stuff 0.23404255319148937\n",
            "Overall test accuracy:0.9773825633094916\n",
            "Right max : 0.5531914893617021\n",
            "Fooled max: 0.0851063829787234\n",
            "Right min : 0.3829787234042553\n",
            "True min: 0.5106382978723404\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.8997695852534562\n",
            "Special stuff 0.22916666666666666\n",
            "Overall test accuracy:0.9775345622119815\n",
            "Right max : 0.5625\n",
            "Fooled max: 0.08333333333333333\n",
            "Right min : 0.375\n",
            "True min: 0.5\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9002102848403747\n",
            "Special stuff 0.24489795918367346\n",
            "Overall test accuracy:0.9776333397056012\n",
            "Right max : 0.5714285714285714\n",
            "Fooled max: 0.08163265306122448\n",
            "Right min : 0.3877551020408163\n",
            "True min: 0.5102040816326531\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9010085917071349\n",
            "Special stuff 0.24\n",
            "Overall test accuracy:0.9777736271946208\n",
            "Right max : 0.58\n",
            "Fooled max: 0.08\n",
            "Right min : 0.38\n",
            "True min: 0.52\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9003817487729504\n",
            "Special stuff 0.23529411764705882\n",
            "Overall test accuracy:0.9780039992728595\n",
            "Right max : 0.5882352941176471\n",
            "Fooled max: 0.0784313725490196\n",
            "Right min : 0.37254901960784315\n",
            "True min: 0.5098039215686274\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.9004156876920296\n",
            "Special stuff 0.23076923076923078\n",
            "Overall test accuracy:0.9781312127236581\n",
            "Right max : 0.5961538461538461\n",
            "Fooled max: 0.07692307692307693\n",
            "Right min : 0.36538461538461536\n",
            "True min: 0.5\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.8992070484581498\n",
            "Special stuff 0.24528301886792453\n",
            "Overall test accuracy:0.9785022026431718\n",
            "Right max : 0.5849056603773585\n",
            "Fooled max: 0.09433962264150944\n",
            "Right min : 0.37735849056603776\n",
            "True min: 0.49056603773584906\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.8987253361271171\n",
            "Special stuff 0.24074074074074073\n",
            "Overall test accuracy:0.9786973982888074\n",
            "Right max : 0.5740740740740741\n",
            "Fooled max: 0.09259259259259259\n",
            "Right min : 0.37037037037037035\n",
            "True min: 0.48148148148148145\n",
            "Binary  0.0\n",
            "Stats for attack type:cw2 on the layer:conv1 with a gradient rank:3\n",
            "Normal taylor predictions 0.8979202772963605\n",
            "Special stuff 0.23636363636363636\n",
            "Overall test accuracy:0.9788561525129983\n",
            "Right max : 0.5636363636363636\n",
            "Fooled max: 0.09090909090909091\n",
            "Right min : 0.36363636363636365\n",
            "True min: 0.4727272727272727\n",
            "Binary  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "6_U32AoPJxTC"
      },
      "source": [
        "!zip -r /content/file.zip /content/Outs\n",
        "from google.colab import files\n",
        "files.download(\"/content/file.zip\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}