{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Thesis.ipynb",
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MAHA06/ThesisOulu/blob/master/Adversarials/Thesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "54pec7A464Ta"
   },
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount(‘/content/gdrive’)"
   ],
   "execution_count": 142,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4GzjWdjAfomN"
   },
   "source": [
    "import math\n",
    "from torch.autograd.gradcheck import zero_gradients\n",
    "\n",
    "import seaborn as sns\n",
    "from torch.autograd import Variable\n",
    "import torch as torch\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pickle\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms,datasets\n",
    "np.random.seed(42) \n",
    "torch.manual_seed(42)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import save_image\n"
   ],
   "execution_count": 143,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GsxBn-i6fmfw"
   },
   "source": [
    "online=False\n",
    "\n",
    "img_size=(28,28)\n",
    "num_classes=10\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.0,), (1.0,))])\n",
    "dataset = datasets.MNIST(root = './data', train=True, transform = transform, download=True)\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [50000, 10000])\n",
    "test_set = datasets.MNIST(root = './data', train=False, transform = transform, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_set,batch_size=1,shuffle=True) \n",
    "val_loader = torch.utils.data.DataLoader(val_set,batch_size=1,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set,batch_size=1,shuffle=True)"
   ],
   "execution_count": 144,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YDRayKODFRju"
   },
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "    self.relu1=nn.ReLU()\n",
    "    self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "    self.relu2=nn.ReLU()\n",
    "    self.max_p1=nn.MaxPool2d(2)\n",
    "    self.dropout1 = nn.Dropout2d(0.25)\n",
    "    self.fc1 = nn.Linear(9216, 128)\n",
    "    self.relu3=nn.ReLU()\n",
    "    self.dropout2 = nn.Dropout2d(0.5)\n",
    "\n",
    "    self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.relu1(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.relu2(x)\n",
    "    x = self.max_p1(x)\n",
    "    x = self.dropout1(x)\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = self.fc1(x)\n",
    "    x = self.relu3(x)\n",
    "    x = self.dropout2(x)\n",
    "    x = self.fc2(x)\n",
    "    output = F.log_softmax(x, dim=1)\n",
    "    return output\n",
    "\n",
    "\n"
   ],
   "execution_count": 145,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aJMQz9fdFRpn",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "05243ea9-5f4d-43b2-e128-2e3b02ff85b4"
   },
   "source": [
    "use_cuda=False\n",
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
    "print(\"Using the device:\",device)"
   ],
   "execution_count": 146,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the device: cpu\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Gl-WshaZFRsF"
   },
   "source": [
    "def fitModel(model,optimizer,scheduler,criterion,device,train_loader,val_loader,epochs):\n",
    "  data_loader={'train':train_loader,'val':val_loader}\n",
    "  print(\"Starting to fit the model\")\n",
    "  train_error,test_error=[],[]\n",
    "  best_points=np.zeros(img_size+(num_classes,),dtype=float)\n",
    "  best_scores=np.full(num_classes,-np.inf)\n",
    "  print(best_scores[1])\n",
    "  for epc in range(epochs):\n",
    "    correct=0\n",
    "    total=0\n",
    "    loss_epoch,val_loss_epoch=0,0\n",
    "    for phase in ('train','val'):\n",
    "\n",
    "      for i,data in enumerate(data_loader[phase]):\n",
    "        \n",
    "        input,label=data[0].to(device),data[1].to(device)\n",
    "        # print(input.shape)\n",
    "        out=model(input)\n",
    "        # print(out)\n",
    "        loss=criterion(out,label)\n",
    "        predict_label=torch.argmax(out,dim=1)\n",
    "        out=out.squeeze().detach().cpu().numpy()\n",
    "        if phase == 'train':\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          loss_epoch+=loss.item()\n",
    "          # print(predict_label.item(),label.item())\n",
    "          # print(out[predict_label.item()],best_scores[predict_label.item()])\n",
    "          # print(out)\n",
    "          if epc == epochs-1 and predict_label.item()==label.item() and out[predict_label.item()]>best_scores[predict_label.item()]:#if last epoch and the predicted label is correct\n",
    "            best_scores[predict_label.item()]=out[predict_label.item()]\n",
    "            best_points[:,:,predict_label]=input.squeeze().detach().cpu().numpy()\n",
    "            # print(label.item(),out[label.item()])\n",
    "            # print(\"The scores were updated:\",best_scores)\n",
    "        else:\n",
    "          total+=1\n",
    "          if predict_label==label:\n",
    "            correct+=1\n",
    "          val_loss_epoch+=loss.item()\n",
    "    scheduler.step(val_loss_epoch/len(val_loader))\n",
    "    print(\"Epoch : {} Accuracy : {}\".format(epc+1,correct/total))\n",
    "    print(\"Epoch : {} Train Loss : {} Eval Loss : {}\".format(epc+1,loss_epoch/len(train_loader),val_loss_epoch/len(val_loader)))\n",
    "    train_error.append(loss_epoch)\n",
    "    test_error.append(val_loss_epoch)\n",
    "  \n",
    "  return train_error,test_error,best_points,best_scores\n",
    "        \n",
    "          \n",
    "\n",
    "\n"
   ],
   "execution_count": 147,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "E2VlV-xmFRuk"
   },
   "source": [
    "model=Net().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.0001, betas=(0.9, 0.999))\n",
    "criterion = nn.NLLLoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "\n",
    "\n",
    "###############\n"
   ],
   "execution_count": 148,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "AWmhvKHDgpCZ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "bffe0928-be65-4a35-b895-f819cf942caf"
   },
   "source": [
    "## RUN THE TRAINING\n",
    "if(online):\n",
    "  loss,val_loss,best_points,best_scores=fitModel(model,optimizer,scheduler,criterion,device,train_loader,val_loader,10)\n"
   ],
   "execution_count": 149,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "HAUXpIF_79an",
    "outputId": "3a8ab6ce-df9b-4020-cbb7-9cc03a5ce5de"
   },
   "source": [
    "if online:\n",
    "  print(best_points.shape)\n",
    "  plt.imshow(best_points[:,:,3])\n",
    "  v=torch.tensor(best_points[:,:,3].reshape(1,1,28,28),dtype=torch.float32)\n",
    "  print(model(v.to(device)))\n",
    "  print(torch.argmax(model(v.to(device))))\n",
    "  print(best_scores)"
   ],
   "execution_count": 150,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R59id5I4pOEj"
   },
   "source": [
    "if online:\n",
    "  filename = './model_MNIST'\n",
    "  pickle.dump(model, open(filename, 'wb'))\n",
    "  filename = './model_MNIST_cpu'\n",
    "  pickle.dump(model.cpu(), open(filename, 'wb'))\n",
    "  filename = './scores.mf'\n",
    "  best_markers={\"b_p\":best_points,\"b_s\":best_scores}\n",
    "  pickle.dump(best_markers,open(filename,'wb'))"
   ],
   "execution_count": 151,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1M92hyHXqrVI"
   },
   "source": [
    "def deepfool(image, net, num_classes=10, overshoot=0.02, max_iter=50):\n",
    "\n",
    "    \"\"\"\n",
    "       :param image: Image of size HxWx3\n",
    "       :param net: network (input: images, output: values of activation **BEFORE** softmax).\n",
    "       :param num_classes: num_classes (limits the number of classes to test against, by default = 10)\n",
    "       :param overshoot: used as a termination criterion to prevent vanishing updates (default = 0.02).\n",
    "       :param max_iter: maximum number of iterations for deepfool (default = 50)\n",
    "       :return: minimal perturbation that fools the classifier, number of iterations that it required, new estimated_label and perturbed image\n",
    "    \"\"\"\n",
    "    is_cuda = torch.cuda.is_available()\n",
    "\n",
    "    if is_cuda:\n",
    "        print(\"Using GPU\")\n",
    "        image = image.cuda()\n",
    "        net = net.cuda()\n",
    "    else:\n",
    "        print(\"Using CPU\")\n",
    "\n",
    "\n",
    "    f_image = net.forward(image).data.cpu().numpy().flatten()\n",
    "    I = (np.array(f_image)).flatten().argsort()[::-1]\n",
    "\n",
    "    I = I[0:num_classes]\n",
    "    label = I[0]\n",
    "\n",
    "    input_shape = image.shape\n",
    "    pert_image = copy.deepcopy(image)\n",
    "    w = np.zeros(input_shape)\n",
    "    r_tot = np.zeros(input_shape)\n",
    "\n",
    "    loop_i = 0\n",
    "\n",
    "    x = pert_image\n",
    "    fs = net.forward(x)\n",
    "    fs_list = [fs[0,I[k]] for k in range(num_classes)]\n",
    "    k_i = label\n",
    "\n",
    "    while k_i == label and loop_i < max_iter:\n",
    "\n",
    "        pert = np.inf\n",
    "        fs[0, I[0]].backward(retain_graph=True)\n",
    "        grad_orig = x.grad.data.cpu().numpy().copy()\n",
    "\n",
    "        for k in range(1, num_classes):\n",
    "            zero_gradients(x)\n",
    "\n",
    "            fs[0, I[k]].backward(retain_graph=True)\n",
    "            cur_grad = x.grad.data.cpu().numpy().copy()\n",
    "\n",
    "            # set new w_k and new f_k\n",
    "            w_k = cur_grad - grad_orig\n",
    "            f_k = (fs[0, I[k]] - fs[0, I[0]]).data.cpu().numpy()\n",
    "\n",
    "            pert_k = abs(f_k)/np.linalg.norm(w_k.flatten())\n",
    "\n",
    "            # determine which w_k to use\n",
    "            if pert_k < pert:\n",
    "                pert = pert_k\n",
    "                w = w_k\n",
    "\n",
    "        # compute r_i and r_tot\n",
    "        # Added 1e-4 for numerical stability\n",
    "        r_i =  (pert+1e-4) * w / np.linalg.norm(w)\n",
    "        r_tot = np.float32(r_tot + r_i)\n",
    "\n",
    "        if is_cuda:\n",
    "            pert_image = image + (1+overshoot)*torch.from_numpy(r_tot).cuda()\n",
    "        else:\n",
    "            pert_image = image + (1+overshoot)*torch.from_numpy(r_tot)\n",
    "\n",
    "        x = Variable(pert_image, requires_grad=True)\n",
    "        fs = net.forward(x)\n",
    "        k_i = np.argmax(fs.data.cpu().numpy().flatten())\n",
    "\n",
    "        loop_i += 1\n",
    "\n",
    "    r_tot = (1+overshoot)*r_tot\n",
    "\n",
    "    return r_tot, loop_i, label, k_i, pert_image\n",
    "def fgsm_attack(input,epsilon,data_grad):\n",
    "  pert_out = input + epsilon*data_grad.sign()\n",
    "  pert_out = torch.clamp(pert_out, 0, 1)\n",
    "  return pert_out\n",
    "\n",
    "def ifgsm_attack(input,epsilon,data_grad):\n",
    "  iter = 10\n",
    "  alpha = epsilon/iter\n",
    "  pert_out = input\n",
    "  for i in range(iter-1):\n",
    "    pert_out = pert_out + alpha*data_grad.sign()\n",
    "    pert_out = torch.clamp(pert_out, 0, 1)\n",
    "    if torch.norm((pert_out-input),p=float('inf')) > epsilon:\n",
    "      break\n",
    "  return pert_out\n",
    "\n",
    "def mifgsm_attack(input,epsilon,data_grad):\n",
    "  iter=10\n",
    "  decay_factor=1.0\n",
    "  pert_out = input\n",
    "  alpha = epsilon/iter\n",
    "  g=0\n",
    "  for i in range(iter-1):\n",
    "    g = decay_factor*g + data_grad/torch.norm(data_grad,p=1)\n",
    "    pert_out = pert_out + alpha*torch.sign(g)\n",
    "    pert_out = torch.clamp(pert_out, 0, 1)\n",
    "    if torch.norm((pert_out-input),p=float('inf')) > epsilon:\n",
    "      break\n",
    "  return pert_out"
   ],
   "execution_count": 152,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j1wEKDumo2wC"
   },
   "source": [
    "def JacobianMatrix(model,input):\n",
    "  print(\"Input size \",input.shape)\n",
    "  number_of_piels=input.shape[0]*input.shape[1]\n",
    "  J = torch.zeros ((1, 784, 10))   # loop will fill in Jacobian\n",
    "  input.requires_grad = True\n",
    "  preds = model (input)\n",
    "  for  i in range (10):\n",
    "      grd = torch.zeros ((1, 10))   # same shape as preds\n",
    "      grd[0, i] = 1    # column of Jacobian to compute\n",
    "      preds.backward (gradient = grd, retain_graph = True)\n",
    "      J[:,:,i] = input.grad   # fill in one column of Jacobian\n",
    "      input.grad.zero_()   # .backward() accumulates gradients, so reset to zero\n",
    "def JSM_attack(model, input):\n",
    "  input.requires_grad=True"
   ],
   "execution_count": 153,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ySq2vSmOJxS7"
   },
   "source": [
    "activation_in = {}\n",
    "activation_out={}\n",
    "def get_activation_input(name):\n",
    "    def hook(model, input, output):\n",
    "        activation_in[name] = input[0]\n",
    "    return hook\n",
    "def get_activation_output(name):\n",
    "    def hook(model, input, output):\n",
    "        activation_out[name] = output.clone().detach()\n",
    "    return hook\n",
    "def nth_derivative(f, wrt, n):\n",
    "\n",
    "    for i in range(n):\n",
    "\n",
    "        grads = torch.autograd.grad(f, wrt, create_graph=True)[0]\n",
    "        f = grads.sum()\n",
    "\n",
    "    return grads\n",
    "def get_higher_gradients(model,input,label, rank=3):\n",
    "  grads=[]\n",
    "  optimizer.zero_grad()\n",
    "  input.requires_grad=True\n",
    "  outs=model(input)\n",
    "  # pred=torch.zeros(num_classes)\n",
    "  # pred[label.item()]=1\n",
    "  l=criterion(outs,label)\n",
    "  aux=torch.autograd.grad(l,input,retain_graph=True,create_graph=True)[0]\n",
    "  grads.append(aux.reshape(1,-1))\n",
    "  for r in range(2,rank+1):\n",
    "    aux=torch.autograd.grad(aux.sum(),input,create_graph=True)[0]\n",
    "    grads.append(aux.reshape(1,-1))\n",
    "  return grads\n",
    "def check_taylor_prediction(model,input,marker_images,check_label,layer_name,rank):\n",
    "  # plt.imshow(input.squeeze().detach().cpu().numpy())\n",
    "  # grads=get_higher_gradients(model,input,true_label)\n",
    "\n",
    "  # grads=get_higher_gradients(model,input,perturbed_label)\n",
    "  # layer_grads=get_layer_gradients(model,input,check_label,\"conv2\")\n",
    "\n",
    "  # input=input.reshape(-1,1)\n",
    "  # # marker_image=marker_images[:,:,true_label.item()].reshape(-1,1)\n",
    "  # # marker_image=torch.tensor(marker_image,dtype=torch.float32)\n",
    "  # # total=1.+torch.mm(grads[0],(input-marker_image))+torch.mm(grads[1],torch.pow((input-marker_image),2))/2\n",
    "  # # total=1.+torch.mm(grads[0],(input-marker_image))+torch.mm(grads[1],torch.pow((input-marker_image),2))/2\n",
    "  # marker_image1=marker_images[:,:,perturbed_label.item()].reshape(-1,1)\n",
    "  # marker_image1=torch.tensor(marker_image1,dtype=torch.float32)\n",
    "  # total1=torch.mm(layer_grads[0],(input-marker_image1))+torch.mm(layer_grads[1],torch.pow((input-marker_image1),2))/2\n",
    "  marker_img=torch.tensor(marker_images[:,:,check_label.item()],dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "  # total_marker=compute_marker_image_prediction(model,input,marker_img,check_label,layer_name,rank)\n",
    "  total_unknown=compute_unknown_image_prediction(model,input,marker_img,check_label,layer_name,rank)\n",
    "\n",
    "  return total_unknown\n",
    "  # return total_marker\n",
    "def compute_marker_image_prediction(model,unk_image,marker_img,marker_label,l_name,rank):\n",
    "  marker_img.requires_grad=True\n",
    "  grads,input_marker,loss_marker=get_layer_gradients(model,marker_img,marker_label,l_name,rank)\n",
    "  optimizer.zero_grad()\n",
    "  input_marker=input_marker.view(-1,1)\n",
    "  # print(input.shape)\n",
    "  outs=model(unk_image)\n",
    "  input_layer=activation_in[l_name].view(-1,1)\n",
    "  total=loss_marker.item()\n",
    "  for i in range(0,rank):\n",
    "    total+=torch.mm(grads[i],torch.pow((input_layer-input_marker),i+1))/(math.factorial(i+1))\n",
    "  return total.item()\n",
    "def compute_unknown_image_prediction(model,unk_image,marker_image,marker_label,l_name,rank):\n",
    "\n",
    "  grads,unk_input,loss_unk=get_layer_gradients(model,unk_image,marker_label,l_name,rank)\n",
    "  optimizer.zero_grad()\n",
    "  layer_input=unk_input.view(-1,1)\n",
    "  # print(input.shape)\n",
    "  outs=model(marker_image)\n",
    "  input_layer=activation_in[l_name].view(-1,1)\n",
    "  total=loss_unk.item()\n",
    "  for i in range(0,rank):\n",
    "    total+=torch.mm(grads[i],torch.pow((input_layer-layer_input),i+1))/(math.factorial(i+1))\n",
    "  # total=loss_unk.item()+torch.mm(grads[0],(input_layer-layer_input))+torch.mm(grads[1],torch.pow((input_layer-layer_input),2))/2\n",
    "\n",
    "  return total.item()\n",
    "\n",
    "def get_layer_gradients(model,target_img,label,l_name, rank):\n",
    "  grads=[]\n",
    "  optimizer.zero_grad()\n",
    "  # target_img.requires_grad=True\n",
    "  outs=model(target_img)\n",
    "  # pred=torch.zeros((1,num_classes),dtype=np.int)\n",
    "  # pred[label.item()]=1\n",
    "  loss_value=criterion(outs,label)\n",
    "  layer_input_data=activation_in[l_name]\n",
    "  # layer_input_data.requires_grad=True\n",
    "  # loss_value.backward()\n",
    "  aux=torch.autograd.grad(loss_value,layer_input_data,retain_graph=True,create_graph=True)[0]\n",
    "  # aux=layer_input_data.grad\n",
    "  grads.append(aux.reshape(1,-1))\n",
    "  for r in range(1,rank):\n",
    "    aux=torch.autograd.grad(aux.sum(),layer_input_data,create_graph=True)[0]\n",
    "    grads.append(aux.reshape(1,-1))\n",
    "  return grads,layer_input_data,loss_value\n",
    "\n",
    "def get_marker_images(model,data_loader):\n",
    "  best_points=np.zeros(img_size+(num_classes,),dtype=float)\n",
    "  best_scores=np.full(num_classes,np.inf)\n",
    "  for i,data in enumerate(data_loader):\n",
    "\n",
    "      input,label=data[0].to(device),data[1].to(device)\n",
    "      # print(input.shape)\n",
    "      out=model(input)\n",
    "      # print(out)\n",
    "      loss=criterion(out,label)\n",
    "      predict_label=torch.argmax(out,dim=1)\n",
    "      out=out.squeeze().detach().cpu().numpy()\n",
    "\n",
    "      if   predict_label.item()==label.item() and loss<best_scores[predict_label.item()]:#if last epoch and the predicted label is correct\n",
    "        best_scores[predict_label.item()]=loss\n",
    "        best_points[:,:,predict_label.item()]=input.squeeze().detach().cpu().numpy()\n",
    "  best_markers={\"b_p\":best_points,\"b_s\":best_scores}\n",
    "  return best_markers\n",
    "\n",
    "filename = './model_MNIST_cpu'\n",
    "model= pickle.load(open(filename, 'rb'))\n",
    "filename = './scores.mf'\n",
    "# best_fits=get_marker_images(model,train_loader)\n",
    "# pickle.dump(best_fits, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "best_fits=pickle.load(open(filename,'rb'))\n",
    "\n",
    "def compute_statistics(confusion_matrix):\n",
    "  FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "  FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "  TP = np.diag(confusion_matrix)\n",
    "  TN = confusion_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "  # Sensitivity, hit rate, recall, or true positive rate\n",
    "  TPR = TP/(TP+FN)\n",
    "  print(\"Sensitivity\",TPR)\n",
    "  # Specificity or true negative rate\n",
    "  TNR = TN/(TN+FP)\n",
    "  print(\"Specificity\",TNR)\n",
    "  # Precision or positive predictive value\n",
    "  PPV = TP/(TP+FP)\n",
    "  print(\"Precision\",PPV)\n",
    "  # Negative predictive value\n",
    "  NPV = TN/(TN+FN)\n",
    "  # Fall out or false positive rate\n",
    "  FPR = FP/(FP+TN)\n",
    "  # False negative rate\n",
    "  FNR = FN/(TP+FN)\n",
    "  # False discovery rate\n",
    "  FDR = FP/(TP+FP)\n",
    "  # Overall accuracy\n",
    "  ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "  print(\"ACC:\",ACC)\n",
    "def testModel(model,criterion,device,test_loader,attack,rank,l_name=\"conv1\"):\n",
    "  print()\n",
    "  print(\"Started testing with A:{} on L:{} with Rank:{}\".format(attack,l_name,rank))\n",
    "  right=0\n",
    "  totals=0\n",
    "  fooled=0\n",
    "  binary=0\n",
    "  loss_e=0.02\n",
    "  loser_count=0\n",
    "  c_matrix_data=[]\n",
    "  correct_samples=0\n",
    "  number_samples=0\n",
    "  for data,labels in test_loader:\n",
    "    data,labels=data.to(device),labels.to(device)\n",
    "    data.requires_grad=True\n",
    "    outs=model(data)\n",
    "    number_samples+=1\n",
    "    true_preds=torch.argmax(outs,dim=1)\n",
    "    if true_preds!=labels:\n",
    "      continue\n",
    "    correct_samples+=1\n",
    "    epsilon=0.1\n",
    "    true_loss=criterion(outs,labels)\n",
    "    true_loss.backward()\n",
    "    data_grad=data.grad.data\n",
    "    if attack == \"fgsm\":\n",
    "      perturbed_data = fgsm_attack(data,epsilon,data_grad)\n",
    "    elif attack == \"ifgsm\":\n",
    "      perturbed_data = ifgsm_attack(data,epsilon,data_grad)\n",
    "    elif attack == \"mifgsm\":\n",
    "      perturbed_data = mifgsm_attack(data,epsilon,data_grad)\n",
    "    elif attack==\"deepfool\":\n",
    "      perturbed_data=deepfool(data,model)\n",
    "    elif attack == \"jacobian\":\n",
    "      print(\"Implement the jacobian\")\n",
    "    p_out=model(perturbed_data)\n",
    "    perturb_label=torch.argmax(p_out,dim=1)\n",
    "    # print(perturb_label.item())\n",
    "\n",
    "    if perturb_label!=labels:\n",
    "      totals+=1\n",
    "      # print(\"Trying to determine for the true class:\",labels.item(),\" which was labeled as:\",perturb_label.item())\n",
    "      taylors=range(0,num_classes)\n",
    "      taylors=[check_taylor_prediction(model,perturbed_data,best_fits['b_p'],torch.tensor(lab).reshape(1),layer_name=l_name,rank=rank) for lab in taylors ]\n",
    "      # taylors=[check_taylor_prediction(model,data,best_fits['b_p'],torch.tensor(lab).reshape(1),layer_name=l_name) for lab in taylors ]\n",
    "      taylor_np=np.array(taylors)\n",
    "      winning_labels=np.where((taylor_np>-5) )\n",
    "      # if(perturb_label.item() in winning_labels[0]):#check if the predicted label holds 0 loss\n",
    "      #   binary+=1\n",
    "      # print(\"Binary \",binary/totals)\n",
    "      # print(\"Right min :\",loser_count/totals)\n",
    "\n",
    "      abs_x=[abs(t) for t in taylors]\n",
    "      max_l=np.argmax(taylors)\n",
    "      min_l=np.argmin(abs_x)\n",
    "      c_matrix_data.append([ labels.item(),max_l])\n",
    "      m=np.where((taylor_np>1e-8) & (taylor_np <(taylor_np.sum()/10)))\n",
    "      t=np.argsort(taylor_np)\n",
    "      if max_l == perturb_label.item():\n",
    "        fooled+=1\n",
    "      if max_l == labels.item():\n",
    "        right+=1\n",
    "        # save_image(data,\"./Outs/original.png\")\n",
    "        # save_image(perturbed_data,\"./Outs/perturbed_data.png\")\n",
    "        # save_image(perturbed_data-data,\"./Outs/gradient visualization.png\")\n",
    "      if min_l == labels.item():\n",
    "        loser_count+=1\n",
    "      # r=check_taylor_prediction(model,data,best_fits['b_p'],perturb_label)\n",
    "\n",
    "  print(\"Stats for attack type:\"+attack +\" on the layer:\"+l_name+\" with a gradient rank:\"+str(rank))\n",
    "  print(\"Overall test accuracy:{}\".format(correct_samples/number_samples))\n",
    "  print(\"Right max :\",right/totals)\n",
    "  print(\"Fooled max:\",fooled/totals)\n",
    "  print(\"Right min :\",loser_count/totals)\n",
    "  print(\"Binary \",binary/totals)\n",
    "  confusion_array=np.array(c_matrix_data)\n",
    "  cm =   confusion_matrix(confusion_array[:,0],confusion_array[:,1])\n",
    "  compute_statistics(cm)\n",
    "  f = sns.heatmap(cm, annot=True)\n",
    "  plt.savefig('./Outs/heatMap-'+attack+'_'+l_name+'.png')\n",
    "  plt.clf()\n",
    "  return right/totals\n",
    "# conv2\n",
    "model.conv1.register_forward_hook(get_activation_input('conv1'))\n",
    "model.conv2.register_forward_hook(get_activation_input('conv2'))\n",
    "model.relu1.register_forward_hook(get_activation_input('relu1'))\n",
    "model.relu1.register_forward_hook(get_activation_input('relu2'))\n",
    "\n",
    "model.relu3.register_forward_hook(get_activation_input('relu3'))\n",
    "model.fc1.register_forward_hook(get_activation_input('fc1'))\n",
    "model.fc1.register_forward_hook(get_activation_input('fc2'))\n",
    "\n",
    "print(\"Starting\")\n",
    "ranks=[1,2,3,4]\n",
    "layers=[\"conv1\",\"relu1\",\"conv2\",\"relu2\",\"fc1\",\"relu3\",\"fc2\"]\n",
    "attack_types=[\"fgsm\",\"mifgsm\",\"ifgsm\"]\n",
    "\n",
    "for att in attack_types:\n",
    "\n",
    "  for r in ranks:\n",
    "    acc_mifgsm=[]\n",
    "    for l in layers:\n",
    "      t=testModel(model,criterion,'cpu',test_loader,att,r,l_name=l)\n",
    "      acc_mifgsm.append(t)\n",
    "    plt.plot(layers,acc_mifgsm)\n",
    "    plt.savefig('./Outs/{}_R{}.png'.format(att,r))\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ],
   "execution_count": 154,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n",
      "\n",
      "Started testing with A:fgsm on L:conv1 with Rank:1\n",
      "Stats for attack type:fgsm on the layer:conv1 with a gradient rank:1\n",
      "Overall test accuracy:0.9795\n",
      "Right max : 0.47320169252468264\n",
      "Fooled max: 0.35895627644569816\n",
      "Right min : 0.30888575458392104\n",
      "Binary  0.0\n",
      "Sensitivity [0.5        0.31578947 0.55       0.23157895 0.41176471 0.64804469\n",
      " 0.46808511 0.46470588 0.56140351 0.27118644]\n",
      "Specificity [0.94918999 0.99191771 0.94827586 0.93121693 0.95673077 0.89830508\n",
      " 0.97650744 0.95192308 0.91596639 0.89      ]\n",
      "Precision [0.3030303  0.62068966 0.63583815 0.19469027 0.56451613 0.47933884\n",
      " 0.6875     0.56834532 0.56140351 0.18285714]\n",
      "ACC: [0.93018336 0.96473907 0.89210155 0.88434415 0.89139633 0.86671368\n",
      " 0.92595205 0.89351199 0.85895628 0.83850494]\n",
      "\n",
      "Started testing with A:fgsm on L:relu1 with Rank:1\n",
      "Stats for attack type:fgsm on the layer:relu1 with a gradient rank:1\n",
      "Overall test accuracy:0.9814\n",
      "Right max : 0.4474053295932679\n",
      "Fooled max: 0.38779803646563815\n",
      "Right min : 0.29453015427769985\n",
      "Binary  0.0\n",
      "Sensitivity [0.5        0.2        0.49746193 0.25252525 0.48618785 0.52352941\n",
      " 0.40559441 0.53932584 0.45098039 0.37313433]\n",
      "Specificity [0.94070278 0.98828697 0.94873881 0.93519216 0.94939759 0.89649682\n",
      " 0.97194076 0.95272436 0.91734861 0.88157895]\n",
      "Precision [0.27027027 0.42857143 0.60869565 0.22522523 0.58278146 0.40639269\n",
      " 0.61702128 0.61935484 0.47668394 0.24630542]\n",
      "ACC: [0.92215989 0.95511921 0.88639551 0.88779804 0.89060309 0.85203366\n",
      " 0.91514727 0.90112202 0.85063114 0.83380084]\n",
      "\n",
      "Started testing with A:fgsm on L:conv2 with Rank:1\n",
      "Stats for attack type:fgsm on the layer:conv2 with a gradient rank:1\n",
      "Overall test accuracy:0.9802\n",
      "Right max : 0.49782608695652175\n",
      "Fooled max: 0.36231884057971014\n",
      "Right min : 0.3391304347826087\n",
      "Binary  0.0\n",
      "Sensitivity [0.6        0.30769231 0.51010101 0.3125     0.4852071  0.65662651\n",
      " 0.5        0.49090909 0.52036199 0.4173913 ]\n",
      "Specificity [0.94469697 0.98870482 0.94162437 0.93224299 0.96036334 0.90444811\n",
      " 0.98228663 0.94814815 0.93874029 0.89802372]\n",
      "Precision [0.33027523 0.51612903 0.59411765 0.25641026 0.63076923 0.48444444\n",
      " 0.75824176 0.5625     0.61827957 0.27118644]\n",
      "ACC: [0.92971014 0.96304348 0.87971014 0.88913043 0.90217391 0.87463768\n",
      " 0.93405797 0.89347826 0.87173913 0.85797101]\n",
      "\n",
      "Started testing with A:fgsm on L:relu2 with Rank:1\n",
      "Stats for attack type:fgsm on the layer:relu2 with a gradient rank:1\n",
      "Overall test accuracy:0.9789\n",
      "Right max : 0.4422514619883041\n",
      "Fooled max: 0.3793859649122807\n",
      "Right min : 0.3048245614035088\n",
      "Binary  0.0\n",
      "Sensitivity [0.55172414 0.21212121 0.45327103 0.18072289 0.45806452 0.59060403\n",
      " 0.53076923 0.40490798 0.5        0.31355932]\n",
      "Specificity [0.94427481 0.98694316 0.93240901 0.93463035 0.95136026 0.89253486\n",
      " 0.9733441  0.9526971  0.91637324 0.8912    ]\n",
      "Precision [0.3047619  0.4516129  0.55428571 0.15151515 0.54615385 0.40182648\n",
      " 0.67647059 0.53658537 0.54976303 0.21387283]\n",
      "ACC: [0.92763158 0.9495614  0.85745614 0.88888889 0.89546784 0.85964912\n",
      " 0.93128655 0.8874269  0.84576023 0.84137427]\n",
      "\n",
      "Started testing with A:fgsm on L:fc1 with Rank:1\n",
      "Stats for attack type:fgsm on the layer:fc1 with a gradient rank:1\n",
      "Overall test accuracy:0.981\n",
      "Right max : 0.196415770609319\n",
      "Fooled max: 0.2738351254480287\n",
      "Right min : 0.20430107526881722\n",
      "Binary  0.0\n",
      "Sensitivity [0.1641791  0.20338983 0.09589041 0.16326531 0.22727273 0.26815642\n",
      " 0.1221374  0.21153846 0.28636364 0.16964286]\n",
      "Specificity [0.97063253 0.90718563 0.96513605 0.94525829 0.84367446 0.91611842\n",
      " 0.96281646 0.9023406  0.77276596 0.91036633]\n",
      "Precision [0.22       0.08823529 0.33870968 0.18390805 0.15283843 0.32\n",
      " 0.25396825 0.21428571 0.19090909 0.14179104]\n",
      "ACC: [0.93189964 0.87741935 0.82867384 0.89032258 0.77562724 0.83297491\n",
      " 0.88387097 0.82508961 0.69605735 0.85089606]\n",
      "\n",
      "Started testing with A:fgsm on L:relu3 with Rank:1\n",
      "Stats for attack type:fgsm on the layer:relu3 with a gradient rank:1\n",
      "Overall test accuracy:0.9773\n",
      "Right max : 0.22448979591836735\n",
      "Fooled max: 0.27696793002915454\n",
      "Right min : 0.23760932944606414\n",
      "Binary  0.0\n",
      "Sensitivity [0.20689655 0.22222222 0.1039604  0.17708333 0.31638418 0.25153374\n",
      " 0.13492063 0.29090909 0.2705314  0.22608696]\n",
      "Specificity [0.96651446 0.91520244 0.96239316 0.95768025 0.85523013 0.91397849\n",
      " 0.96950241 0.89478045 0.79914163 0.89260143]\n",
      "Precision [0.21428571 0.112      0.32307692 0.23943662 0.24454148 0.28275862\n",
      " 0.30909091 0.27428571 0.19310345 0.16149068]\n",
      "ACC: [0.93440233 0.88338192 0.83600583 0.90306122 0.78571429 0.83527697\n",
      " 0.89285714 0.82215743 0.71938776 0.83673469]\n",
      "\n",
      "Started testing with A:fgsm on L:fc2 with Rank:1\n",
      "Stats for attack type:fgsm on the layer:fc2 with a gradient rank:1\n",
      "Overall test accuracy:0.9787\n",
      "Right max : 0.22044960116026105\n",
      "Fooled max: 0.27918781725888325\n",
      "Right min : 0.22987672226250908\n",
      "Binary  0.0\n",
      "Sensitivity [0.14545455 0.17741935 0.14646465 0.14606742 0.28395062 0.26011561\n",
      " 0.14492754 0.28313253 0.32242991 0.13114754]\n",
      "Specificity [0.97734139 0.90584662 0.9661304  0.93410853 0.87181594 0.91542289\n",
      " 0.96051571 0.91014015 0.77939914 0.90214797]\n",
      "Precision [0.21052632 0.08148148 0.42028986 0.13265306 0.22772277 0.30612245\n",
      " 0.28985507 0.30128205 0.21165644 0.11510791]\n",
      "ACC: [0.94416244 0.87309645 0.8484409  0.88324873 0.80275562 0.83321247\n",
      " 0.87889775 0.8346628  0.70848441 0.83393764]\n",
      "\n",
      "Started testing with A:fgsm on L:conv1 with Rank:2\n",
      "Stats for attack type:fgsm on the layer:conv1 with a gradient rank:2\n",
      "Overall test accuracy:0.9803\n",
      "Right max : 0.40775988286969256\n",
      "Fooled max: 0.34260614934114203\n",
      "Right min : 0.2884333821376281\n",
      "Binary  0.0\n",
      "Sensitivity [0.52380952 0.09090909 0.49760766 0.15       0.36363636 0.57763975\n",
      " 0.38848921 0.47798742 0.40358744 0.27642276]\n",
      "Specificity [0.91864927 0.99160946 0.92566984 0.92923795 0.9480198  0.90124481\n",
      " 0.97310513 0.93869097 0.90901137 0.90185036]\n",
      "Precision [0.23741007 0.3125     0.54736842 0.11650485 0.47058824 0.43867925\n",
      " 0.62068966 0.50666667 0.46391753 0.21794872]\n",
      "ACC: [0.90043924 0.95534407 0.8601757  0.88360176 0.88213763 0.86310395\n",
      " 0.9136164  0.88506589 0.82650073 0.84553441]\n",
      "\n",
      "Started testing with A:fgsm on L:relu1 with Rank:2\n",
      "Stats for attack type:fgsm on the layer:relu1 with a gradient rank:2\n",
      "Overall test accuracy:0.9795\n",
      "Right max : 0.37319884726224783\n",
      "Fooled max: 0.37103746397694526\n",
      "Right min : 0.34221902017291067\n",
      "Binary  0.0\n",
      "Sensitivity [0.35714286 0.14285714 0.51219512 0.26041667 0.30246914 0.48344371\n",
      " 0.42142857 0.42134831 0.34433962 0.22033898]\n",
      "Specificity [0.93474962 0.99324324 0.92476754 0.91718266 0.94453507 0.86176233\n",
      " 0.96955128 0.94710744 0.92006803 0.88661417]\n",
      "Precision [0.22522523 0.47058824 0.54123711 0.18939394 0.41880342 0.29918033\n",
      " 0.60824742 0.53956835 0.43712575 0.15294118]\n",
      "ACC: [0.9056196  0.95893372 0.86383285 0.87175793 0.86959654 0.82060519\n",
      " 0.91426513 0.879683   0.83213256 0.82997118]\n",
      "\n",
      "Started testing with A:fgsm on L:conv2 with Rank:2\n",
      "Stats for attack type:fgsm on the layer:conv2 with a gradient rank:2\n",
      "Overall test accuracy:0.9804\n",
      "Right max : 0.27769679300291544\n",
      "Fooled max: 0.29081632653061223\n",
      "Right min : 0.18658892128279883\n",
      "Binary  0.0\n",
      "Sensitivity [0.24561404 0.03174603 0.33495146 0.14814815 0.13815789 0.43478261\n",
      " 0.33093525 0.16666667 0.38325991 0.26612903]\n",
      "Specificity [0.9148289  0.98930481 0.87392796 0.87916344 0.96065574 0.88109001\n",
      " 0.95214923 0.92066116 0.93362445 0.88862179]\n",
      "Precision [0.11111111 0.125      0.31944444 0.07142857 0.30434783 0.3271028\n",
      " 0.43809524 0.2195122  0.53374233 0.19186047]\n",
      "ACC: [0.88702624 0.94533528 0.79300292 0.83600583 0.86953353 0.8287172\n",
      " 0.88921283 0.83163265 0.8425656  0.83236152]\n",
      "\n",
      "Started testing with A:fgsm on L:relu2 with Rank:2\n",
      "Stats for attack type:fgsm on the layer:relu2 with a gradient rank:2\n",
      "Overall test accuracy:0.9797\n",
      "Right max : 0.37971872686898595\n",
      "Fooled max: 0.3774981495188749\n",
      "Right min : 0.3493708364174685\n",
      "Binary  0.0\n",
      "Sensitivity [0.38888889 0.12280702 0.48387097 0.27777778 0.26829268 0.42307692\n",
      " 0.43697479 0.45806452 0.4254386  0.22522523]\n",
      "Specificity [0.9383192  0.98686244 0.93386243 0.91673275 0.95282224 0.87866109\n",
      " 0.96590909 0.93394649 0.91273375 0.88629032]\n",
      "Precision [0.20792079 0.29166667 0.58333333 0.19230769 0.44       0.31279621\n",
      " 0.55319149 0.47333333 0.4974359  0.15060241]\n",
      "ACC: [0.91635825 0.95040711 0.86158401 0.87416728 0.86972613 0.82605477\n",
      " 0.91931902 0.87934863 0.83049593 0.83197631]\n",
      "\n",
      "Started testing with A:fgsm on L:fc1 with Rank:2\n",
      "Stats for attack type:fgsm on the layer:fc1 with a gradient rank:2\n",
      "Overall test accuracy:0.9799\n",
      "Right max : 0.2179580014482259\n",
      "Fooled max: 0.1716147719044171\n",
      "Right min : 0.06951484431571325\n",
      "Binary  0.0\n",
      "Sensitivity [0.22222222 0.17910448 0.34554974 0.08695652 0.30322581 0.17241379\n",
      " 0.22962963 0.18128655 0.20982143 0.1440678 ]\n",
      "Specificity [0.92238131 0.92085236 0.84117647 0.93017843 0.92088091 0.93951947\n",
      " 0.90609952 0.93140496 0.89109767 0.92240697]\n",
      "Precision [0.10434783 0.10344828 0.25882353 0.08163265 0.32638889 0.29126214\n",
      " 0.20945946 0.27192982 0.2716763  0.14782609]\n",
      "ACC: [0.89500362 0.88486604 0.77262853 0.87400434 0.85155684 0.84286749\n",
      " 0.83997104 0.83852281 0.78059377 0.85590152]\n",
      "\n",
      "Started testing with A:fgsm on L:relu3 with Rank:2\n",
      "Stats for attack type:fgsm on the layer:relu3 with a gradient rank:2\n",
      "Overall test accuracy:0.9796\n",
      "Right max : 0.2347949080622348\n",
      "Fooled max: 0.2581329561527581\n",
      "Right min : 0.1746817538896747\n",
      "Binary  0.0\n",
      "Sensitivity [0.19753086 0.203125   0.342723   0.23913043 0.20606061 0.19760479\n",
      " 0.275      0.17714286 0.26415094 0.168     ]\n",
      "Specificity [0.94823706 0.95333333 0.81598668 0.92057489 0.92073659 0.90777867\n",
      " 0.9675425  0.94673123 0.84193012 0.91311094]\n",
      "Precision [0.18823529 0.17105263 0.24829932 0.17322835 0.2556391  0.22297297\n",
      " 0.44       0.31958763 0.22764228 0.15789474]\n",
      "ACC: [0.90523338 0.91937765 0.7446959  0.87623762 0.83734088 0.82390382\n",
      " 0.90876945 0.85148515 0.7553041  0.84724187]\n",
      "\n",
      "Started testing with A:fgsm on L:fc2 with Rank:2\n",
      "Stats for attack type:fgsm on the layer:fc2 with a gradient rank:2\n",
      "Overall test accuracy:0.9791\n",
      "Right max : 0.21785714285714286\n",
      "Fooled max: 0.18071428571428572\n",
      "Right min : 0.06142857142857143\n",
      "Binary  0.0\n",
      "Sensitivity [0.21311475 0.10606061 0.38308458 0.09375    0.22641509 0.11949686\n",
      " 0.27067669 0.25       0.22173913 0.10434783]\n",
      "Specificity [0.90664675 0.91529235 0.83903253 0.94555215 0.92908944 0.93392425\n",
      " 0.91239148 0.93606557 0.88461538 0.9229572 ]\n",
      "Precision [0.0942029  0.05833333 0.28518519 0.1125     0.29032258 0.18811881\n",
      " 0.24489796 0.36585366 0.27419355 0.10810811]\n",
      "ACC: [0.87642857 0.87714286 0.77357143 0.88714286 0.84928571 0.84142857\n",
      " 0.85142857 0.84785714 0.77571429 0.85571429]\n",
      "\n",
      "Started testing with A:fgsm on L:conv1 with Rank:3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_24164/142286010.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    255\u001B[0m     \u001B[0macc_mifgsm\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    256\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0ml\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mlayers\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 257\u001B[0;31m       \u001B[0mt\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtestModel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcriterion\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'cpu'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtest_loader\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0matt\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mr\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0ml_name\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0ml\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    258\u001B[0m       \u001B[0macc_mifgsm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mt\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    259\u001B[0m     \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlayers\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0macc_mifgsm\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_24164/142286010.py\u001B[0m in \u001B[0;36mtestModel\u001B[0;34m(model, criterion, device, test_loader, attack, rank, l_name)\u001B[0m\n\u001B[1;32m    196\u001B[0m       \u001B[0;31m# print(\"Trying to determine for the true class:\",labels.item(),\" which was labeled as:\",perturb_label.item())\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    197\u001B[0m       \u001B[0mtaylors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mnum_classes\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 198\u001B[0;31m       \u001B[0mtaylors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcheck_taylor_prediction\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mperturbed_data\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mbest_fits\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'b_p'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlab\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mlayer_name\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0ml_name\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mrank\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrank\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mlab\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtaylors\u001B[0m \u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    199\u001B[0m       \u001B[0;31m# taylors=[check_taylor_prediction(model,data,best_fits['b_p'],torch.tensor(lab).reshape(1),layer_name=l_name) for lab in taylors ]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    200\u001B[0m       \u001B[0mtaylor_np\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtaylors\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_24164/142286010.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    196\u001B[0m       \u001B[0;31m# print(\"Trying to determine for the true class:\",labels.item(),\" which was labeled as:\",perturb_label.item())\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    197\u001B[0m       \u001B[0mtaylors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mnum_classes\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 198\u001B[0;31m       \u001B[0mtaylors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcheck_taylor_prediction\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mperturbed_data\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mbest_fits\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'b_p'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlab\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mlayer_name\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0ml_name\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mrank\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrank\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mlab\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtaylors\u001B[0m \u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    199\u001B[0m       \u001B[0;31m# taylors=[check_taylor_prediction(model,data,best_fits['b_p'],torch.tensor(lab).reshape(1),layer_name=l_name) for lab in taylors ]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    200\u001B[0m       \u001B[0mtaylor_np\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtaylors\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_24164/142286010.py\u001B[0m in \u001B[0;36mcheck_taylor_prediction\u001B[0;34m(model, input, marker_images, check_label, layer_name, rank)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     50\u001B[0m   \u001B[0;31m# total_marker=compute_marker_image_prediction(model,input,marker_img,check_label,layer_name,rank)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 51\u001B[0;31m   \u001B[0mtotal_unknown\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcompute_unknown_image_prediction\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mmarker_img\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcheck_label\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mlayer_name\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mrank\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     52\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m   \u001B[0;32mreturn\u001B[0m \u001B[0mtotal_unknown\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_24164/142286010.py\u001B[0m in \u001B[0;36mcompute_unknown_image_prediction\u001B[0;34m(model, unk_image, marker_image, marker_label, l_name, rank)\u001B[0m\n\u001B[1;32m     67\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mcompute_unknown_image_prediction\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0munk_image\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mmarker_image\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mmarker_label\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0ml_name\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mrank\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     68\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 69\u001B[0;31m   \u001B[0mgrads\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0munk_input\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mloss_unk\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mget_layer_gradients\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0munk_image\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mmarker_label\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0ml_name\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mrank\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     70\u001B[0m   \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     71\u001B[0m   \u001B[0mlayer_input\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0munk_input\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mview\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_24164/142286010.py\u001B[0m in \u001B[0;36mget_layer_gradients\u001B[0;34m(model, target_img, label, l_name, rank)\u001B[0m\n\u001B[1;32m     91\u001B[0m   \u001B[0;31m# layer_input_data.requires_grad=True\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     92\u001B[0m   \u001B[0;31m# loss_value.backward()\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 93\u001B[0;31m   \u001B[0maux\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgrad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mloss_value\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mlayer_input_data\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mretain_graph\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcreate_graph\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     94\u001B[0m   \u001B[0;31m# aux=layer_input_data.grad\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     95\u001B[0m   \u001B[0mgrads\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maux\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/ThesisOulu/lib/python3.7/site-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mgrad\u001B[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001B[0m\n\u001B[1;32m    223\u001B[0m     return Variable._execution_engine.run_backward(\n\u001B[1;32m    224\u001B[0m         \u001B[0moutputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_outputs_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 225\u001B[0;31m         inputs, allow_unused, accumulate_grad=False)\n\u001B[0m\u001B[1;32m    226\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    227\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "6_U32AoPJxTC"
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  }
 ]
}